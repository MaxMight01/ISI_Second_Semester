
\documentclass[15pt,a4paper]{book}

\usepackage{amsmath, amsthm, amssymb} 
\usepackage{graphicx} % For including graphics
\usepackage{hyperref} % For clickable links
\usepackage{bookmark} % Better control over bookmarks
\usepackage{geometry} % Customize page layout
\usepackage{xcolor} % Colors for text and graphics
\usepackage{enumitem} % Customizable lists
\usepackage{fancyhdr} % Header and footer
\usepackage{titlesec} % Custom section/chapter titles
\usepackage[toc,page]{appendix} % For the appendix
\usepackage{longtable} % For tables spanning multiple pages
\usepackage{mathrsfs} % For script fonts in math mode
\usepackage{tocloft} % Custom table of contents
\usepackage{datetime2} % For dates
\usepackage{caption} % For better control over captions
\usepackage{float} % Fine control over figure/table placement
\usepackage{imakeidx} % For index

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\cftchapfont}{\normalfont} % Remove bold for chapter names
\renewcommand{\cftchappagefont}{\normalfont} % Remove bold for chapter page numbers
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\eax}[1]{\emph{#1}\index{#1}} % Macro for emphasis and index
\newcommand{\abs}[1]{\left| #1 \right|} % Absolute value
\newcommand{\N}{\mathbb{N}} % Natural Numbers
\newcommand{\R}{\mathbb{R}} % Real numbers
\newcommand{\Z}{\mathbb{Z}} % Integers
\newcommand{\Q}{\mathbb{Q}} % Rationals
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\osc}{\text{osc}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


% Custom Notation List Environment
\newlist{notationlist}{description}{1}
\setlist[notationlist]{font=\bfseries,labelsep=1em}

% Geometry Settings
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
}

% Hyperref Colors
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    citecolor=red
}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}

% Custom Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Chapter name on top left
\fancyhead[R]{\rightmark}  % section name on top right
\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Making index
\makeindex[intoc]

% Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries \centering}
  {\chaptername\ \thechapter}{20pt}{\Huge \centering}

\titlespacing*{\chapter}{0pt}{20pt}{100pt}

\begin{document}

\pagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vspace*{\fill}
    % Title in all caps
    {\Huge \textbf{\MakeUppercase{Real Analysis II}}\par}

    \vspace{0.5cm} % Adjust vertical spacing between title and subtitle
    % Subtitle in normal text, slightly enlarged
    {\Large Jaydeb Sarkar, notes by Ramdas Singh\par}

    \vspace{0.5cm} % Additional spacing before the author
    % Author information
    {\large Second Semester\par}
    \vspace*{\fill}
    \end{center}
\end{titlepage}

\clearpage

\pagenumbering{roman}

\chapter*{List of Symbols}
\begin{notationlist}
    \item $[a,b]$, the set of all real numbers $x$ such that $a \leq x \leq b$.
    \item $\mathbb{N} = \{1,2,\ldots\}$, the set of all natural numbers.
    \item $\mathbb{Z}_{+}$, defined as $\mathbb{N} \cup \{0\}$.
    \item $\mathcal{B}[a,b]$, the set of all boundary functions defined as $\{f:[a,b] \to \mathbb{R}\}$. It is a vector space (also an algebra) over $\mathbb{R}$.
    \item $\mathcal{P}[a,b]$, the set of all partitions of the set $[a,b]$.
    \item $I_{j}$, the $j^{\text{th}}$ subinterval of $[a,b]$, controlled by a partition set.
    \item $L(f,P)$, the lower Riemann sum for a function $f$ and partition $P$.
    \item $U(f,P)$, the upper Riemann sum for a function $f$ and partition $P$.
    \item $\int_{\underline{a}}^{b} f$, the lower Riemann integration for a function $f$.
    \item $\int_{a}^{\overline{b}} f$, the upper Riemann integration for a function $f$.
    \item $\mathcal{R}[a,b]$, the set of all Riemann integrable functions over the set $[a,b]$.
    \item $\cC[a,b]$, the set of all continuous functions over the set $[a,b]$.
    \item $\{a_{n}\}_{n \geq 1}$, a sequence of real numbers.
    \item $T_{P}$, a tag set for the partition $P$.
    \item $\cD$, the set of all differentiable functions.
    \item $\cF (\R)$, the set of all real functions.
\end{notationlist}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
\pagenumbering{arabic}
\pagestyle{fancy}


%%-------------------------------------------------------------------------------------------------

\chapter{THE RIEMANN INTEGRAL}

\section{On The Path of Definitions}

\textit{January 6th.}

\begin{definition}
    A \eax{partition} of $[a,b]$ are all the points $a=x_{0}<x_{1}<\ldots<x_{n} = b$. These points within are termed \eax{nodes}, and there are $n-1$ of them. The set $I_{j}$, defined by $[x_{j-1},x_{j}]$ denotes the $j^{\text{th}}$ subinterval.
\end{definition}

\begin{definition}
    If $I = (a,b), [a,b], (a,b], [b,a)$, then the \eax{length of the interval} $I$ is denoted by $b-a$.
\end{definition}

Denote by $\mathcal{P}[a,b]$, the set of all partition sets of $[a,b]$. For $P \in \mathcal{P}[a,b]$, with $n-1$ nodes, the length of $[a,b]$ will be $\abs{[a,b]} = \sum_{j=1}^{n} I_{j}$. We also note that for all $P, \tilde{P} \in \mathcal{P}[a,b]$, $P \cup \tilde{P} \in \mathcal{P}[a,b]$. Note that here we consider $n$ to be finite.

\begin{example}
    The set $\{\frac{1}{n}\}_{n \geq 1} \cup \{0\}$ does not belong to the set of all partitions of the unit interval, $\mathcal{P}[0,1]$.
\end{example}

Let $f \in \mathcal{B}[a,b]$, and $P \in \mathcal{P}[a,b]$. Suppose $P$ has the nodes $a = x_{0} < x_{1} < \ldots < x_{n} = b$. For all $j = 1, \ldots n$, define $m_{j} = \inf_{x \in I_{j}} f(x)$ and $M_{j} = \sup_{x \in I_{j}} f(x)$. Finally, denote by $m$ the value of $\inf_{x \in [a,b]} f(x)$ and $M$ to be $\sup_{x \in [a,b]} f(x)$. These are all real values.

Note that for all valid $j$, $m \leq m_{j} \leq M_{j} \leq M$ always holds. This must mean that
\begin{align}
    m \abs{I_{j}} &\leq m_{j} \abs{I_{j}} \leq M_{j} \abs{I_{j}} \leq M \abs{I_{j}} \notag \\
    m(b-a) &\leq \sum_{j=1}^{n} m_{j} \abs{I_{j}} \leq \sum_{j=1}^{n} M_{j} \abs{I_{j}} \leq M(b-a).
\end{align}

\begin{definition}
    Let $f \in \mathcal{B}[a,b]$. For $P$ $(a = x_{0}, x_{1}, \ldots, x_{n} = b)$ $\in \mathcal{P}[a,b]$, the \eax{lower Riemann sum} and the \eax{upper Riemann sum} are defined as
    \begin{equation}
        L(f,P) = \sum_{j=1}^{n} m_{j} \abs{I_{j}} \text{ and } U(f,P) = \sum_{j=1}^{n} M_{j} \abs{I_{j}},
    \end{equation}
    respectively. Thus, $m(b-a) \leq L(f,P) \leq U(f,P) \leq M(b-a)$ $\forall$ $P \in \mathcal{P}[a,b]$.
\end{definition}

\begin{remark}
    Clearly, $L(f,P), U(f,P) \in \mathbb{R}$ for all paritions $P \in \mathcal{P}[a,b]$ and all boundary functions $f \in \mathcal{B}[a,b]$. In fact, $L(f,P), U(f,P) \in [m(b-a),M(b-a)]$.
\end{remark}

\begin{definition}
    For $f \in \mathcal{B}[a,b]$, the \eax{lower Riemann integration} is defined as
    \begin{equation}
        \int_{\underline{a}}^{b} f = \sup\{L(f,P) | P \in \mathcal{P}[a,b]\}.
    \end{equation}
    Subsequently, the \eax{upper Riemann integration} is defined as
    \begin{equation}
        \int_{a}^{\overline{b}} f = \inf\{U(f,P) | P \in \mathcal{P}[a,b]\}.
    \end{equation}
\end{definition}
\begin{remark}
    Note that both $\int_{\underline{a}}^{b} f$ and $\int_{a}^{\overline{b}} f$ belong to the set $[m(b-a),M(b-a)]$.
\end{remark}

\begin{definition}
    A function $f \in \mathcal{B}[a,b]$ is \eax{Riemann integrable} if the lower and the upper Riemann integration are equal, that is, $\int_{\underline{a}}^{b} f = \int_{a}^{\overline{b}} f$. We denote this value by $\int_{a}^{b} f$, and call it the integration of $f$ over $[a,b]$. We then say that $f \in \mathcal{R}[a,b]$.
\end{definition}
\textit{January 8th.}
\begin{example}
    Note that $\cR [a,b] \subseteq \cB [a,b]$. In fact, it is a proper subset; for there exists the \eax{Dirichlet function} $f:[0,1] \to \R$ defined by
    \begin{equation}
        f(x) = \begin{cases}
            1 &\text{ if } x \in \mathbb{Q} \cap [0,1],\\
            0 &\text{ if otherwise.}
        \end{cases}
    \end{equation}
    Clearly, $f$ is a boundary function but not a continuous one. Now pick a partition $P$ with $x_{0}=0<x_{1}<\ldots<x_{n}=1$. Now, $m_{j}=0 \; \forall \; \implies L(f,P) = 0 \; \forall \; P \implies \int_{\underline{0}}^{1} f = 0$. If we consider that $M_{j}$ is always 1, we get $\int_{0}^{\overline{1}} f = 1$. $f$ does not belong to the set of Riemann integrable functions.
\end{example}
\begin{example}
    We show that $\cR [a,b]$ is not empty. Simply pick $f:[a,b] \to \R$ defined by $f(x) = c$ for all valid $x$. For every partition of this interval, we $m_{j} = M_{j} = c$ for all $j$. Finally, after computing the lower Riemann and upper Riemann sums, we get $\int_{\underline{a}}^{b} f = \int_{a}^{\overline{b}} f = c(b-a)$.
\end{example}
\begin{example}
    There exists a function $f \in \cB [a,b]$ such that $\abs{f} \in \cR [a,b]$ but $f \notin \cR [a,b]$. Indeed, simply pick a modification of the Dirichlet function defined as
    \begin{equation}
        f(x) = \begin{cases}
            -1 &\text{ if } x \in \mathbb{Q} \cap [a,b],\\
            1 &\text{ if otherwise.}
        \end{cases}
    \end{equation}
\end{example}

\begin{definition}
    Let $P, \tilde{P} \in \cP [a,b]$. We say $\tilde{P} \supset P$, or $\tilde{P}$ is a \eax{refinement} of $P$ if the nodes of $P$ are a subset of the nodes of $\tilde{P}$.
\end{definition}
\begin{example}
    For all $P,\tilde{P} \in \cP [a,b]$, we have $P \cup \tilde{P} \supset P, \tilde{P}$.
\end{example}
\begin{proposition}
    Let $f \in \cB [a,b]$, and $P, \tilde{P} \in cP [a,b]$. Suppose $\tilde{P} \supset P$. Then
    \begin{equation}
        L(f,P) \leq L(f,\tilde{P}) \leq U(f,\tilde{P}) \leq U(f,P).
    \end{equation}
\end{proposition}
\begin{proof}
    Note that it is sufficient to prove the first inequality; the second one is already true and the third one is analagous to the first one. Set $\tilde{P} = P \cup \{\tilde{x}\}$ with $\tilde{x} \notin P$, and set $P$ as $a = x_{0} < x_{1} < \ldots < x_{n} = b$. As $\tilde{x}$ is not part of $P$, there must exist some $j \in \{1, \ldots, n\}$ such that $\tilde{x} \in (x_{j-1},x_{j})$.

    For this $j$, let $\tilde{m}_{j-1} = \inf_{[x_{j-1},\tilde{x}]} f$ and let $\tilde{m}_{j} = \inf_{[\tilde{x},x_{j}]} f$. Therefore, we shall have
    \begin{align}
        L(f,\tilde{P}) - L(f,P) &= \tilde{m}_{j-1}(\tilde{x}-x_{j-1}) + \tilde{m}_{j}(x_{j}-\tilde{x}) - m_{j}(x_{j}-x_{j-1}) \notag \\
        &= \tilde{m}_{j-1}(\tilde{x}-x_{j-1}) + \tilde{m}_{j}(x_{j}-\tilde{x}) - m_{j}(x_{j}-\tilde{x}) - m_{j}(\tilde{x}-x_{j-1}) \notag \\
        &= (\tilde{m}_{j}-m_{j})(x_{j}-\tilde{x}) + (\tilde{m}_{j-1} - m_{j})(\tilde{x}-x_{j-1}) \geq 0 \notag \\
        \implies L(f,\tilde{P}) - L(f,P) &\geq 0.
    \end{align}
    Induction may now be applied to make any refinement $\tilde{P}$ of $P$. A similar logic applies to the upper Riemann sums.
\end{proof}
\begin{corollary}
    Let $f \in \cB [a,b]$. Then, for all $P,Q \in \cP [a,b]$, $L(f,P) \leq U(f,Q)$.
\end{corollary}
\begin{proof}
    Choose $R = P \cup Q$ to be a refinement of both $P$ and $Q$. Applying the previous proposition, we simply get $L(f,P) \leq L(f,R) \leq U(f,R) \leq U(f,Q)$.
\end{proof}
\begin{corollary}
    For all $f \in \cB [a,b]$, $\int_{\underline{a}}^{b} f \leq \int_{a}^{\overline{b}} f$ is always true.
\end{corollary}
\begin{proof}
    The lower Riemann intergral is the supremum of all the lower Riemann sums, so it must be the lowest upper bound, and, thus, has to be lesser than the upper Riemann sums. Similarly, the upper Riemann integral is greater than the lower Riemann sums. Consequently, we get the desired inequality.
\end{proof}
\section{Classification and Computation}
We now discuss the classification of Riemann integrable functions, and the computation of the Riemann integral.
\begin{theorem}
    Let $f \in \cB [a,b]$. Then, for $f$ to be Riemann intergrable, the only neccessary and sufficient condition is $\int_{\underline{a}}^{b} f \geq \int_{a}^{\overline{b}} f$.
\end{theorem}
\begin{proof}
    If the condition is satisfied, then we must conclude that the Riemann integrals have to be equal. The converse follows the opposite argument.
\end{proof}
\begin{theorem}
    Let $f \in \cB [a,b]$. Then $f \in \cR [a,b]$ if and only if for every $\varepsilon > 0$, there exists a $P \in \cP [a,b]$ such that $U(f,P) - L(f,P) < \varepsilon$.
\end{theorem}
\begin{proof}
    We first prove the converse; assume that for every $\varepsilon > 0$, there exists a $P$ satisfying $U(f,P) - L(f,P) < \varepsilon$. Now,
    \begin{align}
        L(f,P) \leq \underline{\int} f \leq \overline{\int} f \leq U(f,P) < \varepsilon + L(f,P) \leq \varepsilon + \underline{\int} f \notag \\
        \implies \overline{\int} f - \underline{\int} f < \varepsilon \; \forall \; \varepsilon < 0\\
        \implies \overline{\int} f = \underline{\int} f.
    \end{align}
    To show that every Riemann integrable function satisfies this property, let $f \in \cR [a,b]$ and $\varepsilon > 0$. The Riemann integrals are a supremum and an infimum, so there must exist a $P_{1} \in \cP [a,b]$ such that $L(f,P_{1}) > \int f - \frac{\varepsilon}{2}$ and a $P_{2} \in \cP [a,b]$ such that $U(f,P_{1}) < \int f + \frac{\varepsilon}{2}$. Now choose $P$ to be $P_{1} \cup P_{2}$, a refinement of both $P_{1}$ and $P_{2}$. Therefore,
    \begin{align}
        U(f,P) \leq U(f,P_{2}) < \int f + \frac{\varepsilon}{2} < (L(f,P_{1}) + \frac{\varepsilon}{2}) + \frac{\varepsilon}{2} \leq L(f,P) + \varepsilon \notag \\
        \implies U(f,P) - L(f,P) < \varepsilon.
    \end{align}
\end{proof}

\begin{definition}
    Let $P$ be a partition with $a=x_{0} < x_{1} < \ldots < x_{n} = b$. We define the \eax{norm} of $P$, or the \eax{mesh} of $P$, as $\norm{P} = \max_{j}\{x_{j}-x_{j-1}\}$.
\end{definition}

\begin{theorem}[\eax{Darboux's theorem}]
    Let $f \in \cB [a,b]$. Then $f \in \cR [a,b]$ if and only if for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $U(f,P)-L(f,P) < \varepsilon$ for all $P \in \cP [a,b]$ with $\norm{P} < \delta$. 
\end{theorem}
\begin{remark}
    To prove this, we define $\eta: \cP [a,b] \to \R_{\geq 0}$ by $\eta(P) = U(f,P) - L(f,P)$ for all $P \in \cP [a,b]$.
\end{remark}
\begin{proof}
    The proof of the converse is trivial and follows the same reasoning as before. To show that every Riemann integrable function satisfies this property, let $f \in \cR [a,b]$ and $\varepsilon > 0$. There exists a $\tilde{P} \in \cP [a,b]$ such that $U(f,\tilde{P}) - L(f,\tilde{P}) < \frac{\varepsilon}{2}$. Denote the number of nodes in $\tilde{P}$ by $p$, and set $\delta = \frac{\varepsilon}{8pM}$, where $M$ is the supremum of $f$ over $[a,b]$. Pick $P \in \cP [a,b]$ and assume that $\norm{P} < \delta$. Now set $\hat{P} = P \cup \tilde{P}$; $\hat{P}$ has at most $p$ points that are not in $P$.

    For now, assume that $p = 1$. Then, $\tilde{P} = P \cup \{\tilde{x}\}$ with $\tilde{x} \notin P$. Thus, with variables defined as before,
    \begin{equation}
        L(f,\hat{P}) - L(f,P) = (\tilde{m}_{j}-m_{j})(x_{j}-\tilde{x}) + (\tilde{m}_{j-1} - m_{j})(\tilde{x}-x_{j-1}).
    \end{equation}
    Notice that $(\tilde{m}_{j}-m_{j}), (\tilde{m}_{j-1} - m_{j}) < 2M$ and $(x_{j}-\tilde{x}), (\tilde{x}-x_{j-1}) < \delta$. In fact, in general, for an arbitrary $p$, we have
    \begin{equation}
        L(f,\hat{P}) - L(f,P) < 4pM\delta = \frac{\varepsilon}{2}.
    \end{equation}
    A similar story unfolds for the upper sums,
    \begin{equation}
        U(f,P) - U(f, \hat{P}) < \frac{\varepsilon}{2}.
    \end{equation}
    Together, the equations combine to form
    \begin{align}
        U(f,P) - L(f,P) < \varepsilon + U(f,\hat{P})-L(f,\hat{P}) < \varepsilon + U(f,\tilde{P})-L(f,\tilde{P}) < 2\varepsilon.
    \end{align}
\end{proof}
\textit{January 13th.}\\
We now wish to answer two questions; which elements reside in the set $\cR[a,b]$, and the value of the Riemann integral $\int_{a}^{b} f$ for some $f \in \cR[a,b]$.

Let us first wonder whether $\cC[a,b]$, the set of continuous functions, is a subset of $\cR[a,b]$. In fact, this is true.

\begin{theorem}
    $\cC[a,b] \subseteq \cR[a,b]$.
\end{theorem}
\begin{proof}
    Fix $f \in \cC[a,b]$; thus, $f:[a,b] \to \R$ is also uniformly continuous. For all $\varepsilon > 0$, there eixsts $\delta > 0$ such that
    \begin{equation}
        \abs{f(x)-f(y)} < \frac{\varepsilon}{b-a} \text{ for all } \abs{x-y} < \delta.
    \end{equation}
    Pick $P \in \cP[a,b]$ such that $\norm{P} < \delta$, and fix such a $P : a = x_{0} < x_{1} < \ldots < x_{n} = b$. Thus,
    \begin{align}
        U(f,P) - L(f,P) &= \sum_{j=1}^{n} (M_{j}-m_{j})\abs{I_{j}}.
    \end{align}
    Now, $f|_{I_{j}} : I_{j} \to \R$ is a continuous function for all valid $j$. Therefore, there exist $\eta_{j}, \zeta_{j} \in I_{j}$ such that $f(\eta_{j}) = M_{j}$ and $f(\zeta_{j}) = m_{j}$. $M_{j}-m_{j}$ can be rewritten as $f(\eta_{j}) - f(\zeta_{j})$. As $\abs{\eta_{j}-\zeta_{j}} < \delta$, it follows that
    \begin{align}
        M_{j}-m_{j} &< \frac{\varepsilon}{b-a} \text{ for all } j \\
        \implies (M_{j}-m_{j})\abs{I_{j}} &< \frac{\varepsilon}{b-a} \abs{I_{j}} \notag \\
        \implies \sum_{j=1}^{n} (M_{j}-m_{j}) \abs{I_{j}} &< \varepsilon.
    \end{align}
    By Darboux's theorem, $f$ is Riemann integrable.
\end{proof}
We now wish to compute $\int_{a}^{b} f$. Our first attempt at this is the following theorem.
\begin{theorem}
    Let $f \in \cB[a,b]$. Then $f \in \cR[a,b]$ if and only if there exists a sequence $\{P_{n}\}_{n \geq 1} \subseteq \cP[a,b]$ such that
    \begin{equation}
        \lim_{n \to \infty} U(f,P_{n}) - L(f,P_{n}) = 0.
    \end{equation}
    Moreover, in this case,
    \begin{equation}
        \int_{a}^{b} f = \lim_{n \to \infty} U(f,P_{n}) = \lim_{n \to \infty} L(f,P_{n}).
    \end{equation}
\end{theorem}
\begin{proof}
    Let us first assume that $f$ is Riemann integrable. Thus, for $\varepsilon = \frac{1}{n}$, there exists $P_{n} \in \cP[a,b]$ such that
    \begin{equation}
        0 \leq U(f,P) - L(f,P) < \frac{1}{n}
        \implies U(f,P) - L(f,P) \to 0.
    \end{equation}
    For the converse, let $\varepsilon > 0$. There exists $N \in \N$ such that
    \begin{equation}
        0 \leq U(f,P_{n}) - L(f,P_{n}) < \varepsilon \text{ for all } n \geq N.
    \end{equation}
    Pick $n$ to be $N$. Thus, $U(f,P_{N}) - L(f,P_{N}) < \varepsilon$ must imply that $f \in \cR[a,b]$.

    Let us now show the computation of the integral. We have
    \begin{align}
        0 \leq U(f,P_{n}) - \overline{\int_{a}^{b}} = U(f,P_{n}) - \underline{\int_{a}^{b}} f \leq U(f,P_{n}) - L(f,P_{n}) &\to 0 \implies \\
        \implies U(f,P_{n}) &\to \int_{a}^{b}f.
    \end{align}
    Similarly,
    \begin{align}
        0 \leq \underline{\int_{a}^{b}} f - L(f,P_{n}) = \int_{a}^{b} f - U(f,P_{n}) + U(f,P_{n}) - L(f,P_{n}) &\to 0 \\
        \implies L(f,P_{n}) &\to \int_{a}^{b} f.
    \end{align}
\end{proof}
\begin{remark}
    Let $f \in \cB[a,b]$, and let $\{P_{n}\}_{n \geq 1} \subseteq \cP[a,b]$. If $L(f,P_{n}) \to \lambda$ and if $U(f,P_{n}) \to \lambda$. We then must have $f \in \cR[a,b]$ and $\int_{a}^{b} = \lambda$. This is reminiscent of Newton's method of integration.
\end{remark}
\begin{example}
    Let us compute $\int_{0}^{1}f$ where $f(x) = x^{2}$ on $[0,1]$. For all $n \in \N$, consider the partitions $P_{n} : 0 = x_{0} < x_{1} = \frac{1}{n} < x_{2} = \frac{2}{n} < \ldots < x_{n} = \frac{n}{n} = 1$. Thus, $I_{j} = \left[ \frac{j-1}{n}, \frac{j}{n} \right] \text{ for all } j = 1, \ldots, n$. We then have $M_{j} = (\frac{j}{n})^{2}$ and $m_{j} = (\frac{j-1}{n})^{2}$. The sums can be computed as
    \begin{align}
        U(f,P_{n}) &= \sum_{j=1}^{n} \frac{1}{n} \cdot \frac{j^{2}}{n^{2}} = \frac{1}{n^{3}} \sum_{j=1}^{n} j^{2} = \frac{(n+1)(2n+1)}{6n^{2}} &\to \frac{1}{3}, \\
        L(f,P_{n}) &= \sum_{j=1}^{n} \frac{1}{n} \cdot \frac{(j-1)^{2}}{n^{2}} = \frac{1}{n^{3}} \sum_{j=1}^{n} (j-1)^{2} = \frac{(n-1)(2n-1)}{6n^{2}} &\to \frac{1}{3}.
    \end{align}
    Both sums converge to $\frac{1}{3}$; $f$ is Riemann integrable and $\int_{0}^{1} f = \frac{1}{3}$.
\end{example}
\begin{example}
    We show that $\cC[a,b]$ is a proper subset of $\cR[a,b]$. Let us consider the function $f:[a,b] \to \R$ defined by
    \begin{equation}
        f(x) = \begin{cases}
        1 &\text{ if } 0 \leq x < \frac{1}{2}, \\
        \frac{1}{2} &\text{ if } x = \frac{1}{2}, \\
        0 &\text{ if } \frac{1}{2} < x \leq 1.
        \end{cases}
    \end{equation}
    Fix $\varepsilon > 0$. Choose the partition $P_{\varepsilon} : 0 < \frac{1}{2}-\varepsilon < \frac{1}{2} + \varepsilon < 1$. We then have $m_{1} = 1 = M_{1}$, $m_{2} = 0 = m_{3}$, and $M_{2} = 1$, $M_{3} = 0$. Therefore, we have
    \begin{equation}
        L(f,P) = \frac{1}{2}-\varepsilon, \; U(f,P) = \frac{1}{2} + \varepsilon.
    \end{equation}
    Finally,
    \begin{equation}
        U(f,P) - L(f,P) = 2\varepsilon < 3\varepsilon.
    \end{equation}
    $f$ is Riemann integrable, but is not a continuous function.
\end{example}
\textit{January 15th.}\\ \\
We now discuss a more refined way of computing the Riemann integral.
\begin{definition}
    Let $P:a=x_{0} < x_{1} < \ldots < x_{n} = b$ be a partition of $[a,b]$. A \eax{tag} of $P$ is a function $T_{P}:\{I_{j}\}_{j=1}^{n} \to [a,b]$ such that $T_{p}(I_{j}) \in I_{j}$ for all $j = 1,2,\ldots,n$. In other words, $T_{p} = \{\zeta_{j}\}_{j=1}^{n}$ such that $\zeta_{j} \in I_{j}$ for all valid $j$.
\end{definition}
\begin{definition}
    Let $f \in \cB[a,b]$, $P \in \cP[a,b]$, and $T_{P}$ be a tag set. The \eax{Riemann sum} of $f$ with respect $(P,T_{P})$ is
    \begin{equation}
        S(f,P) = \sum_{j=1}^{n} f(\zeta_{j})\abs{I_{j}}.
    \end{equation}
\end{definition}
What good is Riemann sum and why the need for defining it? Let us fix $f \in \cB[a,b]$, $P \in \cP[a,b]$, and $T_{P} = \{\zeta_{j}\}_{j=1}^{n}$. We now have $m_{j} \leq f(\zeta_{j}) \leq M_{j}$ for all valid $j$. Multiplying by the subintervals $\abs{I_{j}}$ and summing over all $j$'s gives us
\begin{equation}
    L(f,P) \leq S(f,P) \leq U(f,P)
\end{equation}
for any tag set $T_{P}$. This gives us a better condition as if both the lower and upper sum collapse, then the Riemann sum will give us a value, say $\lambda$, for any tag set $T_{P}$.

What do we hope for? We wish to show that $L(f,P) \to \lambda$ as $\norm{P} \to 0$. Let us write this more formally.
\begin{definition}
    Given $f \in \cB[a,b]$ and $\lambda \to \R$, we say
    \begin{equation}
        \lim_{\norm{P} \to 0} S(f,P) = \lambda
    \end{equation}
    if for every $\varepsilon > 0$, there exists $\delta > 0$ such that
    \begin{equation}
        \abs{S(f,P) - \lambda} < \varepsilon \text{ for all } P \in \cP[a,b] \text{ satisfying } \norm{P} < \delta \text{ for any } T_{P}.
    \end{equation}
\end{definition}
Note that if such $\lambda$ exists, it is unique.
\begin{theorem}
    Let $f \in \cB[a,b]$. Then $f \in \cR[a,b]$ if and only if there exists $\lambda \in \R$ such that $\lim_{\norm{P} \to 0} S(f,P) = \lambda$. Also, in this case, $\int_{a}^{b} f = \lambda$.
\end{theorem}
\begin{proof}
    Assume $f$ is Riemann integrable. Let $\lambda = \int_{a}^{b} f$, For every $\varepsilon > 0$, there exists $\delta > 0$ such that
    \begin{equation*}
        U(f,P) - L(f,P) < \varepsilon \text{ for all } \norm{P} < \delta.
    \end{equation*}
    We know that $L(f,P) \leq S(f,P) \leq U(f,P)$ for all tag sets $T_{P}$. Now,
    \begin{equation}
        L(f,P) \geq U(f,P) - \varepsilon \geq \overline{\int} f - \varepsilon = \lambda - \varepsilon.
    \end{equation}
    Similarly,
    \begin{equation}
        U(f,P) < \varepsilon + L(f,P) \leq \varepsilon + \underline{\int} f = \varepsilon + \lambda.
    \end{equation}
    Thus, we have, for all $P \in \cP[a,b]$ satisfying $\norm{P} < \delta$ and for all $T_{P}$, we have
    \begin{equation*}
        \lambda - \varepsilon < S(f,P) \leq \lambda + \varepsilon \implies \abs{S(f,P) - \lambda} < \varepsilon \implies \lim_{\norm{P} \to 0} S(f,P) = \lambda.
    \end{equation*}
    For the converse, let $\lambda = \lim_{\norm{P} \to 0} S(f,P)$. Then for all $\varepsilon > 0$, there exists $\delta > 0$ such that
    \begin{equation*}
        \abs{S(f,P) - \lambda} < \frac{\varepsilon}{3}
    \end{equation*}
    for all $\abs{P} < \delta$ and for all $T_{P}$. Note that $S(f,P)$ is just $\sum_{j=1}^{n} f(\zeta_{j}) \abs{I_{j}}$. By taking the infimum and supremum of tag sets for a fixed $P$ over their valid intervals, we have
    \begin{equation}
        \lambda-\frac{\varepsilon}{3} < L(f,P) < \lambda + \frac{\varepsilon}{3}, \; \lambda - \frac{\varepsilon}{3} < U(f,P) < \lambda + \frac{\varepsilon}{3}.
    \end{equation}
    Finally, we can now minimize $U(f,P) - L(f,P)$ for Darboux's criteria---
    \begin{equation}
        U(f,P) - L(f,P) < \lambda+\frac{\varepsilon}{3} - \lambda + \frac{\varepsilon}{3} = \frac{2\varepsilon}{3} < \varepsilon \implies f \in \cR[a,b].
    \end{equation}
    Finally, for all $\norm{P} < \delta$, we have
    \begin{equation}
        \lambda - \frac{\varepsilon}{3} < L(f,P) \leq \underline{\int}f = \int f = \overline{\int}f < U(f,P) < \lambda + \frac{\varepsilon}{3} \implies \lambda = \int f.
    \end{equation}
\end{proof}

We are now done with the classification and computation of the Riemann integral.

\begin{theorem}
    Let $f \in \cR[a,b]$ and let $\{P_{n}\} \subseteq \cP[a,b]$ such that $\norm{P_{n}} \to 0$. Then
    \begin{equation}
        \lim_{n \to \infty} S(f,P_{n}) = \int_{a}^{b} f
    \end{equation}
    for all tag sets $T_{P_{n}}$.
\end{theorem}
\begin{proof}
    $f$ is Riemann integrable. For all $\varepsilon > 0$, there exists $\delta > 0$ such that for all $\norm{P} < \delta$, $U(f,P)-L(f,P) < \varepsilon$. There also exists a natural $N$ such that $\norm{P_{n}} < \delta$ for all $n \geq N$. This tells us that $U(f,P_{n})-L(f,P_{n}) < \varepsilon$ for all $n \geq N$. We rewrite this as
    \begin{align}
        U(f,P_{n}) - \int f + \int f - L(f,P_{n}) < \varepsilon \text{ for all } n \geq N.
    \end{align}
    Pairing up the terms on the left, we find that they are non-negative, so each pair individually must be less than $\varepsilon$---
    \begin{equation}
        0 \leq U(f,P_{n}) - \int f < \varepsilon \text{ and } 0 \leq \int f - L(f,P_{n}) < \varepsilon
    \end{equation}
    Using this equation, we can finally write
    \begin{equation}
        \int f - \varepsilon < L(f,P_{n}) \leq S(f,P_{n}) \leq U(f,P_{n}) < \int f + \varepsilon \implies \lim_{n \to \infty} S(f,P_{n}) = \int f.        
    \end{equation}
    Again, this is regardless of the choice of tag sets.
\end{proof}
\section{School Integration Rocks}
Let us now connect to Newton's definition of integration. Pick $f \in \cC[a,b]$. For any $n \in \N$, consider the partition $P_{n} : a = x_{0} < x_{1} < \ldots < x_{n} = b$ such that $x_{j}-x_{j-1} = \frac{b-a}{n}$. This is the standard school partition. Note that $\norm{P_{n}} = \frac{b-a}{n} \to 0$. For all tag sets $\{\zeta_{j}^{(n)}\}$ of $P_{n}$, we find that
\begin{equation}
    \int_{a}^{b} f = \lim_{n \to \infty} \sum_{j=1}^{n} f(\zeta_{j}^{(n)}) \frac{b-a}{n}.
\end{equation}
In school, the tag set was generally chosen as the left endpoints or right endpoints of the subintervals. The left endpoints tag set is
\begin{equation}
    \zeta_{j} = a + \frac{b-a}{n}(j-1).
\end{equation}

\chapter{ANTIDERIVATIVES}
\textit{January 20th.}\\ \\
Let us first summarise; let $f$ be a bounded function on the interval $[a,b]$. Then, the following are equivalent---
\begin{itemize}
    \item $f \in \cR[a,b]$,
    \item For $\varepsilon > 0$, there exists $P \in \cP[a,b]$ such that $U(f,P)-L(f,P) < \varepsilon$,
    \item For $\varepsilon > 0$, there exists $\delta > 0$ such that for all $P \in \cP[a,b]$ satisfying $\norm{P} < \delta$, $U(f,P)-L(f,P) < \varepsilon$,
    \item There exists $\{P_{n}\} \subset \cP[a,b]$ such that $U(f,P_{n})-L(f,P_{n}) \to 0$,
    \item There exists $\{P_{n}\} \subset \cP[a,b]$ such that $\lim_{n \to \infty} U(f,P_{n}) = \lim_{n \to \infty} L(f,P_{n})$,
    \item $\lim_{\norm{P} \to 0} S(f,P) = \lambda$.
\end{itemize}

\section{Spaces and Algebras}
Note that $\cB[a,b]$ is a vector space; a linear combination of two elements in it is also part of this set. If we now define a multiplication of two vectors in it as $(f \cdot g)(x) = f(x) \cdot g(x)$ for $f,g \in \cB[a,b]$, we see that $f \cdot g$ is also a vector in this space. Thus, we have made $\cB[a,b]$ into an algebra.

We now question if $\cR[a,b]$ is an algebra, or even a vector space. We begin by defining $\cI: \cR[a,b] \to \R$ by $\cI (f) = \int_{a}^{b} f$. We can ask the following questions:
\begin{itemize}
    \item whether $\cI$ is linear; $\cI(rf+g) = r\cI(f) + \cI(g)$, for $r \in \R$,
    \item whether $\cI$ is multiplicative; $\cI(f \cdot g) = \cI(f) \cdot \cI(g)$.
\end{itemize}
Other questions can also be asked; is $\cI$ a monotonic function, or even a homomorphism if $\cR[a,b]$ proves to be a vector space.\\

Given $f \in \cB[a,b]$ and $P \in \cP[a,b]$, we have
\begin{equation}
    M_{j} - m_{j} = \sup\{\abs{f(x)-f(y)} : x,y \in I_{j}\}.
\end{equation}
We denote this value by $\osc_{I_{j}} f$, the oscillation of $f$ over $I_{j}$. If we adopt this notation, we would then have
\begin{equation}
    U(f,P) - L(f,P) = \sum_{j=1}^{n} \osc_{I_{j}} f \cdot \abs{I_{j}}.
\end{equation}
\subsection{Results on $\cI$ and $\cR[a,b]$}
Assume that, here, $f,g \in \cR[a,b]$ and $r \in \R$.
\begin{enumerate}
\item Coming back, let us prove that $\cI$ is, in fact, linear.
\begin{proof}
    First, we show that $rf+g$ is Riemann integrable for $f,g \in \cR[a,b]$. For any partition $P \in \cP[a,b]$, we have
    \begin{align}
        S(rf+g,P) &= \sum_{j=1}^{n} (rf+g)(\zeta_{j})\abs{I_{j}} \notag \\
        &= r \sum_{j=1}^{n} f(\zeta_{j})\abs{I_{j}} + \sum_{j=1}^{n} g(\zeta_{j}) \abs{I_{h}} = rS(f,P)+S(g,P).
    \end{align}
    This result is regardless of choice of tag set $T_{P}$. Thus, $rf+g \in \cR[a,b]$. Now, we show the linearity of $\cI$.
    \begin{align}
        \abs{S(rf+g,P) -r\int f - \int g} &= \abs{r(S(f,P)-\int f) + (S(g,P) - \int g)} \notag \\ &\leq \abs{r}\abs{S(f,P)-\int f} + \abs{S(g,P)- \int g} \\
        \implies S(rf+g,P) &\to r \int f + \int g \text{ as } \norm{P} \to 0.
    \end{align}
\end{proof}
\item We now show that $f \cdot g \in \cR[a,b]$ for $f$ and $g$ Riemann integrable. We first show this for $f^{2}$.
\begin{proof}
    We show that $f^{2} \in \cR[a,b]$. We have
    \begin{align}
        \abs{f^{2}(x)-f^{2}(y)} = \abs{f(x)+f(y)}\abs{f(x)-f(y)} &\leq 2M \abs{f(x)-f(y)} \notag \\
        \implies \sum_{j} \osc_{I_{j}} \abs{f}^{2} \cdot \abs{I_{j}} &\leq \sum_{j} 2M \cdot \osc_{I_{j}} f \cdot \abs{I_{j}} \notag \\
        U(f^{2},P)-L(f^{2},P) &\leq 2M(U(f,P)-L(f,P)).
    \end{align}
    Since $f$ is Riemann integrable, $U(f,P)-L(f,P)$ can be lowered to less than $\varepsilon > 0$, we can make the left hand term less than $\varepsilon$. Thus, $f^{2} \in \cR[a,b]$.
\end{proof}
Despite this, $\cI(f^{2}) \neq (\cI(f))^{2}$; $\cI$ is not multiplicative.

\item Let us show $f \cdot g \in \cR[a,b]$.
\begin{proof}
    Break down $f \cdot g$ as
    \begin{equation}
        f \cdot g = \frac{1}{4}\left( (f+g)^{2} - (f-g)^{2}\right).
    \end{equation}
    From the previous results, the right hand side is Riemann integrable. Thus, $f \cdot g \in \cR[a,b]$.
\end{proof}

\item If $f(x) \geq 0$ for all $x \in [a,b]$, then $\cI(f) \geq 0$. This result is left as an exercise to the reader.

\item If $f \geq g$, then $\cI(f) \geq \cI(g)$. This result is also left as an exercise to the reader.

\item If $f \in \cR[a,b]$, then $\abs{f} \in \cR[a,b]$. Moreover, $\abs{\cI(f)} \leq \cI(\abs{f})$.
\begin{proof}
    Start with
    \begin{equation}
        \abs{f(x)}-\abs{f(y)} \leq \abs{f(x)-f(y)}.
    \end{equation}
    Therefore, for all $P \in \cP[a,b]$, $\osc_{I_{j}} \abs{f} \leq \osc_{I_{j}} f$ for all valid $j$. Thus,
    \begin{equation}
        U(\abs{f},P) - L(\abs{f},P) \leq U(f,P) - L(f,P)
    \end{equation}
    tells us that $\abs{f}$ is also Riemann integrable. Using the fact that $\cI$ is monotonous,
    \begin{align}
        -\abs{f} \leq f \leq \abs{f} \implies -\int \abs{f} \leq \int f \leq \int \abs{f} \implies \abs{\cI(f)} \leq \cI(\abs{f}).
    \end{align}
\end{proof}

\item We have $\max\{f,g\},\min\{f,g\} \in \cR[a,b]$. The proof of this result is left as an exercise to the reader.

\item If $\frac{1}{g} \in \cB[a,b]$, then $\frac{1}{g} \in \cR[a,b]$. This would also imply that $\frac{f}{g} \in \cR[a,b]$.
\begin{proof}
    Denote $1/\tilde{M} = \sup_{[a,b]} \frac{1}{g}$. Then,
    \begin{equation}
        \abs{\frac{1}{g(x)} - \frac{1}{g(y)}} \leq \frac{1}{\tilde{M}^{2}} \abs{g(x)-g(y)}.
    \end{equation}
    We can then proceed by using the oscillations.
\end{proof}

\item Let $a < c < b$. Then $f|_{[a,c]} \in \cR[a,c]$ and $f|_{[c,b]} \in \cR[c,b]$. Moreover, $\int_{a}^{c} f + \int_{c}^{b} f = \int_{a}^{b} f$.

\begin{proof}
    For $\varepsilon > 0$, there exists $P \in \cP[a,b]$ such that $U(f,P) - L(f,P) < \varepsilon$. Let, without loss of generality, $c \in P$. If not, we could refine $P$ as $P \cup \{c\}$. We then have $P: x_{0} = a < x_{1} < \ldots < x_{m} = c < \ldots < x_{n} = b$. The nodes from $x_{0}$ to $x_{m}$ form a partition $P_{1} \in \cP[a,b]$, and the nodes $x_{m}$ to $x_{n}$ form a partition $P_{2} \in \cP[c,b]$. Thus,
    \begin{align}
        (U(f,P_{1})-L(f,P_{1})) + (U(f,P_{2})-L(f,P_{2})) < \varepsilon.
    \end{align}
    Both of these pairs of terms are non-negative, so each pair individually is less than $\varepsilon$. Thus, $f|_{[a,c]} \in \cR[a,c]$ and $f|_{[c,b]} \in \cR[c,b]$. Now let $\lambda_{1} = \int_{a}^{c} f$ and $\lambda_{2} = \int_{c}^{b} f$. Then,
    \begin{align}
        \int_{a}^{b} f \geq L(f,P) = L(f,P_{1}) + L(f,P_{2}) > U(f,P_{1}) - \varepsilon + U(f,P_{2}) - \varepsilon \geq \lambda_{1} + \lambda_{2} - 2\varepsilon
    \end{align}
    and
    \begin{align}
        \int_{a}^{b} f \leq U(f,P) = U(f,P_{1}) + U(f,P_{2}) < L(f,P_{1}) + \varepsilon + L(f,P_{2}) + \varepsilon \leq \lambda_{1} + \lambda_{2} + 2\varepsilon.
    \end{align}
    Both inequalities imply that $\abs{\int_{a}^{b} f - (\lambda_{1}+\lambda_{2})} \leq 2\varepsilon$.
\end{proof}
\subsection{Results on $f \in \cR[a,b]$}
Assume that, here, $f \in \cR[a,b]$.
\begin{enumerate}
    \item $m(b-a) \leq \int_{a}^{b} f \leq M(b-a)$. This result is left as an exercise to the reader.
    \item If $f \in \cC[a,b]$, then there exists $c \in [a,b]$ such that $f(c) = \frac{1}{b-a} \int_{a}^{b} f$. This is known as the \eax{mean value theorem}.
    \begin{proof}
        Simply start with $m(b-a) \leq \int_{a}^{b} f \leq M(b-a)$, and divide everything by $b-a \neq 0$. There exists $\eta$ and $\zeta$ such that $f(\eta) = m$ and $f(\zeta) = M$, so $f$ takes every value in between.
    \end{proof}
\end{enumerate}

\begin{theorem}
    Let $f:[a,b] \to [c,d]$ and $g:[c,d] \to \R$, and let $f \in \cR[a,b]$ and $g \in \cC[c,d]$. Then $g \circ f \in \cR[a,b]$. Note that for this to be satisfied, $g$ must be continuous on top of Riemann integrable.
\end{theorem}
\begin{proof}
    Clearly, $g \circ f \in \cB[a,b]$. By uniform continuity, for every $\varepsilon > 0$, there exists $\delta > 0$ such that
    \begin{equation}
        \abs{g(x)-g(y)} < \frac{\varepsilon}{2(b-a)} \text{ for all } \abs{x-y} < \delta.
    \end{equation}
    Also, there exists $P \in \cP[a,b]$ such that
    \begin{equation}
        U(f,P) - L(f,P) < \frac{\varepsilon \delta}{4M}
    \end{equation}
    where $M = \sup_{[c,d]} g(y)$. We claim that $U(g \circ f, P) - L(g \circ f, P) < \varepsilon$. Let $n$ be the number of intervals of $P$. Write $J = \{1,2,\ldots,n\} = J_{1} \sqcup J_{2}$, where we define
    \begin{equation*}
        J_{1} = \{j \in J : \osc_{I_{j}}f < \delta \}, \; J_{2} = \{j \in J : \osc_{I_{j}}f \geq \delta\}.
    \end{equation*}
    Now, if $j \in J_{1}$, then 
    \begin{align}
        \abs{f(x)-f(y)} &< \delta \text{ for all } x,y \in I_{j} \\
        \implies \abs{g(f(x)) = g(f(y))} &< \frac{\varepsilon}{2 (b-a)} \text{ for all } x,y \in I_{j} \notag \\
        \implies \sup_{x,y \in I_{j}} \abs{g(f(x))-g(f(y))} = \osc_{I_{j}} g \circ f &\leq \frac{\varepsilon}{2(b-a)} \notag \\
        \implies \sum_{j \in J_{1}} \osc_{I_{j}} g \circ f \cdot \abs{I_{j}} &\leq \frac{\varepsilon}{2(b-a)} \cdot \sum_{j \in J_{1}} \abs{I_{j}} \leq \frac{\varepsilon}{2}.
    \end{align}
    Now, if $j \in J_{2}$, then
    \begin{align}
        \osc_{I_{j}} g \circ f &\leq 2M \\
        \implies \sum_{j \in J_{2}} \osc_{I_{j}} g \circ f \cdot \abs{I_{j}} &\leq 2M \cdot \sum_{j \in J_{2}} \abs{I_{j}} \leq 2M \cdot \sum_{j \in J_{2}} \abs{I_{j}} \osc_{I_{j}} \cdot \frac{1}{\delta} \notag \\
        &\leq \frac{2M}{\delta} \cdot \sum_{j \in J} \abs{I_{J}} \osc_{I_{j}} f = \frac{2M}{\delta} (U(f,P)-L(f,P)) < \frac{2M}{\delta} \cdot \frac{\varepsilon \delta}{4 M} \\
        \implies \sum_{j \in J_{2}} \osc_{I_{j}} g \circ f \cdot \abs{I_{j}} &< \frac{\varepsilon}{2}
    \end{align}
    Combining both inequalities, we have
    \begin{equation}
        U(g \circ f, P) - L(g \circ f, P) = \sum_{j \in J_{1}} \osc_{I_{j}} g \circ f \cdot \abs{I_{j}} + \sum_{j \in J_{2}} \osc_{I_{j}} g \circ f \cdot \abs{I_{j}} < \varepsilon.
    \end{equation}
\end{proof}
\begin{example}
    Let $f \in \cR[a,b]$. From the previous theorem, $e^{f}, \sin f, \cos f \in \cR[a,b]$ and for $f \geq 0$, $f^{\frac{1}{n}} \in \cR[a,b]$.
\end{example}
\begin{theorem}
    Let $f,g \in \cB[a,b]$. If $f(x) = g(x)$ for all $x$ but finitely many, then $f \in \cR[a,b]$ if and only if $g \in \cR[a,b]$. Moreover, in this case, $\int_{a}^{b} f = \int_{a}^{b} g$.
\end{theorem} So, if $f \equiv 0$ except for some finitely many points in $[a,b]$, then $f \in \cR[a,b]$ and $\int_{a}^{b} f = 0$.
\begin{proof}
    Without loss of generality, let $c \in [a,b]$ and $f(x) = g(x)$ for all $x \in [a,b]\backslash \{c\}$, and $f(c) \neq g(c)$. Note that it is enough to prove $\overline{\int} f = \overline{\int} g$ and $\underline{\int} f = \underline{\int} g$. Let $\tilde{M} \geq \sup f, \sup g$. For $\varepsilon > 0$, there exists $P \in \cP[a,b]$ such that
    \begin{equation}
        U(f,P) < \frac{\varepsilon}{2} + \overline{\int} f.
    \end{equation}
    Now set $\delta = \frac{\varepsilon}{8 \tilde{M}}$. Let $\tilde{P} \supset P$ such that $\norm{\tilde{P}} < \delta$. Now $f \equiv g$ except for $x = c$. Let $\{\tilde{I_{j}}\}_{j=1}^{n}$ be the subintervals of $\tilde{P}$. Let $p$ (and possibly $p+1$) be such that $f(x) \neq g(x)$ on $I_{p}$ (and possibly on $I_{p+1})$. Note that
    \begin{align*}
        \abs{\sup_{I_{j}} f - \sup_{I_{j}}} g &= 0 \text{ for all } j \neq p, p+1 \text{ or }\\
        &\leq 2\tilde{M} \text{ for } j = p,p+1.
    \end{align*}
    Thus,
    \begin{align}
        \abs{U(f,\tilde{P})-U(g,\tilde{P})} &\leq \sum_{j=1}^{n} \abs{I_{j}} \cdot \abs{\sup_{I_{j}}f - \sup_{I_{j}} g} = \sum_{j = p,p+1} \abs{I_{j}} \cdot \abs{\sup_{I_{j}}f - \sup_{I_{j}} g} \leq 4\delta \tilde{M} \\
        \implies \abs{U(f,\tilde{P}) - U(g,\tilde{P})} &< \frac{\varepsilon}{2}.
    \end{align}
    Using this, we have
    \begin{align}
        \overline{\int} g &\leq U(g,\tilde{P}) < U(f,\tilde{P}) + \frac{\varepsilon}{2} \leq U(f,P) + \frac{\varepsilon}{2} < \frac{\varepsilon}{2} + \overline{\int} f + \frac{\varepsilon}{2} \notag\\
        \implies \overline{\int} g &< \overline{\int} f + \varepsilon \text{ for all } \varepsilon > 0 \notag\\
        \implies \overline{\int} g &\leq \overline{\int} f.
    \end{align}
    Note that if we switch $f$ and $g$ around, we would get $\overline{\int} f \leq \overline{\int} g$. Thus, we must have
    \begin{equation*}
        \overline{\int} f = \overline{\int} g.
    \end{equation*}
\end{proof}

\section{The Fundamental Theorem of Calculus}
To start off, we will define the set(s) $\cD$ as the set of all differentiable functions, and $\cF(\R)$ is the set of all real valued functions. Recall that we define the integral function as 
\begin{equation}
    \cI : \cR[a,b] \to \cF (\R) \text{ defined as } \cI (f) (x) = \int_{a}^{x}f.
\end{equation}
We will also define differentiation as a function,
\begin{equation}
    \frac{d}{dx} : \cD \to \cF(\R) \text{ defined as } \frac{d}{dx} (f) = \frac{df}{dx}. 
\end{equation}
The fundamental theorem of calculus, roughly, states that both $\frac{d}{dx} \circ \cI$ and $\cI \circ \frac{d}{dx}$ are the identity function. There is a little trouble with this rough statement as the composition here makes not much sense.
\begin{definition}
    Let $S \subseteq \R$, and let $f:S \to \R$ be a function. A differentiable function $F$ is called the \eax{antiderivative} of $f$ if $f(x) = F'(x)$ for all $x \in S$.
\end{definition}
\begin{example}
    We state some antiderivatives here.
    \begin{enumerate}
        \item The antiderivative of $x$ is $\frac{1}{2}x^{2}$.
        \item Polynomials have an antiderivative.
        \item Continuous functions have an antiderivative.
        \item The function $f:\R \to \R$ defined as $f(x) = 0$ if $x \geq 0$ and $f(x) = 1$ if $x < 0$ does not have an antiderivative.
    \end{enumerate}
\end{example}
\begin{theorem}[Darboux.]
    Let $f:(a,b) \to \R$ be differentiable, and let $a < a_{0} < b_{0} < b$. If $f'(a_{0}) < r < f'(b_{0})$, then there exists $c_{0} \in (a_{0},b_{0})$ such that $f'(c_{0}) = r$.    
\end{theorem}
\begin{proof}
    Construct a new function as $g(x) = - f(x) + rx$ for all $x \in (a,b)$. Note that $g:(a,b) \to \R$ is differentiable, and $g'(x) = -f'(x) + r$ for all $x \in (a,b)$. Also, $g'(a_{0}) > 0$ and $g'(b_{0}) < 0$. Now, $g|_{[a_{0},b_{0}]}$ is uniformly continuous. Using the sign of the derivatives, we have
    \begin{align}
        g(a_{0}+h)-g(a_{0}) &> 0 \text{ for some small } h > 0,\\
        g(b_{0}+h)-g(b_{0}) &> 0 \text{ for some small } h < 0.
    \end{align}
    From these two equations, we can see that $g$ attains a maximum at some $c_{0} \in (a_{0},b_{0})$. This implies that $g'(c_{0}) = 0$ and $f'(c_{0}) = r$.
\end{proof}
\textit{January 27th.}
\begin{theorem}[The \eax{first fundamental theorem of calculus}]
    Let $f \in \cR[a,b]$, $F \in \cC[a,b]$, and let $F$ be an antiderivative of $f$ on $(a,b)$, that is, $F'(x) = f(x)$ for all $x \in (a,b)$. Then
    \begin{equation}
        \int_{a}^{b} f = F(b)-F(a).
    \end{equation}
\end{theorem}
\begin{proof}
    Let $P \in \cP[a,b]$ defined as $P:a=x_{0}<x_{1}<\ldots<x_{n} = b$. Now,
    \begin{align}
        F(b) - F(a) = \sum_{j=1}^{n} F(x_{j}) - F(x_{j-1}) \notag.
    \end{align}
    Since $F \in \cC[a,b]$, we have $F|_{[x_{j-1},x_{j}]} \in \cC[x_{j-1},x_{j}] \cap \cD(x_{j-1},x_{j})$. Thus, from the intermediate value theorem, there exists some $\zeta_{j} \in (x_{j-1},x_{j})$ such that
    \begin{align}
        F(x_{j})-F(x_{j-1}) = F'(\zeta_{j}) \cdot (x_{j}-x_{j-1}) = f(\zeta_{j}) \cdot (x_{j}-x_{j-1}).
    \end{align}
    We choose these $\zeta_{j}$'s to be our tag set. Thus, we shall have
    \begin{align}
        F(b)-F(a) &= \sum_{j=1}^{n}F(x_{j})-F(x_{j-1}) = \sum_{j=1}^{n}f(\zeta_{j}) \cdot \abs{I_{j}} \notag \\
        \implies L(f,P) &\leq F(b)-F(a) \leq U(f,P) \text{ for all } P \in \cP[a,b] \notag \\
        \implies F(b)-F(a) &= \int_{a}^{b} f.
    \end{align}
\end{proof}
\begin{remark}
    Note that the first fundamental theorem of calculus implies that $\int_{a}^{b} f$ can be computed simply by finding an antiderivative of $f$. We now ask where the antiderivative is, and even if it exists. The second fundamental theorem of calculus answers this.
\end{remark}
\begin{theorem}[The \eax{second fundamental theorem of calculus}]
    Let $f \in \cR[a,b]$. Define $F:[a,b] \to \R$ as $F(x) = \int_{a}^{x} f(t)dt$ for all $x \in [a,b]$. Then,
    \begin{enumerate}
        \item $F \in \cC[a,b]$,
        \item if $f$ is continuous at $x_{0} \in (a,b)$, then $F$ is differentiable at $x_{0}$ and $F'(x_{0}) = f(x_{0})$,
        \item if $f$ is continuous from the right at $a$, then $F_{+}'(a) = f(a)$.
    \end{enumerate}
\end{theorem}
\begin{corollary}
    Let $f \in \cC[a,b]$. Then
    \begin{equation}
        \frac{d}{dx} \left( \int_{a}^{x} f(t) dt \right) = f(x).
    \end{equation}
\end{corollary}
This is an interesting corollary that comes in handy. We now prove the theorem.
\begin{proof}
    Set $M = \sup_{x \in [a,b]} \abs{f(x)}$. Now,
    \begin{align}
        \abs{F(x)-F(y)} = \abs{\int_{x}^{y} f(t) dt}.
    \end{align}
    Now start with
    \begin{align}
        -M &\leq f(t) \leq M \text{ for all } t \in [a,b] \notag\\
        \implies -M(y-x) &\leq \int_{x}^{y} f(t) dt \leq M(y-x) \notag\\
        \implies \abs{F(x)-F(y)} = \abs{\int_{x}^{y}f(t) dt} &\leq M\abs{x-y} \text{ for all } x,y \in [a,b].
    \end{align}
    Any function that satisfies thie property is a \eax{lipschitz function}. It can be shown that any lipschitz function is continuous. This part of the proof is left as an exercise. The first part is now proved.

    Let $f$ be continuous on some $x_{0} \in (a,b)$. For some $x \neq x_{0}$, we can start with
    \begin{align}
        \frac{F(x)-F(x_{0})}{x-x_{0}} - f(x_{0}) &= \frac{1}{x-x_{0}} \int_{x_{0}}^{x} f(t) dt - \frac{1}{x-x_{0}} \int_{x_{0}}^{x} f(x_{0}) dt = \frac{1}{x-x_{0}} \int_{x_{0}}^{x} (f(t)-f(x_{0}))dt.
    \end{align}
    $f$ is continuous at $x_{0}$, so for $\varepsilon > 0$, there exists $\delta > 0$ such that
    \begin{equation*}
        \abs{f(t)-f(x_{0})} < \varepsilon \text{ for all } \abs{t-x_{0}} < \delta.
    \end{equation*}
    Therefore,
    \begin{align}
        \abs{\frac{F(x)-F(x_{0})}{x-x_{0}} - f(x_{0})} &= \abs{\frac{1}{x-x_{0}} \int_{x_{0}}^{x} (f(t)-f(x_{0}))dt} \leq \frac{1}{\abs{x-x_{0}}} \abs{\int_{x_{0}}^{x} \abs{f(t)-f(x_{0})}dt} \notag \\
        &< \frac{1}{\abs{x-x_{0}}} \abs{\int_{x_{0}}^{x} \varepsilon dt} = \varepsilon \text{ for all } x \in (x_{0}-\delta,x_{0}+\delta) \backslash \{x_{0}\}.
    \end{align}
    This must imply that
    \begin{equation}
        \lim_{x \to x_{0}} \frac{F(x)-F(x_{0})}{x-x_{0}} = f(x_{0}).
    \end{equation}
\end{proof}
Here is an interesting example.
\begin{example}
    Define $f:[0,2] \to \R$ by $f = \chi_{[0,1]}$. Clearly, $f \in \cR[0,2]$. Let $F:[0,2] \to \R$ be defined by $F = \int_{0}^{x} f$. We have
    \begin{align*}
        \text{ for all } x \in [0,1], &\; F(x) = \int_{0}^{x} \chi_{[0,1]} = \int_{0}^{x} 1 = x,\\
        \text{ for all } x \in (1,2], &\; F(x) = \int_{0}^{x} \chi_{[0,1]} = \int_{0}^{1} 1 + \int_{1}^{x} 0 = 1. 
    \end{align*}
    The corollary guarantees that the antiderivative of a continuous function is differentiable; if our function is Riemann integrable but not continuous, the antiderivative may not be differentiable.
\end{example}

\subsection{Some Interesting Methods}
\begin{theorem}[The method of \eax{integration by parts}]
    Let $f,g \in \cD[a,b]$ and $f',g' \in \cR[a,b]$. Then,
    \begin{align}
        \int_{a}^{b} f'g + \int_{a}^{b} fg' = f(b)g(b) - f(a)g(a).
    \end{align}
\end{theorem}
\begin{proof}
    Simply start with $\int_{a}^{b} (fg)'$.
\end{proof}
\begin{theorem}[The method of \eax{change of variable}]
    Let $u \in \cD[a,b]$, $u' \in \cR[a,b]$, and let $f \in \cC(u([a,b]))$. Then,
    \begin{equation}
        \int_{a}^{b} f(u(t)) u'(t) dt = \int_{u(a)}^{u(b)} f(x) dx.
    \end{equation}
\end{theorem}
\begin{proof}
    The proof for a constant function $u$ is trivial. So, assume that $u$ is a non-constant function. Note that $(f \circ u) u'$ in Riemann integrable on $[a,b]$. Define $F:u([a,b]) \to \R$ as
    \begin{equation}
        F(x) = \int_{u(a)}^{x} f(t) dt \text{ for all } t \in u([a,b]).
    \end{equation}
    The second fundamental theorem of calculus tells us that $F' = f$ on $u([a,b])$. Now for $t \in [a,b]$, $(F \circ u)'(t) = F'(u(t)) u'(t)$. By the first fundmental theorem of calculus,
    \begin{align}
        \int_{a}^{b} F'(u(t)) u'(t) dt &= \int_{a}^{b} (F \circ u)' (t) dt \\
        \implies \int_{a}^{b} f(u(t)) u'(t) dt &= F(u(b)) - F(u(a)) = \int_{u(a)}^{u(b)} f(t) dt.
    \end{align}
\end{proof}

\end{enumerate}

\chapter{IMPROPER INTEGRALS}
\textit{February 3rd.}

We first state some `assumptions' on Riemann intgerals, which we now consider to be \eax{proper integrals}.
\begin{enumerate}
    \item The function to be integrated was bounded, that is, $f \in \cB[a,b]$.
    \item The interval on which the function was to be integrated was bounded.
\end{enumerate}

We want to talk about integrals of functions that \textit{don't} follow these assumptions.
\begin{enumerate}
    \item Evaluating $\int_{a}^{b} f$ even if $f$ is not bounded on $[a,b]$.
    \item Evaluating $\int_{a}^{\infty} f$, $\int_{-\infty}^{b} f$, or $\int_{-\infty}^{\infty} f$.
\end{enumerate}

\section{Improper Integrals of Type I}

\begin{definition}[An \eax{improper integral of type I}.]
    Let $f \notin \cR[a,b]$, and $f \in \cR[c,b]$ for all $a < c < b$. If
    \begin{equation}
        \lim_{\varepsilon \to 0^{+}} \int_{a+\varepsilon}^{b} f \; \left( = \int_{a}^{b} f \right)
    \end{equation}
    exists, then we say that the integral $\int_{a}^{b} f$ converges to this value, and diverges otherwise. The convergence is equivalent to saying that $\lim_{c \to a^{+}} \int_{c}^{b} f$ exists.

    Similarly, if $f \notin \cR[a,b]$ and $f \in \cR[a,c]$ for all $a < c < b$, and if
    \begin{equation}
        \lim_{\varepsilon \to 0^{+}} \int_{a}^{b-\varepsilon} f \; \left( = \int_{a}^{b} f \right)
    \end{equation}
    exists, then we say that the integral $\int_{a}^{b} f$ converges to this value, and diverges otherwise. The convergence is equivalent to saying that $\lim_{c \to b^{-}} \int_{a}^{c} f$ exists.

    If $f$ properly diverges at a point $a < c < b$ in $[a,b]$, then we define
    \begin{equation}
        \int_{a}^{b} f = \int_{a}^{c} f + \int_{c}^{b} f
    \end{equation}
    provided that \textit{either} of the right hand side integrals exist.
\end{definition}

\begin{example}
    We wish to compute $\int_{0}^{1} \frac{1}{x^{2}} dx$. This is an improper integral of type I; the function is unbounded at $x = 0$. Therefore, for small $\varepsilon > 0$,
    \begin{equation}
        \int_{\varepsilon}^{1} \frac{1}{x^{2}} dx = \left[ -\frac{1}{x} \right]_{\varepsilon}^{1} = \frac{1}{\varepsilon} - 1.
    \end{equation}
    Taking the limit as $\varepsilon$ tends to zero from the positive side, we see that the limit diverges to positive infinity. The improper integral diverges.
\end{example}

\begin{example}
    We wish to compute $\int_{0}^{1} \frac{1}{\sqrt{x}} dx$. For $\varepsilon > 0$, small,
    \begin{align}
        \int_{\varepsilon}^{1} \frac{1}{\sqrt{x}} dx = \left[ 2\sqrt{x} \right]_{\varepsilon}^{1} = 2-2\sqrt{\varepsilon} \to 2 \text{ as } \varepsilon \to 0^{+}.
    \end{align}
    Thus, $\int_{0}^{1} \frac{1}{\sqrt{x}} dx = 2$.
\end{example}

\begin{example}
    We wish to compute $\int_{0}^{2} \frac{1}{2x-x^{2}} dx$. Let us split this integral as $\int_{0}^{2} f = \int_{0}^{1} + \int_{1}^{2}$. For $\varepsilon > 0$, small,
    \begin{equation}
        \int_{1}^{2-\varepsilon} \frac{1}{x(2-x)}dx = \left[ \frac{1}{2} \ln \left( \frac{x}{2-x} \right) \right]_{1}^{2-\varepsilon} = \frac{1}{2} \ln \left( \frac{2-\varepsilon}{\varepsilon} \right) - \frac{1}{2} \ln 1.
    \end{equation}
    This integral diverges, so the entire integral must diverge.
\end{example}

In general,
\begin{example}
    Let $p > 0$. We wish to evaluate $\int_{0}^{1} \frac{1}{x^{p}} dx$. For $\varepsilon > 0$, small,
    \begin{align}
        \int_{\varepsilon}^{1} \frac{1}{x^{p}} dx = \begin{cases}
            \left[ \frac{x^{1-p}}{1-p} \right]_{\varepsilon}^{1} &\text{ if } p \neq 1, \\
            \left[ \ln x \right]_{\varepsilon}^{1} &\text{ if } p = 1
        \end{cases} = \begin{cases}
            \frac{1}{1-p}(1-\varepsilon^{1-p}) &\text{ if } p \neq 1, \\
            - \ln \varepsilon &\text{ if } p = 1.
        \end{cases}
    \end{align}
    If we apply $\lim_{\varepsilon \to 0^{+}}$, we see that $\int_{0}^{1} \frac{1}{x^{p}}$ converges to $\frac{1}{1-p}$ if $0 < p < 1$, and diverges otherwise.
\end{example}

\subsection{Tests of Convergence}
We now discuss when an improper integral of type I is convergent or not.
\begin{theorem}[The \eax{comparison test I}]
    Let $0 \leq f(x) \leq g(x)$ for all $x \in [a,b)$. Assume that $\int_{a}^{b} f$ and $\int_{a}^{b} g$ are improper integrals solely due to the point $x = b$.
    \begin{enumerate}
        \item If $\int_{a}^{b} g$ converges, then $\int_{a}^{b} f$ converges.
        \item If $\int_{a}^{b} f$ diverges, then $\int_{a}^{b} g$ diverges.
    \end{enumerate}
\end{theorem}
\begin{proof}
    We prove part the first part only. Set $G(x) = \int_{a}^{x} g$ for all $x \in [a,b)$. As $\int_{a}^{b} g$ converges, and $G$ is non-decreasing function, we have
    \begin{equation*}
        \int_{a}^{b} g = \sup \{\int_{a}^{x} g \; | x \in [a,b) \}.
    \end{equation*}
    Now, from the inequality, since the integral function is monotonous,
    \begin{align}
        0 &\leq \int_{a}^{x} f \leq \int_{a}^{x} g \text{ for all } x \in [a,b) \notag \\
        \implies 0 &\leq \sup_{x \in [a,b)} \int_{a}^{x} f \leq \int_{a}^{b} g < \infty \\
        \implies \lim_{c \to b^{-}} \int_{a}^{c} f &< \infty.
    \end{align}
\end{proof}

\begin{example}
    We question whether $\int_{0}^{\frac{\pi}{2}} \frac{\sin x}{x^{p}} dx$ converges for $p > 0$. Firstly, note that
    \begin{align*}
        0 \leq \sin x \leq 1 \implies 0 \leq \frac{\sin x}{x^{p}} \leq \frac{1}{x^{p-1}} \text{ for all } x \in (0, \frac{\pi}{2}].
    \end{align*}
    Note that the integral of the rightmost term converges for $1 < p < 2$. Also, $\int_{0}^{1} \frac{1}{x^{p}} dx$ also converges for $0 < p < 1$. The case for $p = 1$ is also convergent. Hence, $\int_{0}^{\frac{\pi}{2}} \frac{\sin x}{x^{p}}$ converges if $0 < p < 2$.
\end{example}

\begin{theorem}[The \eax{limit comparison test I}]
    Let $f(x), g(x) \geq 0$ for all $x \in [a,b)$. Suppose
    \begin{equation*}
        \lim_{x \to b^{-}} \frac{f(x)}{g(x)} = l \neq 0, \infty.
    \end{equation*}
    Then the improper integrals $\int_{a}^{b} f$ and $\int_{a}^{b} g$ converge, or diverge, together.
\end{theorem}
\begin{proof}
    We know $0 < l < \infty$. Pick $\varepsilon > 0$ such that $l- \varepsilon > 0$. There exists $c \in [a,b)$ such that
    \begin{align}
        \abs{\frac{f(x)}{g(x)} - l} &< \varepsilon \text{ for all } x \in (c,b) \notag \\
        \implies l - \varepsilon < \frac{f(x)}{g(x)} &< l+\varepsilon \text{ for all } x \in (c,b).
    \end{align}
    Therefore, $0 < (l-\varepsilon) g(x) < f(x)$ for all $x \in (c,b)$. By the comparison test - I, if $\int_{a}^{b} f$, that is, $\int_{c}^{b} f$ converges, then $\int_{c}^{b} (l-\varepsilon) g$, that is, $\int_{a}^{b} (l-\varepsilon) g$ converges. This tells us that $\int_{a}^{b} g$ converges. To show that $\int_{a}^{b} g$ converges implies that converge of $\int_{a}^{b} f$ is left as an exercise to the reader.
\end{proof}

\textit{February 6th.}
\begin{definition}
    An improper integral $\int_{a}^{b} f$ is absolutely convergent if the improper integral $\int_{a}^{b} \abs{f}$ converges.
\end{definition}

\begin{theorem}
    The absolute convergence of an improper integral implies the convergence of the improper integral.
\end{theorem}
\begin{proof}
    Let $\int_{a}^{b}f$ be an improper integral at $a$. Now, $-\abs{f(x)} \leq f(x) \leq \abs{f(x)}$ implies that $0 \leq \abs{f(x)} + f(x) \leq 2\abs{f(x)}$ for all $x \in (a,b]$. Therefore, for all $a < c < b$,
    \begin{align}
        0 \leq \int_{c}^{b} (\abs{f(x)}+f(x))dx \leq 2 \int_{c}^{b} \abs{f(x)} dx.
    \end{align}
    If the rightmost integral converges, then by the comparison test, $\int_{a}^{b} \abs{f}+f$ converges. Finally,
    \begin{align}
        \int_{c}^{b} f(x)dx &= \int_{c}^{b} (\abs{f(x)}+f(x)) dx - \int_{c}^{b} \abs{f(x)} dx \notag \\
        \implies \lim_{c \to a^{+}} \int_{c}^{b} f(x)dx &= \int_{a}^{b} (\abs{f(x)}+f(x)) dx - \int_{a}^{b} \abs{f(x)} dx.
    \end{align}
\end{proof}

\section{Improper Integrals of Type II}

\begin{definition}[An \eax{improper integral of type II}.]
    Fix $a \in \R$ and let $f \in \cR[a,r]$ for all $r > a$. If $\lim_{r \to \infty} \int_{a}^{r} f$ exists, then we say that $\int_{a}^{\infty} f$ converges and we write
    \begin{align}
        \int_{a}^{\infty} f = \lim_{r \to \infty} \int_{a}^{r} f.
    \end{align}
    If the limit diverges, then we say that the integral diverges. Similarly, we define
    \begin{align}
        \int_{-\infty}^{b} = \lim_{r \to \infty} \int_{-r}^{a} f
    \end{align}
    provided that the limit exists.
\end{definition}

\begin{definition}
    Let $f \in \cR[a,b]$ for all $a < b \in \R$. If there exists a $c \in \R$ such that $\int_{-\infty}^{c} f$ and $\int_{c}^{\infty} f$ exist, then we say that $\int_{-\infty}^{\infty} f$ exists and define
    \begin{align}
        \int_{-\infty}^{\infty} f= \int_{-\infty}^{c} f + \int_{c}^{\infty} f.
    \end{align}
    Note that the value of the integral, if it exists, is independent of the choice of $c$; this result is left as an exercise to the reader.
\end{definition}

\begin{example}
    We wish to compute $\int_{0}^{\infty} \sin x dx$. Note that, by the fundamental theorem of calculus,
    \begin{align}
        \int_{0}^{r} \sin x dx = [-\cos x]_{0}^{r} = 1 - \cos r.
    \end{align}
    The right hand side does not converge as $r \to \infty$, so we conclude that the integral does not converge.
\end{example}
\begin{example}
    We wish to compute $\int_{-\infty}^{0} e^{-x} dx$. Note that $e^{-x} \in \cR[-r,0]$ for all $r > 0$. Also,
    \begin{align}
        \int_{-r}^{0} e^{-x} dx = [-e^{-x}]_{-r}^{0} = e^{r}-1 \to \infty \text{ as } r \to \infty.
    \end{align}
    Therefore, the integral diverges.
\end{example}
\begin{example}
    We wish to compute $\int_{-\infty}^{\infty} \frac{dx}{1+x^{2}}$. Let us take $c = 0$ to be out `split'; evaluating the integrals, we have
    \begin{align}
        \int_{0}^{\infty} \frac{dx}{1+x^{2}} = \lim_{r \to \infty} \int_{0}^{r} \frac{dx}{1+x^{2}} = \left[ \arctan x \right]_{0}^{r} = \arctan r \to \frac{\pi}{2} \text{ as } r \to \infty,\\
        \int_{-\infty}^{0} \frac{dx}{1+x^{2}} = \lim_{r \to \infty} \int_{-r}^{0} \frac{dx}{1+x^{2}} = \left[ \arctan x \right]_{-r}^{0} = \arctan r \to \frac{\pi}{2} \text{ as } r \to \infty.
    \end{align}
    Thus,
    \begin{align}
        \int_{-\infty}^{\infty} \frac{dx}{1+x^{2}} = \int_{-\infty}^{0} \frac{dx}{1+x^{2}} + \int_{0}^{\infty} \frac{dx}{1+x^{2}} = \frac{\pi}{2} + \frac{\pi}{2} = \pi.
    \end{align}
\end{example}
We say that $f \in \cR[a,\infty)$ if $f \in \cR[a,r]$ for all $r > a$; this is just another notation.

\subsection{Tests of Convergence}
\begin{theorem}[The \eax{comparison test II}]
    Let $a \in \R$ and let $f,g \in \cR[a,\infty)$. Suppose that $0 \leq f(x) \leq g(x)$ for all $x \in [a,\infty)$.
    \begin{enumerate}
        \item If $\int_{a}^{\infty} g$ converges, then $\int_{a}^{\infty} f$ converges.
        \item If $\int_{a}^{\infty} f$ diverges, then $\int_{a}^{\infty} g$ diverges.
    \end{enumerate}
\end{theorem}
\begin{proof}
    For all $t > a$, it follows that
    \begin{align}
        0 \leq \int_{a}^{t} f(x)dx \leq \int_{a}^{t} g(x)dx.
    \end{align}
    Denote the two depicted integrals by $F(t)$ and $G(t)$ respectively, for all $t > a$. The result follows.
\end{proof}
\begin{example}
    We question whether $\int_{a}^{\infty} \frac{dx}{e^{x}+1}$ converges. Note that $\frac{1}{e^{x}+1} \leq e^{-x}$ for all $x \in [a,\infty)$. Evaluting the integral of the right hand side function,
    \begin{align}
        \int_{a}^{\infty} e^{-x}dx = \lim_{r \to \infty} \int_{a}^{r} e^{-x} dx = \lim_{r \to \infty} (e^{-a}-e^{-r}) = e^{-a}.
    \end{align}
    It converges, so our original integral also converges.
\end{example}

\textit{February 10th.}
\begin{theorem}[The \eax{limit comparison test II}]
    Let $f,g \in \cR[a,\infty)$ and suppose $f(x),g(x) \geq 0$ for all $x \in [0,\infty)$. Also suppose
    \begin{equation*}
        \lim_{x \to \infty} \frac{f(x)}{g(x)} > 0.
    \end{equation*}
    Then the imporper integrals $\int_{a}^{\infty} f$ and $\int_{a}^{\infty} g$ converge, or diverge, together.
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{example}
    We wish to test the convergence of $\int_{1}^{\infty} \frac{dx}{x\sqrt{x^{2}+1}}$. Let $f(x) = \frac{1}{x\sqrt{x^{2}+1}}$ and let $g(x) = \frac{1}{x^{2}}$. We see that
    \begin{equation}
        \frac{f(x)}{g(x)} = \frac{1}{\sqrt{1+\frac{1}{x^{2}}}} \to 1 > 0 \text{ as } x \to \infty.
    \end{equation}
    As $\int_{1}^{\infty} \frac{1}{x^{2}} dx$ is convegent, then by the limit comparison test, $\int_{1}^{\infty}\frac{dx}{x\sqrt{x^{2}+1}}$ must also converge.
\end{example}

\begin{theorem}
    Let $f \in \cR[a,\infty)$. If $\int_{a}^{\infty} \abs{f}$ converges, then $\int_{a}^{\infty} f$ also converges. The converse is not true.
\end{theorem}


\chapter{REFINING OF CONVERGENCE}

Recall the following:
\begin{enumerate}
    \item The \eax{Cauchy limit criterion}; $\lim_{x \to a} f(x)$ exists if and only if for all $\varepsilon > 0$, there exists a $\delta > 0$ such that
    \begin{equation}
        \abs{f(x_{1})-f(x_{2})} < \varepsilon \text{ for all } x_{1},x_{2} \in (a-\delta,a+\delta)\backslash \{a\}.
    \end{equation}

    \item We say $\lim_{x \to \infty} f(x) = l$ if for every $\varepsilon > 0$, there exiss $M > 0$ such that
    \begin{equation}
        \abs{f(x)-l} < \varepsilon \text{ for all } x > M.
    \end{equation}

    \item Also, $\lim_{x \to \infty} f(x) = l$ if and only if for every $\varepsilon > 0$, there exists $M > 0$ such that
    \begin{equation}
        \abs{f(x_{1})-f(x_{2})} < \varepsilon \text{ for all } x_{1},x_{2} > M.
    \end{equation}
\end{enumerate}

\begin{theorem}[\eax{Cauchy test for convergence of integrals}]
    Let $f \in \cR[a,\infty)$. Then $\int_{a}^{\infty} f$ converges if and only if for every $\varepsilon > 0$, there exists $M > 0$ such that
    \begin{equation*}
        \abs{\int_{R_{1}}^{R_{2}} f} < \varepsilon \text{ for all } R_{1},R_{2} > M.
    \end{equation*}
\end{theorem}
\begin{proof}
    We can easily see that $\int_{a}^{\infty} f$ converges $\iff$ $\lim_{R \to \infty} \int_{a}^{R} f$ converges $\iff$ for every $\varepsilon > 0$, there exists $M > 0$ such that
    \begin{equation}
        \abs{\int_{a}^{R_{1}} f - \int_{a}^{R_{2}} f} < \varepsilon \text{ for all } R_{1},R_{2} > M.
    \end{equation}
    But the left hand side is just $\int_{R_{1}}^{R_{2}} f$.
\end{proof}

\begin{theorem}
    Let $\int_{a}^{b} f$ be an improper integral at $b$. Then $\int_{a}^{b} f$ exists if and only if for all $\varepsilon > 0$, there exists $\delta > 0$ such that $a < b - \delta$ and
    \begin{equation*}
        \abs{\int_{x_{1}}^{x_{2}} f} < \varepsilon \text{ for all } x_{1},x_{2} \in (b-\delta,b).
    \end{equation*}
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{theorem}[\eax{Absolute convergence test}]
    Let $\varphi \in \cB[a,\infty) \cap \cR[a,\infty)$. If $\int_{a}^{\infty} f$ is absolutely convergent, then $\int_{a}^{\infty} \varphi f$ is also absolutely convergent.
\end{theorem}
\begin{proof}
    Set $M$ to be $\sup_{x \in [a,\infty)} \abs{\varphi(x)}$. Then, we must have $\abs{(\varphi f)(x)} \leq M\abs{f(x)}$ for all $x \in [a,\infty)$. By the limit comparison test, we are done.
\end{proof}

\section{The Mean Value Theorems}
\begin{theorem}[The \eax{first mean value theorem}]
    Let $f,g \in \cR[a,b]$ and suppose that $f$ keeps the same sign over $[a,b]$. Then there exists $\zeta \in [\inf g, \sup g]$ such that
    \begin{equation*}
        \int_{a}^{b} fg = \zeta \int_{a}^{b} f.
    \end{equation*}
\end{theorem}
\begin{remark} Before proving the first mean value theorem, we have a remark.
    \begin{enumerate}
        \item If $g \in \cC[a,b]$, then $\int_{a}^{b} fg = g(c) \int_{a}^{b} f$ for some $c \in [a,b]$.
        \item If $f \equiv 1$, and $g \in \cC[a,b]$, then there exists $c \in [a,b]$ such that
        \begin{equation*}
            g(c) = \frac{1}{b-a} \int_{a}^{b} g.
        \end{equation*}
    \end{enumerate}
\end{remark}
\begin{proof}
    Let $m = \inf_{[a,b]} g$ and let $M = \sup_{[a,b]} g$. Thus, $m \leq g(x) \leq M$ for all $x \in [a,b]$. Without loss of generality, assume that $f > 0$. The inequality implies that
    \begin{align}
        mf(x) \leq f(x) g(x) \leq M f(x) \implies m\int_{a}^{b} f \leq \int_{a}^{b} fg \leq M \int_{a}^{b} f.
    \end{align}
    Thus, there exists a $\zeta \in [m,M]$ such that $\int_{a}^{b} fg = \zeta \int_{a}^{b} f$.
\end{proof}

\begin{example}
    Let $r \in (0,1)$. Then,
    \begin{align}
        \frac{\pi}{6} \leq \int_{0}^{\frac{1}{2}} \frac{dx}{\sqrt{(1-x^{2})(1-rx^{2})}} \leq \frac{\pi}{6} \frac{1}{\sqrt{1-\frac{r}{4}}}.
    \end{align}
    To show this, let $f(x) = \frac{1}{\sqrt{1-x^{2}}}$ and let $g(x) = \frac{1}{\sqrt{1-rx^{2}}}$ for all $x \in [0,\frac{1}{2}]$. Clearly, $f,g \in \cC[0,\frac{1}{2}]$ and $f > 0$. Let us denote $\int_{0}^{\frac{1}{2}} fg$ by $I$. So, by the first mean value theorem, $I = g(c) \int_{0}^{\frac{1}{2}} f = g(c) \frac{\pi}{6}$ for some $c \in [0,\frac{1}{2}]$. For $c \leq \frac{1}{2}$, notice that $c^{2} \leq \frac{1}{4} \implies rc^{2} \leq \frac{r}{4}$ which will give us $g(c) \leq \frac{1}{\sqrt{1-\frac{r}{4}}}$. For the left inequality, we observe $0 \leq f \leq fg \implies \frac{\pi}{6} \leq \int_{0}^{\frac{1}{2}} fg = I$.
\end{example}

\textit{February 12th.}
\begin{lemma}[\eax{Abel's lemma}]
    Fix $\alpha, \beta \in \R$ and let $\{\omega_{j}\}_{j=1}^{n} \subseteq \R$. If
    \begin{equation*}
        \alpha \leq \sum_{j=1}^{m} \omega_{j} \leq \beta \text{ for all } m = 1,2,\ldots,n,
    \end{equation*}
    then for all decreasing $a_{1} \leq a_{2} \leq \ldots \leq a_{n} \leq 0$, we have
    \begin{equation*}
        a_{1}\alpha \leq \sum_{j=1}^{n} a_{j}\omega_{j} \leq a_{1}\beta.
    \end{equation*}
\end{lemma}
\begin{proof}
    Set $S_{m} = \sum_{j=1}^{m} \omega_{j}$. We are given that $\alpha \leq S_{m} \leq \beta$ for all $m = 1,2,\ldots,n$. Thus,
    \begin{align}
        \sum_{j=1}^{n} a_{j}\omega_{j} &= a_{1}S_{1} + a_{2}(S_{2}-S_{1}) + \ldots + a_{n}(S_{n}-S_{n-1}) \notag \\
        &= (a_{1}-a_{2})S_{1} + (a_{2}-a_{3})S_{2} + \ldots + (a_{n-1}-a_{n})S_{n-1} + a_{n}S_{n} \notag \\
        &\leq \beta(a_{1}-a_{2}+a_{2}-a_{3} + \ldots -a_{n}+a_{n}) = \beta a_{1} \text{ and }\\
        &\geq \alpha(a_{1}-a_{2}+a_{2}-a_{3} + \ldots -a_{n}+a_{n}) = \alpha a_{1}.
    \end{align}
    Thus, we have shown both the inequalities.
\end{proof}
\begin{theorem}[The \eax{Bonnet form of the second mean value theorem}]
    Let $f, \varphi \in \cR[a,b]$. Assume that $\varphi \geq 0$ and monotonically decreasing. Then there exists $\zeta \in [a,b]$ such that
    \begin{equation*}
        \int_{a}^{b} \varphi f = \varphi(a) \int_{a}^{\zeta} f.
    \end{equation*}
\end{theorem}
\begin{proof}
    Fix a partition $P \in \cP[a,b]$ defined as $P:a=x_{0} < x_{1} < \ldots < x_{n} = b$. Consider a tag set $\{\zeta_{j}\}_{j \in \{1,\ldots,n\}}$ of $P$. Assume that $a = \zeta_{1}$. Now,
    \begin{align}
        m_{j}(x_{j}-x_{j-1}) &\leq \int_{x_{j-1}}^{x_{j}} f \leq M(x_{j}-x_{j-1}) \text{ for all } j \text{ and } \\
        m_{j}(x_{j}-x_{j-1}) &\leq f(\zeta_{j}) \abs{I_{j}} \leq M(x_{j}-x_{j-1}) \text{ for all } j.
    \end{align}
    These equations imply
    \begin{align}
        \sum_{j=1}^{t} m_{j}\abs{I_{j}} &\leq \int_{a}^{x_{t}} f \leq \sum_{j=1}^{t} M_{j}\abs{I_{j}} \text{ for all } t \text{ and }\\
        \sum_{j=1}^{t} m_{j}\abs{I_{j}} &\leq \sum_{j=1}^{t} f(\zeta_{j}) \abs{I_{j}} \leq \sum_{j=1}^{t} M_{j} \abs{I_{j}} \text{ for all } t \\
        \implies \abs{\int_{a}^{x_{t}} f - \sum_{j=1}^{t} f(\zeta_{j}) \abs{I_{j}}} &\leq \sum_{j=1}^{t} (M_{j}-m_{j}) \abs{I_{j}} \text{ for all } t = 1,\ldots,n \notag \\
        \implies \abs{\int_{a}^{x_{t}} f - \sum_{j=1}^{t} f(\zeta_{j})\abs{I_{j}}} &\leq \osc_{P} f \\
        \implies \int_{a}^{x_{t}} f - \osc_{P}f &\leq \sum_{j=1}^{t} f(\zeta_{j}) \abs{I_{j}} \leq \int_{a}^{x_{t}} f + \osc_{P}f.
    \end{align}
    Recall that the map $F:[a,b] \to \R$ defined by $x \mapsto \int_{a}^{x} f$ is continuous on $[a,b]$. Set $\delta_{1} = \inf_{[a,b]} F$ and set $\delta_{2} = \sup_{[a,b]} F$. Thus,
    \begin{align}
        \delta_{1} - \osc_{P}f \leq \sum_{j=1}^{t} f(\zeta_{j})\abs{I_{j}} \leq \delta_{2} + \osc_{P}f.
    \end{align}
    Set $\omega_{j} = f(\zeta_{j}) \abs{I_{j}}$ and $a_{j} = \varphi(\zeta_{j})$. for all $j = 1,\ldots,n$. By assumption, $a_{1} \geq a_{2} \geq \ldots \geq a_{n} \geq 0$. By Abel's lemma,
    \begin{align}
        \varphi(\alpha)(\delta_{1}-\osc_{P}f) \leq \sum_{j=1}^{n} \omega_{j} a_{j} \leq \varphi(a)(\delta_{2} + \osc_{P}f).
    \end{align}
    As $\norm{P} \to 0$, we shall have
    \begin{align}
        \varphi(a)\delta_{1} &\leq \int_{a}^{b} f \varphi \leq \varphi(a) \delta_{2} \\
        \implies \delta_{1} &\leq \frac{1}{\varphi(a)} \int_{a}^{b} f\varphi \leq \delta_{2}.
    \end{align}
    Recall that the $\delta_{1}$ and $\delta_{2}$ are simply the infimum and supremum of the continuous function $F$ over $[a,b]$, so there exists a $\zeta \in [a,b]$ such that $F(\zeta)$ equals the middle value, or,
    \begin{equation}
        \varphi(a) \int_{a}^{\zeta} f = \int_{a}^{b} \varphi f.
    \end{equation}
\end{proof}

\begin{theorem}[The \eax{Weierstrass form of the second mean value thoerem}]
    Let $f,\varphi \in \cR[a,b]$ and suppose that $\varphi$ is montonic. Then there exists a $\zeta \in [a,b]$ such that
    \begin{equation*}
        \int_{a}^{b} \varphi f = \varphi(a) \int_{a}^{\zeta} f + \varphi(b) \int_{\zeta}^{b} f.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let, without loss of generality, $\varphi$ be increasing. Set $\tilde{\varphi}(x) = \varphi(b)-\varphi(x)$ for all $x \in [a,b]$. By Bonnet form of the second mean value theorem,
    \begin{align}
        \int_{a}^{b} \tilde{\varphi}f &= \tilde{\varphi}(a) \int_{a}^{\zeta} f \text{ for some } \varphi \in [a,b] \\
        \implies \varphi(b) \int_{a}^{b} f - \int_{a}^{b} \varphi f &= (\varphi(b) - \varphi(a)) \int_{a}^{\zeta} f \notag \\
        \implies \int_{a}^{} \varphi f &= \varphi(a) \int_{a}^{\zeta} f + \varphi(b) \int_{\zeta}^{b} f.
    \end{align}
\end{proof}

\section{More Tests for Improper Integrals of Type II}
\begin{theorem}[\eax{Abel's test}]
    Let $\varphi \in \cB[a,\infty) \cap \cR[a,\infty)$ be a monotonic function. If $\int_{a}^{\infty} f$ converges, then $\int_{a}^{\infty} \varphi f$ also converges.
\end{theorem}
\begin{proof}
    Let $R_{2} > R_{1} > a$. By the second mean value theorem,
    \begin{align}
        \int_{R_{1}}^{R_{2}} \varphi f = \varphi(R_{1}) \int_{R_{1}}^{\zeta} f + \varphi(R_{2}) \int_{\zeta}^{R_{2}} f
    \end{align}
    for some $\zeta(R_{1},R_{2}) \in [R_{1},R_{2}]$. Let $M = \sup_{[a,\infty)} \abs{\varphi}$. Fix $\varepsilon > 0$. As $\int_{a}^{\infty} f$ converges, there exists a real $R_{0} > 0$ such that
    \begin{equation}
        \abs{\int_{B_{1}}^{B_{2}} f} < \frac{\varepsilon}{2M} \text{ for all } B_{1},B_{2} > R_{0}.
    \end{equation}
    Thus, for all $R_{1},R_{2} > R_{0}$, we have
    \begin{equation}
        \abs{\int_{R_{1}}^{R_{2}} \varphi f} = \abs{\varphi(R_{1})\int_{R_{1}}^{\zeta} + \varphi(R_{2})\int_{\zeta}^{R_{2}}f} \leq M \frac{\varepsilon}{2M} + M \frac{\varepsilon}{2M} = \varepsilon.
    \end{equation}
    Hence, by the Cauchy criterion, $\int_{a}^{\infty} \varphi f$ converges.
\end{proof}

\textit{February 24th.}

\begin{theorem}[\eax{Dirichlet test}]
    Let $\varphi \in \cB[a,\infty)$ be a monotonic function and let $\lim_{x \to \infty} \varphi(x) = 0$. Suppose $f \in \cR[a,\infty)$ and $x \mapsto \int_{a}^{x}f$ is bounded on $[a,\infty)$. Then, $\int_{a}^{\infty} \varphi f$ converges.
\end{theorem}
\begin{proof}
    Define $M$ to be $\sup_{x \in [a,\infty)} \abs{\int_{a}^{x} f}$. For $\varepsilon \to 0$, as $\{\varphi(x) \to 0 \text{ as } x \to \infty\}$, there exists $m_{0} \in \R$ such that
    \begin{equation}
        \abs{\varphi(x)} < \frac{\varepsilon}{4M} \text{ for all } x \geq m_{0}.
    \end{equation}
    Therefore, for all $R_{1},R_{2} > m_{0}$, there exists a $\zeta$ in between $R_{1}$ and $R_{2}$ such that 
    \begin{align}
        \int_{R_{1}}^{R_{2}} \varphi f &= \varphi(R_{1}) \int_{R_{1}}^{\zeta} f + \varphi(R_{2}) \int_{\zeta}^{R_{2}} f \\
        \implies \abs{\int_{R_{1}}^{R_{2}} \varphi f} &\leq \abs{\varphi(R_{1})} \abs{\int_{R_{1}}^{\zeta} f} + \abs{\varphi(R_{2})} \abs{\int_{\zeta}^{R_{2}} f} < \frac{\varepsilon}{4M} \left( \int_{R_{1}}^{\zeta} f + \int_{\zeta}^{R_{2}} f \right) \notag \\
        &< \frac{\varepsilon}{4M} \left( \abs{\int_{a}^{\zeta}f} + \abs{\int_{a}^{R_{1}} f} + \abs{\int_{a}^{R_{2}} f} + \abs{\int_{a}^{\zeta} f} \right) < \varepsilon \text{ for all } R_{1},R_{2} > m_{0}.
    \end{align}
    By Cauchy criterion, $\int_{a}^{\infty} \varphi f$ converges.
\end{proof}

\begin{example}
    We wish to determine whether $\int_{1}^{\infty} \frac{1}{x} \sin x \log x dx$ converges. Here, we shall let $f(x) = \sin x$ and $\varphi(x) = \frac{\log x}{x}$. We know that $\varphi$ is decreasing monotonically to $0$ as $x \to \infty$. Also, $\abs{\int_{1}^{x} \sin t dt} = \abs{\cos x - \cos 1} \leq 2$. Thus, $x \mapsto \int_{1}^{x} \sin t$ is uniformly bounded tells us that, by the Dirichlet test, the integral converges.
\end{example}

\begin{example}
    We determine the $p$ for which $\int_{1}^{\infty} \frac{\sin x}{x^{p}} dx$ converges. We let $f(x) = \sin x$ and $\varphi(x) = \frac{1}{x^{p}}$ and focus only on the case when $p > 0$. Notice that $\varphi$ monotonically decreases to $0$ as $x \to \infty$. Therefore, $\int_{1}^{\infty} \frac{\sin x}{x^{p}}$ converges for all $p > 0$.

    Note that since $\int_{0}^{1} \frac{\sin x}{x} dx$ converges, $\int_{0}^{\infty} \frac{\sin x}{x} dx$ also converges. By the comparison test, $\int_{1}^{\infty} \frac{\sin x}{x^{p}} dx$ is absolutely convergent for all $p > 1$. We question whether the integral absolutely converges for $0 < p \leq 1$.

    Let us fix $0 < p \leq 1$. Now, $\abs{\sin x} \geq \sin^{2}x$ for all $x \in \R$. Therefore,
    \begin{align}
        \abs{\frac{\sin x}{x^{p}}} \geq \frac{\sin^{2}x}{x^{p}} = \frac{1-\cos 2x}{2x^{p}} = \frac{1}{2x^{p}} - \frac{\cos 2x}{2x^{p}}.
    \end{align}
    But the left term on the right hand side converges if and only $p > 1$. Thus, $\int_{1}^{\infty} \abs{\frac{\sin x}{x^{p}}}dx$ diverges for $0 < p \leq 1$. We provide a more slick proof of this.\\

    Set $f(x) = \frac{\sin x}{x}$. We claim that $\int_{0}^{\infty} \abs{f}$ is not convergent. Fix a natural $n$. Therefore,
    \begin{align}
        \int_{0}^{n\pi} \abs{f} = \int_{0}^{\pi} f + \int_{\pi}^{2\pi} f + \ldots + \int_{(n-1)\pi}^{n\pi} f.
    \end{align}
    For all $1 \leq m \leq n$,
    \begin{align}
        \int_{(m-1)\pi}^{m\pi} \abs{f} = \int_{(m-1)\pi}^{m\pi} \frac{\abs{\sin x}}{x} dx = \int_{0}^{\pi} \frac{\sin x}{(m-1)\pi + x}dx
    \end{align}
    Now for all $x \in [0,\pi]$, $(m-1)\pi + x \leq m\pi \implies \frac{1}{(m-1)\pi+x} \geq \frac{1}{m\pi}$
    \begin{align}
        \implies \int_{(m-1)\pi}^{m\pi} \abs{f} \geq \int_{0}^{\pi} \frac{\sin x}{m \pi} dx = \frac{2}{m \pi} \text{ for all } m = 1,2, \ldots, n.
    \end{align}
    Therefore,
    \begin{align}
        \int_{0}^{\pi} \abs{f} \geq \frac{2}{\pi} \sum_{m=1}^{n} \frac{1}{m}.
    \end{align}
    This tells us that $\lim_{n \to \infty} \int_{0}^{n \pi} \abs{f} = \infty$.
\end{example}

\begin{example}
    Consider the function $f:[0,1] \to \R$ defined as
    \begin{align}
        f(x) = \begin{cases}
            0 &\text{ if } x = 0,\\
            (-1)^{n+1}(n+1) &\text{ if } x \in (\frac{1}{n+1},\frac{1}{n}] \text{ for some } n \in \N.
        \end{cases}
    \end{align}
    Clearly, $\int_{0}^{1} \abs{f}$ and $\int_{0}^{1} f$ are improper integrals of type I at 0. Pick $0 < \varepsilon < 1$. Let $n$ be a natural number such that $\frac{1}{n+1} < \varepsilon \leq \frac{1}{n}$. Thus,
    \begin{align}
        \int_{\varepsilon}^{1} \abs{f} &= \int_{\varepsilon}^{\frac{1}{n}} \abs{f} + \int_{\frac{1}{n}}^{\frac{1}{n-1}} \abs{f} + \ldots + \int_{\frac{1}{3}}^{\frac{1}{2}} \abs{f} + \int_{\frac{1}{2}}^{1} \abs{f} \\
        &= (n+1)\left( \frac{1}{n}-\varepsilon \right) + n\left( \frac{1}{n-1} - \frac{1}{n}  \right) + \ldots + 3 \left( \frac{1}{2}-\frac{1}{3} \right) + 2\left(1-\frac{1}{2}\right) \notag \\
        &= (n+1) \left( \frac{1}{n}-\varepsilon \right) + \sum_{m=1}^{n-1} \frac{1}{m} > \sum_{m=1}^{n-1} \frac{1}{m}.
    \end{align}
    As $\varepsilon \to 0^{+}$, $n \to \infty$ implies that $\int_{0}^{1} \abs{f}$ diverges. Again,
    \begin{align}
        \int_{\varepsilon}^{1} f &= \int_{\varepsilon}^{\frac{1}{n}} (-1)^{n+1}(n+1)dx + \int_{\frac{1}{n}}^{\frac{1}{n-1}} (-1)^{n} n dx + \ldots + \int_{\frac{1}{2}}^{1} 2 dx \\
        &= (-1)^{n+1}(n+1)\left( \frac{1}{n}-\varepsilon \right) + \sum_{m=1}^{n-1} \frac{(-1)^{m+1}}{m} \notag \\
        \implies \abs{\int_{\varepsilon}^{1} f - \sum_{m=1}^{n-1} \frac{(-1)^{m-1}}{m}} &= (n+1)\left( \frac{1}{n}-\varepsilon \right) < (n+1) \left( \frac{1}{n}-\frac{1}{n+1} \right) = \frac{1}{n} \\
        \implies \int_{0}^{1} f &= \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}.
    \end{align}
\end{example}

\begin{theorem}[\eax{Cauchy-Maclaurin test}]
    Let $f:[1,\infty) \to \R$ be a decreasing function with $f > 0$. Then the improper integral $\int_{1}^{\infty} f$ converges if and only if the the infinite series $\sum_{n=1}^{\infty} f(n)$ converges.
\end{theorem}


\section{The Gamma Function}
Recall that for all natural $n \geq 1$, $n!$ is defined as $n \cdot (n-1) \cdots 2\cdot1$. We wish to extend this notation of the factorial for all $x \in \R$. We know that
\begin{align}
    \int_{0}^{\infty} t^{n} e^{-t} dt = n! \text{ for all natural } n \geq 1.
\end{align}
Thus, we propose this as the extended definition.
\begin{definition}
    For all $x > 0$, the \eax{Gamma function} $\Gamma$ is defined as
    \begin{align}
        \Gamma(x) = \int_{0}^{\infty} t^{x-1}e^{-t} dt.
    \end{align}
\end{definition}


\textit{February 28th.}
\begin{theorem}
    $\Gamma(x)$ converges for all $x > 0$.
\end{theorem}
\begin{proof}
    Rewrite the function as $\Gamma(x) = \int_{0}^{1}t^{x-1}e^{-t}dt + \int_{1}^{\infty} t^{x-1}e^{-t} dt$. The two integrals are termed as $\gamma_{1}(x)$ and $\gamma_{2}(x)$ respectively. We first prove the convergence of $\gamma_{1}(x)$.

    For $0 \leq t \leq 1$, $e^{-t} \leq 1$ holds. Note that $t^{x-1}e^{-t}$ is dominated by $t^{x-1}$, and the integral $\int_{0}^{1} t^{x-1}dt$ converges to $\frac{1}{x}$ for $x > 0$. Thus, $\gamma_{1}(x)$ converges.

    To show the convergence of $\gamma_{2}(x)$, we first note that $\lim_{t \to \infty} t^{x+1}e^{-t} = 0$. Thus, there must exist some $M > 0$ such that $t^{x+1}e^{-t} < 1$ holds for all $t \geq M$, which implies that $t^{x-1}e^{-t} < \frac{1}{t^{2}}$. As $\int_{1}^{\infty} \frac{1}{t^{2}}$ converges, $\gamma_{2}(x)$ must also converge by the comparison test.
\end{proof}

\begin{theorem}
    $\Gamma(x+1) = x \Gamma(x)$ for all $x > 0$.
\end{theorem}
\begin{proof}
    For all $r_{1},r_{2} > 0$, $\int_{r_{1}}^{r_{2}} t^{x-1} e^{-t} dt$ represents $\Gamma(x)$ as $r_{2}$ tends to infinity and $r_{1}$ tends to 0. Using the method of intergration by parts, we have
    \begin{align}
        \int_{r_{1}}^{r_{2}} t^{x} e^{-t} dt = \left[-t^{x}e^{-t}  \right]_{t=r_{1}}^{r_{2}} + x \int_{r_{1}}^{r_{2}} t^{x-1} e^{-t} dt = (r_{1}^{x}e^{-r_{1}}-r_{2}^{x}e^{-r_{2}}) + x\int_{r_{1}}^{r_{2}}t^{x-1}e^{-t}dt.
    \end{align}
    The left term on the right hand side tends to $0$ as $r_{1}$ tends to 0 and $r_{2}$ tends to infinity, and the integral tends to $x\Gamma(x)$. The left hand side tends to $\Gamma(x+1)$. Thus, $\Gamma(x+1) = x\Gamma(x)$.
\end{proof}

\section{Cauchy's Principle Value}
\begin{definition}
    Let $f:\R \to \R$ be a function. Then \eax{Cauchy's principle value} for $\int_{-\infty}^{\infty}f$ is defined as $\lim_{R \to \infty} \int_{-R}^{R} f$, if the limit exists.
\end{definition}
If the indefinite integral exists, then Cauchy's principle value exists and equals the indefinite integral. However, the converse is not true.

\begin{example}
    Let us look at the function $f:\R \to \R$ defined as $f(x) = \frac{x}{1+x^{2}}$ for all real $x$. Cauchy's principle value is given as
    \begin{align}
        \int_{-R}^{R} f &= \frac{1}{2}\int_{-R}^{R} \frac{d(1+x^{2})}{1+x^{2}} = \frac{1}{2} (\log (1+R^{2}) - \log (1+R^{2})) = 0 \text{ for all } R > 0 \notag \\
        \implies \lim_{R \to \infty} \int_{-R}^{R} f &= 0.
    \end{align}
    However, the indefinite integral does not converge, and does not exist.
\end{example}


\chapter{SEQUENCE OF FUNCTIONS}

Recall that a sequence of reals $\{x_{n}\} \subseteq \R$ converges to a value $x \in \R$ if, for every $\varepsilon > 0$, there exists $N \in \N$ such that
\begin{align}
    \abs{x_{n}-x} < \varepsilon \text{ for all } n \geq N.
\end{align}
The Cauchy criterion is also equivalent. We now define some notation. $S$ will represent a subset of $\R$, $S \subseteq \R$, and $\cF(S)$ will represent the set of all functions acting on $S$, $\cF(s) = \{f:S \to \R\}$. Our goal here is to replace the real points $x_{n}$ by functions $f_{n} \in \cF(S)$. To begin, we must define a `distance' between functions in $\cF(S)$. This can be done in two ways.

Pick a sequence $\{f_{n}\}_{n=1}^{\infty} \subseteq \cF(S)$. Then for all $x \in S$, note that $\{f_{n}(x)\} \subseteq \R$. Thus, we bring back the absolute function $\abs{.}$ and introduce it to the sequence $\{f_{n}(x)\}$ as our `distance'. This is the first way. The second way, which is harder, is done by introducing an original definition for $\abs{.}$ acting on functions and use the same definition.

\section{Convergence in Sequence of Functions}
\begin{definition}[The \eax{pointwise convergence of a sequence of functions}]
    Let $\{f_{n}\} \subseteq \cF(S)$ and $f \in \cF(S)$. We say that $f_{n}$ converges to $f$ pointwise if for each $x \in S$, $f_{n}(x)$ converges to $f(x)$ ($\star$). Thus, we will use $\abs{f_{n}-f} < \varepsilon$ to denote ($\star$).
\end{definition}
Note that ($\star$) is equivalent to saying that for every $\varepsilon > 0$, there exists a natural $N$ such that $\abs{f_{n}(x) - f(x)} < \varepsilon \text{ for all } n \geq N$. But here, $N$ is both a function of $\varepsilon$ and $x$ which poses a problem.

\begin{example}
    Let $S=[0,1]$ and define $f_{n}:S \to \R$ to be $f_{n}(x) = x^{n}$ for all natural $n$ and all $x \in S$. Then for all $x \in [0,1)$, $f_{n}(x) = x^{n}$ converges to 0 as $n$ tends to infinity, and for $x = 1$, $f_{n}(x) = 1$ converges to 1 as $n$ tends to infinity. If we set $f:S \to \R$ defined as
    \begin{align}
        f(x) = \begin{cases}
            0 &\text{ if } 0 \leq x < 1,\\
            1 &\text{ if } x = 1.
        \end{cases}
    \end{align}
    Then $f_{n}$ will converge to $f$ pointwise. Note that $f_{n}$ is both a continuous and infinite differentiable function for all natural $n$, but $f$ is not even a continuous function. We see that there is a need for a better definition of convergence of sequence of functions.
\end{example}

\begin{example}
    For $S = [0,\infty)$, define $f_{n}:S \to \R$ as $f_{n}(x) = \frac{1}{x+n}$. As $\frac{1}{x+n} \leq \frac{1}{n}$, we find that $f_{n}(x)$ converges to $0$ as $n$ tends to infinity for all $x \in [0,\infty)$. Thus, $f_{n}$ converges to the zero function $f$, pointwise. However, for all $x \in [0,\infty)$,
    \begin{align}
        \abs{f_{n}(x) - 0} &= \frac{1}{x+n} < \frac{1}{n} \notag \\
        \implies \abs{f_{n}(x) - f(x)} &< \varepsilon \text{ for all } n > \frac{1}{\varepsilon}
    \end{align}
    implies that the choice of $N$ is independent of $x$. This gives us an idea for the new definition of convergence.
\end{example}

\begin{definition}[The \eax{uniform convergence of a sequence of functions}]
    Let $\{f_{n}\} \subseteq \cF(S)$ and $f \in \cF(S)$. We say $\{f_{n}\}$ converges to $f$ uniformly if for every $\varepsilon > 0$, there exists a natural $N$ such that
    \begin{align}
        \abs{f_{n}(x)-f(x)} < \varepsilon \text{ for all } n \geq N \text{ and } x \in S.
    \end{align}
\end{definition}
\begin{remark}
    Note that the condition here is equivalent to saying that $f(x) - \varepsilon \leq f_{n}(x) \leq f(x) + \varepsilon$ for all $n \geq N$ and $x \in S$.
\end{remark}

\begin{example}
    It can be shown that $\{x^{n}\} \subseteq \cF([0,1])$ is not uniformly convergent, but the sequence $\{\frac{1}{x+n}\} \subseteq \cF([0,\infty))$ uniformly converges.
\end{example}

\begin{remark}
    The uniform convergence of $f_{n}$ to $f$ implies the pointwise convergence of $f_{n}$ to $f$.
\end{remark}

Despite these definitions, it is unclear what our notion of distance even is.

\begin{definition}
    $\cB(S)$ denotes the set of all bounded functions from $S$ to $\R$, that is, $\cB(S) = \{f:S \to \R;f \text{ is bounded}\}$.
\end{definition}
We may note that $\cB(S)$ is a vector space, and even an algebra. We will make use of this fact to define the distance.

\begin{definition}
    For all $f \in \cB(S)$, we define the \eax{norm of a function} $f$ by $\norm{f} = \sup_{x \in S} \abs{f(x)}$.
\end{definition}

\begin{remark}
    We note that $\norm{.}$ is a function $\norm{.} : \cB(S) \to \R_{\geq 0}$. It also satisfies the properties of the distance;
    \begin{itemize}
        \item $\norm{f} = 0$ if and only if $f \equiv 0$,
        \item $\norm{f+g} \leq \norm{f} + \norm{g}$,
        \item $\norm{\alpha f} = \abs{\alpha} \norm{f}$,
        \item $\norm{fg} \leq \norm{f}\norm{g}$.
    \end{itemize}
    The non-negativity and the first three properties are reminiscent of the absolute value function defined on the reals. The last property, however, involves an inequality and the norm function is termed \eax{submultiplicative} due to this. 
\end{remark}

\begin{definition}
    For all $f,g \in \cB(S)$, we define the \eax{distance between two functions} $f$ and $g$ as $d(f,g) = \norm{f-g}$.
\end{definition}
\begin{remark}
    Again, we note that $d$ is a function $d:\cB(S) \times \cB(S) \to R_{\geq 0}$. Other properties include
    \begin{itemize}
        \item $d(f,g) = \sup_{x \in S} \abs{f(x)-g(x)}$,
        \item $d(f,g) \leq d(f,h) + d(h,g)$ for all $h \in \cB(S)$,
        \item $d(f,g) = 0$ if and only if $f = g$.
    \end{itemize}
\end{remark}
We modify our definition of uniform convergence as an equivalent statement.

\begin{remark}
    Let $\{f_{n}\} \subseteq \cF(S)$ and $f \in \cF(S)$. $f_{n}$ converges to $f$ uniformly on $S$ if for every $\varepsilon > 0$, there exists a natural $N$ such that
    \begin{align}
        \norm{f_{n}-f} < \varepsilon \text{ for all } n \geq N.
    \end{align}
    This statement is equivalent to the previous definition.
\end{remark}
\textit{March 3rd.}\\
The following example depicts a useful trick, where we truncate that `bad part' of the function to give us a sequence of functions that we intuitively know converges to the desired function.

\begin{example}
    Let $S = [-1,1]$. Define $\{f_{n}\} \subseteq \cF(S)$ by
    \begin{equation}
        f_{n}(x) = \begin{cases}
            \frac{1}{n} &\text{ if } \abs{x} \leq \frac{1}{n},\\
            \abs{x} &\text{ if } \frac{1}{n} < \abs{x} \leq 1.
        \end{cases}
    \end{equation}
    Note that $f_{n}$ converges to $f$ pointwise, where $f(x) = \abs{x}$. This is not too hard to show. We question the uniform convergence. Computing the norm gives us $\norm{f_{n}-f} = \sup_{x \in [-1,1]} \abs{f_{n}(x)-f(x)} = \frac{1}{n}$, where the supremum is achieved at $x = 0$. Clearly, this can be minimized. Thus, $f_{n}$ also uniformly converges to $f$.
\end{example}

\begin{theorem}[The \eax{Cauchy criterion for a sequence of functions}]
    Let $\{f_{n}\} \subseteq \cF(S)$. Then $\{f_{n}\}$ converges uniformly if and only if for every $\varepsilon > 0$, there exists a natural $N \in \N$ such that
    \begin{align*}
        \norm{f_{n}-f_{m}} < \varepsilon \text{ for all } n,m \geq N.
    \end{align*}
\end{theorem}
\begin{proof}
    Let $f_{n}$ uniformly converge to $f$. Then for every $\varepsilon > 0$, there exists $N \in \N$ such that $\norm{f_{n}-f} < \frac{\varepsilon}{2}$ for all $n \geq N$. Thus, for all $n,m \geq N$,
    \begin{align}
        \norm{f_{n}-f_{m}} = \norm{(f_{n}-f)-(f_{m}-f)} \leq \norm{f_{n}-f} + \norm{f_{m}-f} < \varepsilon.
    \end{align}
    For the converse, we have, for every $\varepsilon > 0$ and some $N \in \N$,
    \begin{align}
        \norm{f_{n}-f_{m}} &< \frac{\varepsilon}{2} \text{ for all } n,m \in \N \notag \\
        \implies \sup_{x \in S} \abs{f_{n}(x)-f_{m}(x)} &< \frac{\varepsilon}{2}
    \end{align}
    The last inequality is true for any evaluation at $x \in S$, so the sequence $\{f_{n}(x)\}$ is Cauchy for all $x \in S$. This implies that there exists a $f \in \cF(S)$ such that $\lim_{n \to \infty} f_{n}(x) = f(x)$ for all $x \in S$. This implies the pointwise convergence of $f_{n}$ to $f$. To show the uniform convergence, we `blow up' the $m$. We have
    \begin{align}
        f_{n}(x) - \frac{\varepsilon}{2} &\leq f_{m}(x) \leq f_{n}(x) + \frac{\varepsilon}{2} \text{ for all } x \in S,\; n,m \geq N \notag\\
        \implies f_{n}(x) - \frac{\varepsilon}{2} &\leq f(x) \leq f_{n}(x) + \frac{\varepsilon}{2} \notag \\
        \implies \abs{f_{n}(x)-f(x)} &< \frac{\varepsilon}{2} \text{ for all } x \in S,\; n \geq N \notag \\
        \implies \norm{f_{n}-f} &< \frac{\varepsilon}{2} \text{ for all } n \geq N. 
    \end{align}
\end{proof}

\begin{example}
    Let $\{r_{n}\}_{n=1}^{\infty}$ be an enumeration of $\Q \cap [0,1]$. For all $n \geq 1$, define $f_{n}:[0,1] \to \R$ by
    \begin{align}
        f_{n}(x) = \begin{cases}
            0 &\text{ if } x \in \{r_{1},r_{2},\ldots,r_{n}\},\\
            1 &\text{ if } x \notin \{r_{1},r_{2},\ldots,r_{n}\}.
        \end{cases}
    \end{align}
    It can be shown that $f_{n}$ converges to the Dirichlet function, pointwise. We question the uniform convergence. Note that for a fixed $n \in \N$, note that $f_{n}(r_{n+1}) = 1$ and $f_{n+1}(r_{n+1}) = 0$. Thus, $\norm{f_{n}-f_{n+1}} = 1$, which cannot be minimized. This violates the Cauchy criterion of uniform convergence.
\end{example}

\begin{theorem}[The \eax{M-test}]
    Let $f_{n}$ be a sequence of functions converging to $f$, pointwise. For all $n \in \N$, set $M_{n} = \sup_{x \in S} \abs{f_{n}(x)-f(x)}$. Then $f_{n}$ converges to $f$ uniformly if and only if $M_{n}$ converges to 0.
\end{theorem}
\begin{proof}
    This statement is trivial if you consider the fact that $M_{n} = \norm{f_{n}-f}$.
\end{proof}

\begin{example}
    For all $n \in \N$, define $f_{n}(x) = \frac{nx}{1+n^{2}x^{2}}$ on the interval $S = [0,1]$. Again, it can be shown with ease that $f_{n}$ converges to $f \equiv 0$, pointwise. We use the M-test to check the uniform convergence. Thus,
    \begin{align}
        M_{n} = \sup_{x \in [0,1]} \abs{\frac{nx}{1+n^{2}x^{2}} - 0} = \sup_{x \in [0,1]}\abs{f_{n}(x)} = \frac{1}{2} \text{ for all } n \in \N.
    \end{align}
    Certainly, $\frac{1}{2}$ does not converge to 0. The function does not uniformly converge.
\end{example}

\section{Properties of Limit Convergence}
We note that boundedness is not preserved under the notion of pointwise convergence; one may show by picking the example
\begin{align}
    f_{n}(x) = \begin{cases}
        0 &\text{ if } 0 \leq x \leq \frac{1}{n},\\
        \frac{1}{x} &\text{ if } \frac{1}{n} < x \leq 1.
    \end{cases}
\end{align}
Riemann integrability is also not preserved under pointwise convergence due to Example 5.16. The sequence $\{x^{n}\} \subseteq \cF([0,1])$ shows that continuity and differentiability are also not preserved in pointiwse convergence. In fact, due to the above sequence, not even limits are preserved! We infer that pointwise convergence is not a `good' form of convergence to study. We shift our focus to uniform convergence.

\begin{theorem}
    Let $x_{0} \in S$, and let $\{f_{n}\} \cup \{f\} \subseteq \cF(S \backslash \{x_{0}\})$. Suppose that $f_{n}$ converges uniformly to $f$. If $\lim_{x \to x_{0}} f_{n}(x)$ exists for all $n \in \N$, then $\lim_{x \to x_{0}} f(x)$ exists. Moreover,
    \begin{align*}
        \lim_{n \to \infty} \lim_{x \to x_{0}} f_{n}(x) = \lim_{x \to x_{0}} \lim_{n \to \infty} f_{n}(x).
    \end{align*}
\end{theorem}
\begin{proof}
    For every $\varepsilon > 0$, there exists $N \in \N$ such that $\norm{f_{n}-f_{m}} < \frac{\varepsilon}{2}$ for all $n,m \geq N$ on $S \backslash \{x_{0}\}$. For all $n \in \N$, let $a_{n} = \lim_{x \to x_{0}} f_{n}(x)$. Therefore, $\abs{a_{n}-a_{m}} = \lim_{x \to x_{0}} \abs{f_{n}(x)-f_{m}(x)}$; there exists $a \in \R$ such that
    \begin{align*}
        \lim_{n \to \infty} a_{n} = a \left( =\lim_{n \to \infty} \lim_{x \to x_{0}} f_{n}(x) \right) < \frac{\varepsilon}{2} \text{ for all } n,m \geq N.
    \end{align*}
    
    Now, there exists $n_{0} \in \N$ such that $\norm{f_{n}-f} < \frac{\varepsilon}{3}$ for all $n \geq n_{0}$ on $S \backslash \{x_{0}\}$. Also, there exists $\tilde{n}_{0} \in \N$ such that $\abs{a_{n}-a} < \frac{\varepsilon}{3}$ for all $n \geq \tilde{n}_{0}$. Let $\hat{n} = \max \{n_{0},\tilde{n}_{0}\}$. We know that $\lim_{x \to x_{0}} f_{\hat{n}}(x) = a_{\hat{n}}$ exists. Therefore, there exists $\delta > 0$ such that $\abs{f_{\hat{n}}(x)-a_{\hat{n}}} < \frac{\varepsilon}{3}$ for all $0 < \abs{x-x_{0}} < \delta$. Thus, combining these three, we have
    \begin{align*}
        \abs{f(x)-a} \leq \abs{f(x)-f_{\hat{n}}(x)} + \abs{f_{\hat{n}}(x)-a_{\hat{n}}} + \abs{a_{\hat{n}}-a} < \varepsilon \text{ for all } 0 < \abs{x-x_{0}} < \delta.
    \end{align*}
    This implies that $\lim_{x \to x_{0}} f(x) = a$.
\end{proof}

\textit{March 5th.}

\begin{corollary}
    Let $f_{n}$ converge uniformly to $f$ on $S$. Let $x_{0} \in S$ and suppose that $f_{n}$ is continuous at $x_{0}$ for all $n$. Then $f$ is continuous at $x_{0}$.
\end{corollary}
\begin{proof}
    Recall that $f_{n}(x_{0}) = \lim_{x \to x_{0}} f_{n}(x)$ for all $n$. The above theorem may be applied.
\end{proof}

\begin{remark}
    If $\{f_{n}\} \subseteq \cC[0,1]$ and $f_{n}$ converges to $f$, uniformly, then $f \in \cC[0,1]$.
\end{remark}

\begin{theorem}
    Let $\{f_{n}\} \subseteq \cB(S)$ and suppose that $f_{n}$ converges uniformly to $f$. Then $f \in \cB(S)$.
\end{theorem}
\begin{proof}
    For every $\varepsilon > 0$, there exists a natural $N$ such that $\norm{f_{n}-f} < \varepsilon$ for all $n \geq N$. Therefore, for all $x \in S$,
    \begin{align}
        \abs{f(x)} = \abs{f(x) - f_{N}(x) + f_{N}(x)} \leq \abs{f(x)-f_{N}(x)} + \abs{f_{N}(x)} &< \varepsilon + \norm{f_{N}} \\
        \implies \norm{f} &\leq \varepsilon + \norm{f_{N}}.
    \end{align}
    $f$ is bounded.
\end{proof}

\begin{corollary}
    If $\{f_{n}\} \subseteq \cB(S)$ and suppose that $f_{n}$ uniformly converges to $f$. Then $f_{n}$ is also uniformly bounded.
\end{corollary}
\begin{proof}
    Note that $\norm{f_{n}} \leq \norm{f_{n}-f} + \norm{f}$. We are done by the above theorem.
\end{proof}

\begin{theorem}
    Let $\{f_{n}\} \subseteq \cR[a,b]$, and suppose $f_{n}$ uniformly converges to $f$. Then $f \in \cR[a,b]$. Moreover,
    \begin{align*}
        \lim_{n \to \infty} \int_{a}^{b} f_{n} = \int_{a}^{b} \lim_{n \to \infty} f_{n}.
    \end{align*}
\end{theorem}
\begin{proof}
    For every $\varepsilon > 0$, there exists a natural $N$ such that $\norm{f_{N}-f} < \frac{\varepsilon}{b-a}$. This gives us $f(x) - \frac{\varepsilon}{b-a} < f_{N}(x) < f(x) + \frac{\varepsilon}{b-a}$ for all $x \in [a,b]$. Now, $f_{N} \in \cR[a,b]$ implies that there exists a partition $P \in \cP[a,b]$ such that $U(f_{N},P)-L(f_{N},P) < \varepsilon$. But for all $x \in [a,b]$, we have
    \begin{align}
        f(x) < f_{N}(x) + \frac{\varepsilon}{b-a} &\implies U(f,P) < U(f_{N},P) + \varepsilon,\\
        f(x) > f_{N}(x) - \frac{\varepsilon}{b-a} &\implies L(f,P) > L(f_{N},P) - \varepsilon.
    \end{align}
    Subtracting $L(f,P)$ from $U(f,P)$ and making use of the inequalities, we get
    \begin{align}
        U(f,P) - L(f,P) < U(f_{N},P) - L(f_{N},P) + 2\varepsilon < 3\varepsilon.
    \end{align}
    Thus, $f$ is Riemann integrable on $[a,b]$. We now show the commutativity of the limit and the integral. Note that since $f_{n}$ uniformly converges to $f$, for every $\varepsilon > 0$, there exists a natural $n_{0}$ such that $\norm{f_{n}-f} < \frac{\varepsilon}{b-a}$ for all $n \geq n_{0}$. For all $x \in [a,b]$, we have
    \begin{align}
        \abs{\int_{a}^{x}f-\int_{a}^{x}f_{n}} = \abs{\int_{a}^{x}(f-f_{n})} \leq \int_{a}^{x} \abs{f-f_{n}} < \frac{\varepsilon}{b-a}\abs{x-a} < \varepsilon.
    \end{align}
\end{proof}

However, despite so many properties being conserved, the following example talks about the derivative.

\begin{example}
    Define $\{f_{n}\} \subseteq \cF(\R)$ by $f_{n}(x) = \frac{x}{1+nx^{2}}$ for all $x \in \R$. For $x \neq 0$, we have
    \begin{align}
        \abs{f_{n}(x)} = \abs{\frac{x}{1+nx^{2}}} \leq \frac{1}{2\sqrt{n}}.
    \end{align}
    Thus, $f_{n}$ converges uniformly to the null function. We note that both $f_{n}$ and the zero funciton are differentiable. But there is a flaw here. Differentiating $f_{n}(x)$ gives us $f_{n}'(x) = \frac{-nx^{2}+1}{(1+nx^{2})^{2}}$. For $x \neq 0$, this derivative tends to 0 as $n$ tends to infinity. However, when $x = 0$, the derivative is (tends to) 1! We see that $f_{n}'$ converges to $F$ pointwise where $F(x) = 0$ for $x \neq 0$ and $F(0) = 1$. The limit and derivative do \textit{not} commute in general.
\end{example}

This example suggests that unlike the previous theorems, the theorem regarding differentiability will turn out to be slightly uglier.

\begin{theorem}
    Let $\{f_{n}\}$ be a sequence of differentiable functions on $[a,b]$. Suppose
    \begin{itemize}
        \item there exists $x_{0} \in [a,b]$ such that $\{f_{n}(x_{0})\}$ converges, and
        \item $\{f_{n}'\}$ converges uniformly on $[a,b]$.
    \end{itemize}
    Then $\{f_{n}\}$ converges uniformly on $[a,b]$ to a differentiable funciton $f$. Moreover,
    \begin{align*}
        f'(x) = \lim_{n \to \infty} f_{n}'(x) \text{ for all } x \in [a,b].
    \end{align*}
\end{theorem}
\begin{proof}
    For every $\varepsilon > 0$, there exists a natural $N$ such that both $\norm{f_{n}'-f_{m}'} < \frac{\varepsilon}{2(b-a)}$ and $\abs{f_{n}(x_{0}) - f_{m}(x_{0})} < \frac{\varepsilon}{2}$ hold true for all $n,m \geq N$. We now apply the mean value theorem to $f_{n}-f_{m}$; fix $x \in [a,b]\backslash\{x_{0}\}$ to get
    \begin{align}
        (f_{n}-f_{m})(x) = (f_{n}-f_{m})(x_{0}) + (x-x_{0})(f_{n}-f_{m})'(\zeta)
    \end{align}
    from some $\zeta$ between $x$ and $x_{0}$. This tells us that
    \begin{align}
        \abs{(f_{n}-f_{m})(x)} &\leq \abs{(f_{n}-f_{m})(x_{0})} + \abs{x-x_{0}} \abs{f_{n}'(\zeta)-f_{m}'(\zeta)} < \frac{\varepsilon}{2} + \abs{x-x_{0}}\frac{\varepsilon}{2(b-a)} < \varepsilon.
    \end{align}
    This tells us that $\norm{f_{n}-f_{m}} < \varepsilon$ for all $n,m \geq N$. Thus, $f_{n}$ converges uniformly to some function $f$ on $[a,b]$. We now claim that $f$ is differentiable and $f'(x) = \lim_{n \to \infty}f_{n}'(x)$ for all $x \in [a,b]$. Fix $\tilde{x} \in [a,b]$, and set $F_{n}(x) = \frac{f_{n}(x)-f_{n}(\tilde{x})}{x-\tilde{x}}$ and $F(x) = \frac{f(x)-f(\tilde{x})}{x-\tilde{x}}$ for all $x \neq \tilde{x}$. Note that $F_{n}$ converges to $F$, pointwise, and $\lim_{x \to \tilde{x}} F_{n}(x) = f_{n}'(\tilde{x})$ for all $n$.

    Again, by the mean value theorem, there exists $\zeta \in (x,\tilde{x})$ such that
    \begin{align}
        (f_{n}-f_{m})(x)-(f_{n}-f_{m})(\tilde{x}) &= (f_{n}'(\zeta)-f_{m}'(\zeta)) (\tilde{x}-x) \\
        \implies F_{n}(x) - F_{m}(x) &= f_{n}'(\zeta) - f_{m}'(\zeta) \notag \\
        \implies \abs{F_{n}(x)-F_{m}(x)} &< \frac{\varepsilon}{2(b-a)} \text{ for all } n,m \geq N \text{ and } x \neq \tilde{x} \notag \\
        \implies \norm{F_{n}-F_{m}} &< \frac{\varepsilon}{2(b-a)}.
    \end{align}
    Thus, $\{F_{n}\}$ is uniformly convergent on $[a,b]\backslash\{\tilde{x}\}$. This tells us that
    \begin{align}
        \lim_{x \to \tilde{x}}\lim_{n \to \infty} F_{n}(x) = \lim_{n \to \infty}\lim_{x \to \tilde{x}} F_{n}(x) \implies \lim_{x \to \tilde{x}} F(x) = f'(\tilde{x}) = \lim_{n \to \infty} f_{n}'(\tilde{x}).
    \end{align}
\end{proof}

\begin{example}
    Let $f_{n}(x) = \frac{\sin nx}{n}$ for $x \in [0,1]$. Note that $f_{n}$ converges to the zero function $(f)$, uniformly. Thus, $f'$ is also the zero function. But $f_{n}'(x) = \cos nx$ and $\{f_{n}'(x)\}$ does not converge for all $x \neq 0$.
\end{example}

\begin{appendices}

\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries}
  {\chaptername\ \thechapter}{20pt}{\Huge}

\titlespacing*{\chapter}{0pt}{20pt}{40pt}

\chapter{Appendix}
Extra content goes here.

\printindex

\end{appendices}

\end{document}