
\documentclass[15pt,a4paper]{book}

\usepackage{amsmath, amsthm, amssymb} 
\usepackage{graphicx} % For including graphics
\usepackage{hyperref} % For clickable links
\usepackage{bookmark} % Better control over bookmarks
\usepackage{geometry} % Customize page layout
\usepackage{xcolor} % Colors for text and graphics
\usepackage{enumitem} % Customizable lists
\usepackage{fancyhdr} % Header and footer
\usepackage{titlesec} % Custom section/chapter titles
\usepackage[toc,page]{appendix} % For the appendix
\usepackage{longtable} % For tables spanning multiple pages
\usepackage{mathrsfs} % For script fonts in math mode
\usepackage{tocloft} % Custom table of contents
\usepackage{datetime2} % For dates
\usepackage{caption} % For better control over captions
\usepackage{float} % Fine control over figure/table placement
\usepackage{imakeidx} % For index

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\cftchapfont}{\normalfont} % Remove bold for chapter names
\renewcommand{\cftchappagefont}{\normalfont} % Remove bold for chapter page numbers
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\eax}[1]{\emph{#1}\index{#1}} % Macro for emphasis and index
\newcommand{\abs}[1]{\left| #1 \right|} % Absolute value
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor} % For flooor
\newcommand{\toup}[1]{\xrightarrow{#1}}

% Custom Notation List Environment
\newlist{notationlist}{description}{1}
\setlist[notationlist]{font=\bfseries,labelsep=1em}

% Geometry Settings
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
}

% Hyperref Colors
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    citecolor=red
}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}

% Custom Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Chapter name on top left
\fancyhead[R]{\rightmark}  % section name on top right
\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Making index
\makeindex[intoc]

% Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries \centering}
  {\chaptername\ \thechapter}{20pt}{\Huge \centering}

\titlespacing*{\chapter}{0pt}{20pt}{100pt}

\begin{document}

\pagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vspace*{\fill}
    % Title in all caps
    {\Huge \textbf{\MakeUppercase{Probability Theory II}}\par}

    \vspace{0.5cm} % Adjust vertical spacing between title and subtitle
    % Subtitle in normal text, slightly enlarged
    {\Large Matthew Joseph, notes by Ramdas Singh\par}

    \vspace{0.5cm} % Additional spacing before the author
    % Author information
    {\large Second Semester\par}
    \vspace*{\fill}
    \end{center}
\end{titlepage}

\clearpage

\pagenumbering{roman}

\chapter*{List of Symbols}
\begin{notationlist}
    \item $\Omega$, a sample space.
    \item $\omega$, an element of a sample space.
    \item $EX$, the expectation of the random variable $X$.
    \item $\text{Var}X$, the variance of the random variable $X$.
    \item $N(\mu,\sigma^{2})$, a normal distribution with expectation $\mu$ and variance $\sigma^{2}$.
    \item $N_{n}(k)$, the number of paths from $(0,0)$ to $(n,k)$ in a simple random walk.
    \item $N_{n}^{+}(k)$, the number of paths from $(0,0)$ to $(n,k)$ through strictly positive values in a random walk.
    \item $p_{k}^{X}$, the probability mass function for a random variable $X$.
\end{notationlist}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
\pagenumbering{arabic}
\pagestyle{fancy}


%%-------------------------------------------------------------------------------------------------


\chapter{RANDOM WALKS AND MISC. RESULTS}


\textit{January 3rd.}

We first start with some initial statements.
Let $\Omega$ be a countable state space, and let each $\omega \in \Omega$ have a probabiltiy $P(\omega)$ associated with it.

\begin{lemma}
    For random variables $X,Y$ such that $X(\omega) \leq Y(\omega)$ for all $\omega \in \Omega$. Then, $EX \leq EY$.
\end{lemma}
\begin{proof}
    This can easily be seen by summing over all terms via the alternate definition of the expectation,
    \begin{equation}
        EX = \sum_{\omega \in \Omega} X(\omega) P(\omega) \leq \sum_{\omega \in \Omega} Y(\omega) P(\omega) = EY.
    \end{equation}
\end{proof}

We now state Markov's inequality. 

\begin{theorem}[\eax{Markov's inequality}]
    If $X$ is a non-negative randm variable, then for $a > 0$, we have
    \begin{equation}
        P(X > a) \leq \frac{EX}{a}.
    \end{equation}
\end{theorem}
\begin{proof}
    Define an indicator function $I_{a}(\omega)$ as 1 if $X(\omega) \geq a$, and 0 if otherwise. We then have
    \begin{align}
        I_{a}(\omega) &\leq \frac{X(\omega)}{a} \implies P(X \geq a) = EI_{a} \leq \frac{1}{a} EX.
    \end{align}
\end{proof}
\begin{remark}
    A better upper bound here may be found by starting with $I_{a}(\omega)X(\omega)$ instead of just $X(\omega)$.
\end{remark}
If we have $X \sim N(0,1)$, then we can find an upper bound for its probability density function.
\begin{equation}
    P(X > a) = \int_{a}^{\infty} \frac{1}{\sqrt{2\pi}} e^{\frac{-x^{2}}{2}} dx \leq \int_{a}^{\infty} \frac{1}{\sqrt{2\pi}} \frac{x}{a} e^{\frac{-x^{2}}{2}} dx = \frac{e^{\frac{-a^{2}}{2}}}{\sqrt{2\pi}a}.
\end{equation}

Note that $X$ here is a random variable over a continuous state space; the previous lemma and Markov's inequality also work here. We are to show them for the continuous case instead of the discrete one.

\begin{proof}
    Here, we have $0 \leq X(\omega) \leq Y(\omega)$ for all $\omega$ in our continuous state space $\Omega$. We see that $\{X > x\} \subseteq \{Y > x\} \implies P(X > x) \leq P(Y > x)$. Integrating both sides gives us $EX \leq EY$.
\end{proof}

\begin{theorem}[\eax{Chebyshev's inequality}]
    Let $X$ be a random variable with finite mean $\mu = EX$ and finite variance $\sigma^{2} = \text{Var}(X)$. Then for $a > 0$, 
    \begin{equation}
        P(\abs{X-\mu}>a) \leq \frac{\text{Var}(X)}{a^{2}}.
    \end{equation}
\end{theorem}
\begin{proof}
    Start with the proof of Markov's inequality, replacing the indiciator function with one that's unity when $\abs{X-\mu} \geq a$.
\end{proof}
\begin{example}
    Suppose $X_{1}, X_{2}, \ldots, X_{n}$ are $n$ independent and identically distributed random variables, with $EX_{i} = \mu$ and $\text{Var}X_{i} = \sigma^{2}$. If $S_{n} = \sum X_{i}$, we then have
    \begin{equation}
        P(\abs{S_{n}-n\mu} > a) \leq \frac{\text{Var}S_{n}}{a^{2}} = \frac{n\sigma^{2}}{a^{2}}.
    \end{equation}
    If we replace $a$ with $n^{\frac{1}{2}+\varepsilon}$, we then have
    \begin{equation}
        P(\abs{S_{n}-n\mu} > n^{\frac{1}{2}+\varepsilon}) \leq \frac{\sigma^{2}}{n^{2 \varepsilon}} \to 0 \text{ as } n \to \infty.
    \end{equation}
\end{example}
\begin{proposition}
    If $\text{Var}(X) = 0$, then $P(X=EX) = 1$.
\end{proposition}
\begin{proof}
    For all $\varepsilon > 0$, we have
    \begin{equation}
        P(\abs{X-EX} > \varepsilon) \leq \frac{\text{Var}X}{\varepsilon^{2}} = 0.
    \end{equation}
    Define $A_{n}$ as $\{\abs{X-EX} > \frac{1}{n}\}$. Taking $P(\bigcup A_{n}) = \lim_{n \to \infty} P(A_{n})$, the proof follows.
\end{proof}

\section{The Law of Large Numbers}

We start by stating the weak law of large numbers.
\begin{theorem}[\eax{Weak law of large numbers}]
    Let $\{X_{k}\}_{k \geq 1}$ be a sequence of independent and identically distributed random variables with $E\abs{X_{i}} < \infty$. Let $\mu = EX_{i}$. Then for any $a > 0$,
    \begin{equation}
        \lim_{n \to \infty} P\left(\abs{\frac{X_{1}+X_{2}+\ldots+X_{n}}{n} - \mu} > a \right) = 0.
    \end{equation} 
\end{theorem}
\begin{proof}
    For now, let us assume that $\Omega$ is countable. We begin with the case where the variance of $X_{i}$, $\sigma^{2}$, is finite. Fix $a > 0$, and let $S_{n} = X_{1} + X_{2} + \ldots + X_{n}$. Then,
    \begin{equation}
        P\left(\abs{\frac{S_{n}}{n} - \mu} > a\right) = P(\abs{S_{n}-n\mu} > na) \leq \frac{\text{Var}S_{n}}{n^{2}a^{2}} = \frac{n\sigma^{2}}{n^{2}a^{2}} \to 0 \text{ as } n \to \infty.
    \end{equation}
    We now focus the case when the variance, $\sigma^{2}$, is infinite. Assume that the expected value, $\mu$, is 0; if it were non-zero, we would then instead work with $X_{i}-\mu$. Let $\delta > 0$; we shall choose a particular $\delta$ later. For each $n$, define $n$ pairs of random variables, $U_{1},V_{1}, \ldots, U_{n},V_{n}$, as $U_{k} = X_{k}, V_{k} = 0$ if $\abs{X_{k}} \leq \delta n$, and $U_{k} = 0, V_{k} = X_{k}$ if $\abs{X_{k}} > \delta n$. $X_{k}$ can be rewritten as $U_{k} + V_{k}$. We then have
    \begin{align}
        \{\abs{X_{1} + \ldots + X_{n}} \geq na\} &\subseteq \{\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\} \cup \{\abs{V_{1} + \ldots + V_{n}} \geq \frac{na}{2}\} \\
        \implies P\left(\abs{X_{1} + \ldots + X_{n}} \geq na\right) &\leq P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) + P\left(\abs{V_{1} + \ldots + V_{n}} \geq \frac{na}{2}\right).
    \end{align}
    We focus on the first term on the right hand side. The $U_{i}$'s are independently and identically distributed, so
    \begin{align}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4E[\abs{U_{1}+\ldots+U_{n}}^{2}]}{a^{2}n^{2}} = \frac{4}{a^{2}n^{2}} \left( \text{Var}(U_{1} + \ldots + U_{n}) + (nEU_{i})^{2} \right).
    \end{align}
    For the variance, we have
    \begin{equation}
        \text{Var}(U_{1}+\ldots+U_{n}) = n \text{Var}U_{i} \leq n EU_{i}^{2} \leq n E[\abs{U_{i}}\abs{U_{i}}] \leq \delta n^{2} E[\abs{U_{i}}]
    \end{equation}
    which transforms the previous equation as
    \begin{equation}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4}{a^{2}n^{2}} \left( \delta n^{2} E[\abs{U_{i}}] + (nEU_{i})^{2} \right).
    \end{equation}
    A lemma (to be proven later) states that $E[\abs{U_{i}}] = E[\abs{X_{i}}]$ as $n \to \infty$, and $EU_{i} = EX_{i} = 0$ too. So,
    \begin{equation}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4}{a^{2}n^{2}} \left( \delta n^{2} E[\abs{U_{i}}] + (nEU_{i})^{2} \right) \leq \frac{4 \delta E[\abs{U_{i}}]}{a^{2}} + \frac{4}{a^{2}} (EU_{i})^{2}.
    \end{equation}

    For the second term on the right hand side, begin with
    \begin{align}
        P(V_{1}+ \ldots + V_{n} \neq 0) &\leq P(\{V_{1} \neq 0\} \cup \ldots \cup \{V_{n} \neq 0\}) \leq nP(V_{i} \neq 0) = n \sum_{\abs{x} > \delta n} P(X_{i} = x) \notag \\
        &\leq n \sum_{\abs{x} > \delta n} \frac{\abs{x}}{\delta n} P(X_{i} = x) = \frac{1}{\delta} E[\abs{V_{i}}]. 
    \end{align}
    The rightmost term here tends to 0 as $n \to \infty$. Now choose $\delta$ to be $\frac{\varepsilon a^{2}}{\abs{6 E{\abs{X_{i}}}}}$, and then choose $N$ to be large enough such that for all $n > N$, both the terms are smaller than $\frac{\varepsilon}{2}$.
\end{proof}
\textit{January 7th.}

We now prove the lemma called upon earlier.
\begin{lemma}
    If $X$ is a discrete random variable and takes values $y_{1},y_{2},\ldots,y_{k}$, and $E[\abs{X}] < \infty$, then $\lim_{n \to \infty} E[\abs{X}1_{\abs{X} \leq n}] = E[\abs{X}]$.
\end{lemma}
\begin{proof}
    Notice that the terms on the left hand side and right hand side are $\sum_{y_{k} : \abs{y_{k}} \leq n}$ and $\sum_{y_{k}} \abs{y_{k}} P(Y = y_{k})$. The condition for convergence may now be applied.
\end{proof}
The above equation, begin inside absolute braces, must imply that the term $E[X \cdot 1_{\abs{X} \leq n}]$ must also absolutely converge to $EX$.

\section{Simple Random Walk}
Let $X_{1},X_{2},\ldots$ be independent and identically distributed random variables, with $X_{i} = 1$ with probability $\frac{1}{2}$ and $X_{i}=-1$ with probability $\frac{1}{2}$. Now define $S_{0}=0$ and $S_{n} = \sum_{i=1}^{n} X_{i}$. The sequence $(S_{n})_{n \geq 0}$ is a \eax{simple random walk}.

Note that $S_{0}=k_{0}=0,S_{1}=k_{1},\ldots,S_{n}=k_{n}$ can occur if and only if $\abs{k_{i}-k_{i+1}} = 1$ for all $0 \leq i \leq n-1$. The sequence $(k_{n})_{n \geq 0}$ is a \eax{simple path} of the simple random walk. By the event $\{S_{n}=k\}$, we are concerned with the event that the random walk visits $k$ at step $n$. If $(k_{n})_{n \geq 0}$ is given we have $X_{i} = k_{i}-k_{i-1}$. Because the $X_{i}$'s are independent and identically distributed, each event $\{X_{1}=l_{1},X_{2}=l_{2},\ldots,X_{n}=l_{n}\}$, where $l_{i} = \pm 1$, is equally likely with probability $\frac{1}{2^{n}}$. Thus,
\begin{equation}
    P(S_{n}=k) = \frac{N_{n}(k)}{2^{n}}
\end{equation}
where $N_{n}(k)$ is defined as the number of distinct of path that start at $0$ and end at $k$ at step $n$. We also define $N_{n}^{+}(k)$ to be the number of distinct paths that end at $k$ at step $n$ and stay above the $x$-axis up to time $n-1$. The probability of the corresponding event is
\begin{equation}
    P(\{S_{1} > 0, S_{2} > 0, \ldots S_{n-1} > 0, S_{n} = k\}) = \frac{N_{n}^{+}(k)}{2^{n}}.
\end{equation}

\begin{lemma}
    Suppose $a,a',b,b'$ are integers, with $0 \leq a < a'$. Then the number of distinct path from $(a,b)$ to $(a',b')$ depends only on $a'-a = n$ and $b'-b = k$, and is given by $\binom{n}{\frac{n+k}{2}}$.
\end{lemma}
\begin{proof}
    Notice that we need $x$ $+1$'s and $y$ $-1$'s to appear, satisfying $x+y = a'-a$ and $x-y = b'-b$. Solving, we get $x = \frac{n+k}{2}$ and $y = \frac{n-k}{2}$. Thus, the number of paths is given by $\binom{n}{\frac{n+k}{2}}$.
\end{proof}
Using this lemma, we find that $N_{n}(k) = \binom{n}{\frac{n+k}{2}}$. The following convention is now followed; if $t$ is not an integer, then $\binom{n}{t} = 0$.

\begin{lemma}[The \eax{method of images}]
    Suppose $a,a',b,b'$ are integers, with $0 \leq a < a'$ and $b,b'>0$. Then the number of distinct paths from $(a,b)$ to $(a',b')$ that intersect the $x$-axis is equal to the number of paths from $(a,-b)$ to $(a',b')$.
\end{lemma}
\begin{proof}
    Consider any path $(b=k_{0},k_{1},\ldots,k_{n-1},k_{n}=b')$, from $(a,b)$ to $(a',b')$, that intersects the $x$-axis. Let $j$ be the smallest index for which $k_{j}=0$. For ease, denote $(a,b)$ by $A$, $(a',b')$ by $A'$, $(a+j,0)$ by $B$, and $(a,-b)$ by $A''$. Reflect the segment from $A$ to $B$ about the $x$-axis to obtain a `mirrored-path' from $A''$ to $B$; $(-b=-k_{0},-k_{1},\ldots,-k_{j-1},k_{j}=0,k_{j+1},\ldots,k_{n}=b')$. There is now a one-to-one correspondence between the paths from $A$ to $A'$ that intersect the $x$-axis, and the paths from $A''$ to $A'$.
\end{proof}
We can now easily compute $N_{n}^{+}(k)$; it simply the number of paths from $(1,1)$ to $(n,k)$ that do not intersect the $x$-axis.
\begin{theorem}[\eax{Ballot theorem}]
    The number of paths that progress from $(0,0)$ to $(n,k)$ through strictly positive values is given by $N_{n}^{+}(k) = \frac{k}{n}N_{n}(k)$.
\end{theorem}
\begin{proof}
    We have
    \begin{align}
        N_{n}^{+}(k) &= \text{ number of paths from $(1,1)$ to $(n,k)$} - \text{ number of such paths that intersect the $x$-axis} \notag \\
        &= N_{n-1}(k-1) - N_{n-1}(k+1) \notag \\
        &= \binom{n-1}{\frac{n+k}{2}-1} - \binom{n-1}{\frac{n+k}{2}} = \frac{k}{n}\binom{n}{\frac{n+k}{2}} = \frac{k}{n}N_{n}(k).
    \end{align}
\end{proof}
Suppose $n = 2\nu$. Define $u_{2\nu}$ to be $P(S_{2\nu}=0) = \frac{\binom{2\nu}{\nu}}{2^{n}}$. The question we ask is to compute the probability that the first return to 0, if at all, occurs after step $n$. It can be found out as
\begin{align}
    P(\text{first return to $0$...}) &= P(S_{1}\neq 0, S_{2} \neq 0, \ldots, S_{2\nu} \neq 0) \\
    &= P(S_{1}>0,\ldots,S_{2\nu}>0) + P(S_{1}<0,\ldots,S_{2\nu}<0) \notag \\
    &= 2P(S_{1}>0,\ldots,S_{2\nu}>0) \notag \\
    &= 2 \sum_{k \text{ even}, k>0} P(S_{1}>0,\ldots,S_{2\nu-1}>0,S_{2\nu}=k) \notag \\
    &= \frac{2}{2^{2\nu}} \sum_{k \text{ even}, k>0} N_{2\nu}^{+}(k) \notag \\
    &= \frac{2}{2^{2\nu}} \sum_{k \text{ even}, k>0} N_{2\nu-1}(k-1)-N_{2\nu-1}(k+1) \notag \\
    &= \frac{2}{2^{2\nu}} N_{2\nu-1}(1) = u_{2\nu}.
\end{align}
We state this down as a lemma.
\begin{lemma}[\eax{Basic lemma}]
    For $n$ even, the probability that the first return to 0, if at all, occurs after step $n$ is the same as the probability that the location at step $n$ is 0. For $n$ odd, it is the probability that the location at step $n-1$ is 0.
\end{lemma}
We ask another question; for a fixed $n$, where does the random walk achieve its first maximum upto time $n$? For this, denote by $M_{n}$ the index $m$ at which the walk $S_{0},S_{1},\ldots,S_{n}$, over $n$ steps, achieves its maximum for the first time.

For $0<m<n$, $M_{n}=m$ if and only if $S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}$ and $S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n}$. Notice that the first of these two conditions depends only on $X_{1},X_{2},\ldots,X_{m}$, and the second condition depends only on $X_{m+1},X_{m+2},\ldots,X_{n}$. So, $P(M_{n}=m) = P(S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}) \cdot P(S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n})$.

The key idea here is to consider the \eax{reversed walk}; define a new walk with $X_{1}'=X_{m},\; X_{2}'=X_{m-1},\ldots,X_{m}'=X_{1}$. Also define $S_{k}' = X_{1}' + \ldots + X_{k}'$. From here, we can deduce that $S_{m}>S_{m-i}$ is true if and only if $X_{m}+\ldots+X_{m-i}>0$ is true, which is true if and only if $S_{i}'>0$ is true. So, $P(S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}) = P(S_{1}'>0,\; S_{2}'>0, \ldots, S_{m}'>0)$. If we now define $S_{k}''=X_{m+1}+\ldots+X_{m+k}$, we have
\begin{align*}
    P(S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n}) &= P(X_{m+1}\leq 0,\; X_{m+1}+X_{m+2}\leq 0, \ldots, X_{m+1}+\ldots+X_{n}\leq 0) \\
    &= P(S_{1}''\leq 0,\; S_{2}''\leq 0,\ldots,S_{n-m}''\leq 0) \\
    &= P(S_{1}''\geq 0,\; S_{2}''\geq 0,\ldots,S_{n-m}''\geq 0)
\end{align*}
The first of the terms discussed, $P(S_{1}'>0,\; S_{2}'>0, \ldots, S_{m}'>0)$, can be computed for $m = 2\nu, 2\nu+1$; it is simply $\frac{1}{2}u_{2\nu}$. For the latter of these terms, we introduce a new random variable $\tilde{X}$ which has the same distribution as the $X_{i}$'s and is independent. Also define $\tilde{S_{i}}$ to be $\tilde{X}+X_{1}+\ldots+X_{i-1}$ and $\tilde{S_{0}}$ to be $0$. We then have
\begin{align}
    \frac{1}{2}P(S_{0} \geq 0, \ldots, S_{n-m} \geq 0) &= P(\tilde{X}=1) \cdot P(S_{0} \geq 0, \ldots, S_{n-m} \geq 0) \notag\\
    &= P(\tilde{X}=1,S_{0} \geq 0, S_{0} \geq 0, \ldots, S_{n-m} \geq 0) \notag\\
    &= P(\tilde{S_{1}}=1,\tilde{S_{2}}>0,\ldots,\tilde{S_{n-m+1}}>0) \notag\\
    &= P(S_{1}>0,S_{2}>0,\ldots,S_{n-m+1}>0).
\end{align}
Thus, we get
\begin{equation}
    P(M_{n}=m) = \frac{1}{2} u_{2k}u_{2\nu-2k}
\end{equation}
where $m$ is of the form $2k$ or $2k+1$, and $n$ is of the form $2\nu$, with $1 < k < \nu$.\\ \\
\textit{January 10th.}

Plugging in $m = 0$, we get $P(M_{n}=0) = P(S_{1} \leq 0, \ldots, S_{2 \nu} \leq 0) = \frac{1}{2}u_{2\nu}$. For $m=n$, we have $P(M_{n}=n) = P(S_{1}\leq 0, \ldots, S_{2\nu} \leq 0) = \frac{1}{2}u_{2 \nu}$. Let us first compute $u_{2k}$.
\begin{align}
    u_{2k} &= P(2k=0) = \frac{\binom{2k}{k}}{2^{2k}} = \frac{(2k)!}{(k!)^{2}2^{2k}} \notag \\
    &\sim \frac{(2k)^{2k+\frac{1}{2}} e^{-2k} \sqrt{2\pi}}{(\sqrt{2\pi} k^{k+\frac{1}{2}} e^{-k})^{2}2^{2k}} = \frac{1}{\sqrt{\pi k}}.
\end{align}
For $0 < a < b < 1$, we have
\begin{align}
    P(an \leq M_{n} \leq bn) &= \sum_{m = an}^{bn} P(M_{n}=m) = \sum_{k=a\nu}^{b\nu} u_{2k}u_{2\nu-2k} \notag \\
    &\sim \sum_{k=a\nu}^{b\nu}\frac{1}{\sqrt{\pi k}}\frac{1}{\sqrt{\pi (\nu-k)}} = \sum_{k=a\nu}^{b\nu} \frac{1}{\nu \sqrt{\pi \frac{k}{\nu}} \sqrt{\pi (1-\frac{k}{\nu})}}\notag \\
    &\to \frac{1}{\pi} \int_{a}^{b} \frac{dx}{\sqrt{x(1-x)}} = \frac{2}{\pi} (\arcsin{\sqrt{b}} - \arcsin{\sqrt{a}}).
\end{align}
In fact, this is the \eax{arcsin law for maxima}; for $0 \leq t \leq 1$, we have
\begin{equation}
    \lim_{n \to \infty} P\left( \frac{M_{n}}{n} \leq t\right) = \frac{2}{\pi} \arcsin{\sqrt{t}}.
\end{equation}
If we look at this as a cumulative density funtion, the probability density function becomes $\frac{d}{dt} \frac{2}{\pi} \arcsin{\sqrt{t}} = \frac{1}{\pi \sqrt{t(1-t)}}$.

We are now interested in $\tilde{M}_{n}$, the last time when maximum up to time $n$ is attained. We can just look at the walk backwards again; in this case, we get
\begin{equation}
    P(\frac{\tilde{M}_{n}}{n}) = P\left(\frac{n-\tilde{M}_{n}}{n} \leq t\right) \to \frac{2}{\pi} \arcsin{\sqrt{t}}.
\end{equation}

We now ask the probability that the random walk of $n = 2\nu$ steps last visit $0$ at time $2k$. We denote by $K_{n}$ the location of the last return to $0$ in a walk of $n$ steps. Now look at
\begin{align}
    \alpha_{2k,2\nu} &= P(K_{n}=2k) = P(S_{2k}=0,S_{2k+1}\neq 0,\ldots,S_{2\nu}\neq 0) \notag
    \\ &= P(S_{2k}=0) \cdot P(X_{2k+1}\neq 0, \ldots, X_{2k+1}+\ldots+X_{2\nu} \neq 0) \notag \\
    &= P(S_{2k}=0) \cdot P(S_{1} \neq 0, \ldots, S_{2\nu-2k} \neq 0) = u_{2k}u_{2\nu-2k}.
\end{align}
We can also state an \eax{arcsin law for last visit} here; for $0 < t < 1$
\begin{equation}
    \lim_{n \to \infty} P(K_{n} \leq tn) = \frac{2}{\pi} \arcsin{\sqrt{t}}.
\end{equation}
If we set the an additional limit that says $t$ tends to $0$, replacing $t$ by an arbitrary $\varepsilon > 0$, we have
\begin{equation}
    \lim_{n \to \infty} P(K_{n}=0) = 0.
\end{equation}
Given enough time, a simple random walk must return to 0.

Denote by $f_{2n}$ the probability that the first return to 0 occurs at time $2n$.
\begin{align}
    f_{2n} &= P(S_{1} \neq 0, \ldots, S_{2n-1} \neq 0, S_{2n} = 0) \notag \notag \\
    &= P(S_{1} \neq 0, \ldots, S_{2n-1} \neq 0) - P(S_{1} \neq 0, \ldots, S_{2n} \neq 0) \notag \\
    &= P(S_{1} \neq 0, \ldots, S_{2n-2} \neq 0) - P(S_{1} \neq 0, \ldots, S_{2n} \neq 0) \notag \\
    &= u_{2n-2} - u_{2n} = \frac{1}{2n-1} u_{2n}.
\end{align}
\begin{lemma}
    With the usual notation,
    \begin{equation}
        u_{2n} = f_{2}u_{2n-2} + f_{4}u_{2n-4} + \ldots + f_{2n} u_{0}.
    \end{equation}
\end{lemma}
\begin{proof}
    We have
    \begin{align}
        P(S_{2n} = 0) &= \sum_{k=1}^{n} P(S_{2n}=0, \text{ first return at $2k$}) \notag\\
        &= \sum_{k=1}^{n} P(\text{first return at 2k}) \cdot P(S_{2n} = 0 \mid \text{first return at 2k}) \notag\\
        \implies P(S_{n} = 0) &= \sum_{k=1}^{n} f_{2k} u_{2n-2k}.
    \end{align}
\end{proof}
\begin{theorem}
    The probability that in the time interval $0$ to $n=2\nu$, the random walk spends $2k$ amount of time on the positive side and $2\nu-2k$ amount of time on the negative side is $\alpha_{2k,2\nu}$.
\end{theorem}
\begin{corollary}
    For $0 < t < 1$,
    \begin{equation}
        P(\text{random walk spends less than $tn$ time on positive side}) \to \frac{2}{\pi} \arcsin{\sqrt{t}}.
    \end{equation}
\end{corollary}
\begin{proof}
    This is the proof of the theorem. We introduce $b_{2k,2\nu}$; it is defined as the probability that the random walk of length $2\nu$ and $2k$ sides above the $x$-axis. We need to show that $b_{2k,2\nu} = \alpha_{2k,2\nu}$. We have
    \begin{align}
        b_{2\nu,2\nu} &= P(S_{1} \geq 0, S_{2} \geq 0, \ldots, S_{2\nu} \geq 0) = u_{2\nu}, \\
        b_{0,2\nu} &= P(S_{1} \leq 0, \ldots, S_{2\nu} \leq 0) = u_{2\nu}.
    \end{align}
    We are left to prove it for $1 \leq k \leq \nu-1$. Assume that exactly $2k$ out of $2\nu$ time are spent above the $x$-axis, with $1 \leq k \leq \nu-1$. Suppose first return to 0 occurs at time $2r < 2\nu$. We deal in cases.
    \begin{itemize}
        \item Case I: $2r$ time units upto first return are on the positive side. Then, $r \leq k \leq \nu-1$. The time from $2r$ to $2\nu$ has to be above the $x$-axis, $2k-2\nu$ time. The number of such paths is $(\frac{1}{2}2^{2r}f_{2r}) (2^{2\nu-2r} b_{2k-2r,2\nu-2r})$.
        \item The $2r$ time units upto the first return are on the negative side. The nubmer of such paths is $(\frac{1}{2}2^{2r}f_{2r})(2^{2\nu-2r}b_{2k,2\nu-2r})$. Also, $\nu-r \geq k$.
    \end{itemize}
    Thus, we have
    \begin{equation}
        b_{2k,2\nu} = \frac{1}{2}\sum_{r=1}^{k} f_{2r} b_{2k-2r,2\nu-2r} + \frac{1}{2} \sum_{r=1}^{\nu-k} f_{2r} b_{2k,2\nu-2r}.
    \end{equation}
    We now proceed with induction on $\nu$. We have already shown this for $\nu = 1$; assume that this is true for $\nu \leq V - 1$. By induction,
    \begin{align}
        b_{2k,2V} &= \frac{1}{2}\sum_{r=1}^{k} f_{2r} \alpha_{2k-2r,2V-2r} + \frac{1}{2} \sum_{r=1}^{V - k} f_{2r} \alpha_{2k,2V-2r} \notag \\
        &= \frac{1}{2} u_{2V-2k} \sum_{r=1}^{k} f_{2r} u_{2k-2r} + \frac{1}{2} u_{2k} \sum_{r=1}^{V-k} f_{2r} u_{2V-2k-2r} \notag \\
        &= u_{2k}u_{2\nu-2k} = \alpha_{2k,2\nu}.
    \end{align}
\end{proof}

\textit{January 17th.}
\begin{theorem}[\eax{Weirstrass's polynomial approximation}.]
    Let $f:[0,1] \to \R$ be a continuous function. Then for every $\varepsilon > 0$, there is a polynomial $P$, dependent on $f$ and $\varepsilon$, such that
    \begin{equation}
        \abs{f(x)-P(x)} < \varepsilon \text{ for all } x \in [0,1].
    \end{equation}
\end{theorem}
\begin{remark}
    Any continuous function $f:[0,1]\to\R$ is bounded and uniformly continuous. This fact will be useful in proving the previous theorem.
\end{remark}
\begin{proof}
    Start with $X_{1},X_{2},\ldots$ which are independent and identically disitributed Bernoulli random variables, $\text{Ber}(x)$. Let $S_{n} = X_{1} + X_{2} + \ldots + X_{n}$. From the weak law of large numbers, we know that $\frac{S_{n}}{n}$ is approximately $x$. We can expect that $f(x)$ will also be approximately $f(\frac{S_{n}}{n})$. We now have
    \begin{align}
        f_{n}(x) &= Ef(\frac{S_{n}}{n}) = \sum_{j=0}^{n} f(\frac{j}{n})P(S_{n}=j) \notag \\
        &= \sum_{j=0}^{n} f(\frac{j}{n}) \binom{n}{j} x^{j} (1-x)^{n-j}.
    \end{align}
    This is now a polynomial; we wish to see how close this is to $f$. Define $A_{\delta}$ to be $\{j : \abs{\frac{j}{n} - x} \leq \delta\}$
    \begin{align}
        \abs{f_{n}(x)-f(x)} &= \abs{\sum_{j=0}^{n} \left( f(\frac{j}{n}) - f(x) \right)} P(S_{n}=j) \notag \\
        &= \abs{\sum_{j \in A_{\delta}} \left( f(\frac{j}{n}) - f(x) \right) + \sum_{j \notin A_{\delta}} \left( f(\frac{j}{n}) - f(x) \right)} P(S_{n}=j) \notag \\
        &\leq \sum_{j \in A_{\delta}} \abs{ f(\frac{j}{n}) - f(x) } P(S_{n}=j) + \sum_{j \notin A_{\delta}} \abs{ f(\frac{j}{n}) - f(x) } P(S_{n}=j).
    \end{align}
    We have two terms to deal with now. For the first term, choose $\delta > 0$ such that $\abs{x-y} < \delta \implies \abs{f(x)-f(y)} < \varepsilon$; this $\delta$ can be chosen since $f$ is uniformly continuous. Similarly, also choose $M = \sup_{x \in [0,1]} \abs{f(x)}$. $M$ is finite since $f$ is bounded. Thus, we have
    \begin{align}
        \sum_{j \in A_{\delta}} \abs{f(\frac{j}{n})} P(S_{n}=j) \leq \sum_{j \in A_{\delta}} \varepsilon P(S_{n} = j) \leq \varepsilon
    \end{align}
    and
    \begin{align}
        \sum_{j \notin A_{\delta}} \leq 2M P(\abs{\frac{S_{n}}{n}-x} > \delta) \leq 2M \frac{\text{Var}(S_{n})}{n^{2}\delta^{2}} = \frac{2Mnx(1-x)}{n^{2}\delta^{2}}.
    \end{align}
    Combining the two, and choosing $n$ large enough, we have
    \begin{align}
        \abs{f_{n}(x)-f(x)} &\leq \varepsilon + \frac{2Mx(1-x)}{n\delta^{2}} \leq \varepsilon + \frac{M}{2n\delta^{2}} \leq 2\varepsilon.
    \end{align}
\end{proof}

\section{Erd\"os-Renyi Random Graph}
We first discuss the setup; start with $n$ vertices of an empty graph. For any pair of points $(i,j)$, with $i \neq j$, join these vertices with an edge with probability $p$ independently for all such pairs. Such a graph is denoted by $G_{n,p}$.

A collection of three points $S = \{i,j,k\}$ form a triangle if $G_{n,p}$ has the edges $\{i,j\}$, $\{j,k\}$, and $\{i,k\}$. We question the probability that such a graph has no formed triangles. Can we find $p = p_{n}$ such that triangles begin to appear at $p_{n}$? Let $S$ be any set of three vertices. Define $X_{S}$ to be the indicator function; 1 if $S$ forms a triangle, and 0 otherwise. We note that $X_{S} \sim \text{Ber}(p^{3})$. We note that
\begin{equation*}
    EX_{S} = p^{3}, \; \text{Var}X_{S} = p^{3}(1-p^{3}) \leq p^{3}.
\end{equation*}
Denote by $N$ the number of triangles in the graph $G_{n,p}$. Clearly,
\begin{equation*}
    N = \sum_{S: \abs{S} = 3} X_{S}, \;  EN = \binom{n}{3} p^{3} < n^{3}p^{3}, \; \text{Var}{N} = \sum_{S} \Var{X_{S}} + \sum_{S}\sum_{T \neq S} \Cov{(X_{S}X_{T})} \leq n^{3}p^{3} + n^{4}p^{5}
\end{equation*}
ALso, $P(N \geq 1) \leq EN < n^{3}p^{3}$. If $p = p_{n} << \frac{1}{n}$, then $P(N \geq 1) \to 0$ as $n \to \infty$. We discuss this for $p >> \frac{1}{n}$. We have
\begin{align}
    P(N = 0) \leq P(\abs{N-EN} \geq EN) \leq \frac{\text{Var}N}{(EN)^{2}} \leq \frac{(n^{3}p^{3}+n^{4}p^{5})}{\frac{n^{6}p^{6}}{100}} \leq \frac{100}{n^{3}p^{3}} + \frac{100}{n(np)} \to 0.
\end{align}
We can state this as a theorem.
\begin{theorem}
    Consider $G_{n,p_{n}}$. Let $E$ be the event that the graph is triangle free. We then have
    \begin{equation}
        P(E) \to \begin{cases}
        0 &\text{ if } \dfrac{p_{n}}{\frac{1}{n}} \to \infty,\\
        1 &\text{ if } \dfrac{p_{n}}{\frac{1}{n}} \to 0.
        \end{cases}
    \end{equation}
\end{theorem}
Now suppose that $\frac{np_{n}} \to C > 0$ as $n \to \infty$. Then we have
\begin{equation}
    N \approx \text{Poisson} \left( \frac{C^{3}}{6} \right).
\end{equation}


\textit{January 21st.}
\begin{remark}
    For this next `game', we will think of $X_{i}$'s as the winnings in game $i$ and $\mu$ to be the entrance fees for a game.
\end{remark}
\begin{definition}
    Suppose that $X_{1},X_{2},\ldots$ are independent, but not necessarily identically distributed. Let $S_{n} = X_{1} + \ldots + X_{n}$. We say a game with accumulated entrance fees $\{\alpha_{n},n\geq 1\}$ is fair if
    \begin{equation}
        P(\abs{\frac{S_{n}}{\alpha_{n}}-1} > \varepsilon) \to 0
    \end{equation}
    for all $\varepsilon > 0$.
\end{definition}
Using this definition of `fair', we look at an example.
\begin{example}
    This is the St.\ Petersburg's paradox. This is the game; toss a coin repeatedly until the first head is observed. If this head occurs at the $k^{\text{th}}$ toss, the amount paid out is $X = 2^{k}$. Let us find a fair accumulated entrance fees. In this case,
    \begin{equation}
        EX = \sum_{k=1}^{\infty} \frac{1}{2^{k}} 2^{k} = \infty.
    \end{equation}
    Suppose we play this game $n$ times. We are to find a fair accumulated sum $\{\alpha_{n}\}$ such that
    \begin{equation}
        P(\abs{S_{n}-\alpha_{n}} > \varepsilon \alpha_{n}) \to 0.
    \end{equation}
    To find this, we will define
    \begin{align}
        U_{j} &= X_{j} 1_{\{X_{j}\leq a_{n}\}}, \notag \\
        V_{j} &= X_{j} 1_{\{X_{j} > a_{n}\}}. \notag
    \end{align}
    $a_{n}$ shall be determined later. Note that $S_{n} = X_{1} + \ldots + X_{n} = U_{1} + \ldots + U_{n} + V_{1} + \ldots V_{n}$. Then,
    \begin{align}
        P(\abs{S_{n}-\alpha_{n}} > \varepsilon \alpha_{n}) \leq  P(\abs{U_{1}+\ldots+U_{n}-\alpha_{n}} > \frac{1}{2}\varepsilon \alpha_{n}) + P(\abs{V_{1} + \ldots + V_{n}} > \frac{1}{2}\varepsilon \alpha_{n}).
    \end{align}
    We first bound the second term on the right hand side. We have
    \begin{align}
        P(\abs{V_{1} + \ldots + V_{n}} > \frac{1}{2}\varepsilon \alpha_{n}) &\leq P(\bigcup_{i=1}^{n} \{V_{i} \neq 0\}) \leq nP(V_{1} \neq 0) = nP(X_{1} > a_{n}) \\
        &= \sum_{2^{k}>a_{n}} P(X=2^{k}) \leq \frac{2n}{a_{n}}.
    \end{align}
    Thus, we will require that $a_{n} >> n$. Also,
    \begin{equation}
        EU_{1} = \sum_{k \leq \log_{2}a_{n}} 2^{k} \cdot 2^{-k} = \floor{\log_{2}a_{n}}, \; \Var{U_{1}} \leq E[U_{1}^{2}] = \sum_{k \leq \log_{2}a_{n}} (2^{k})^{2} \cdot 2^{-k} = 2^{\floor{\log_{2}a_{n}}+1}-1 < 2a_{n}.
    \end{equation}
    $\frac{1}{n}(U_{1}+\ldots+U_{n}) \approx EU_{j} = \floor{\log_{2}a_{n}}$, so we should choose
    \begin{equation}
        \alpha_{n} = n EU_{j} = n \floor{\log_{2}a_{n}}.
    \end{equation}
    This gives us
    \begin{equation}
        P(\abs{U_{1}+\ldots+U_{n}-\alpha_{n}} > \frac{1}{2}\varepsilon \alpha_{n}) \leq \frac{n(2a_{n})}{\frac{1}{4}\varepsilon^{2}\alpha_{n}^{2}}.
    \end{equation}
    Thus, we have another condition where we require that $\frac{na_{n}}{\alpha_{n}^{2}} \to 0$. The conditions we require are
    \begin{equation*}
        \frac{n}{a_{n}} \to 0 \text{ and } \frac{na_{n}}{n^{2}(\log_{2}a_{n})^{2}} \to 0.
    \end{equation*}
    The sequence $\{a_{n}\}$ defined as $a_{n} = n \log_{2} n$ satisfies these properties. The sequnce $\alpha_{n}$ is thus
    \begin{equation}
        \alpha_{n} = n \log_{2} a_{n} = n \log_{2} n + n \log_{2} \log_{2} n.
    \end{equation}
\end{example}

\chapter{GENERATING FUNCTIONS}

\textit{January 24th.}

\begin{definition}
    For a sequence $\{a_{n}\}_{n \geq 0}$, the \eax{generating function} of $\{a_{n}\}$ is given as
    \begin{equation}
        A(s) = \sum_{n=0}^{\infty} a_{n}s^{n}
    \end{equation}
    for some $-s_{0}<s<s_{0}$.
\end{definition}
For this probability course, we will be interested in a particular form; for a random variable $X$ that takes values $k=0,1,\ldots$, the function we look at is
\begin{equation}
    \sum_{k=0}^{\infty}P(X=k) s^{k} \text{ for } -1 \leq s \leq 1.
\end{equation}
Suppose we have two sequences $\{a_{n}\}$ and $\{b_{n}\}$ with generating functions $A(s)$ and $B(s)$, respectively. If we define a new sequence $\{c_{n}\}$ as
\begin{equation}
    c_{n} = a_{0}b_{n} + a_{1}b_{n-1} + \ldots + a_{n-1}b_{1} + a_{n}b_{0} \text{ for all } n \geq 0,
\end{equation}
then the sequence $\{c_{n}\}$ is termed the \eax{convolution} of the sequences $\{a_{n}\}$ and $\{b_{n}\}$, and we shall denote it as
\begin{equation*}
    \{c_{n}\} = \{a_{n}\} \ast \{b_{n}\}.
\end{equation*}
Note that this convolution operation is both associative and commutative. We are now interested in finding the generating function of $\{c_{n}\}$. We have
\begin{align}
    C(s) &= \sum_{n=0}^{\infty} c_{n} s^{n} = \sum_{n=0}^{\infty} \left( \sum_{k=0}^{n} a_{k}b_{n-k} \right) s^{n} \notag \\
    &= \sum_{n=0}^{\infty} \sum_{k=0}^{n} a_{k}s^{k} b_{n-k}s^{n-k} = \sum_{k=0}^{\infty} \sum_{m=0}^{\infty} a_{k}s^{k} b_{m}s^{m} \notag \\
    \implies C(s) &= \left( \sum_{k=0}^{\infty} a_{k}s^{k} \right) \cdot \left( \sum_{m=0}^{\infty} b_{m}s^{m} \right) = A(s) \cdot B(s).
\end{align}
We state this down as a theorem.
\begin{theorem}
    $C(s) = A(s) \cdot B(s)$ when $\{c_{n}\} = \{a_{n}\} \ast \{b_{n}\}$.
\end{theorem}
Suppose $X$ takes values in $\Z_{+} = \{0,1,\ldots\}$. Denote $P(X=k)$ as $p_{k}$. The generating function is, thus,
\begin{equation*}
    \cP (s) = \sum_{k=0}^{\infty} p_{k} s^{k} = E[s^{X}].
\end{equation*}
Also,
\begin{align}
    \cP (1) &= 1,\\
    \cP'(1) &= \sum_{k=1}^{\infty} kp_{k} s^{k-1} |_{s=1} = EX.
\end{align}
Also note that
\begin{align}
    E[X^{2}] = \sum_{k=0}^{\infty} k^{2} p_{k} = \sum k(k-1) p_{k} + \sum k p_{k} = \cP''(1) + \cP'(1)
\end{align}
which gives us the variance of $X$ a
\begin{align}
    \Var X = E[X^{2}] - (EX)^{2} = \cP ''(1) + \cP'(1) - (\cP '(1))^{2}.
\end{align}
The individual probabilities of $X=k$ may also be found as
\begin{equation}
    p_{k} = P(X=k) = \frac{1}{k!} \cdot \frac{d^{k}}{ds^{k}} \cP (s) |_{s=0}.
\end{equation}

Now suppose that $X$ and $Y$ are two independent variables, taking values in $\Z_{+}$. Let $Z = X+Y$. We ask the probability that $Z$ equals $k$. We can find this as
\begin{equation}
    P(Z = k) = \sum_{m=0}^{k} P(X=m, Y = k-m) = \sum_{m=0}^{k} P(X = m) \cdot P(Y = k-m).
\end{equation}
Therefore, denoting $p_{k}^{(X)}$ to be the probability mass function of $X$, we have
\begin{equation}
    \{p_{k}^{(Z)}\} = \{p_{k}^{(X)}\} \ast \{p_{k}^{(Y)}\} \implies \cP^{(Z)}(s) = \cP^{(X)} (s) \cdot \cP^{(Y)} (s).
\end{equation}
There is an easier way to see the last equation; we could have started with $Es^{Z} = E[s^{X} \cdot s^{Y}] = E[s^{X}] E[s^{Y}]$.\\

If we have $S_{n} = X_{1} + X_{2} + \ldots + X_{n}$, where the $X_{i}$'s are independently distributed taking values in $\Z_{+}$, it can be shown that
\begin{equation}
    \{p_{k}^{(S_{n})}\} = \{p_{k}^{(X)}\}^{n \ast}
\end{equation}
\begin{example}
    Let us compute the generating function of $X \sim \text{Bin}(n,p)$. We have
    \begin{align}
        \cP (s) = \sum_{k=0}^{\infty} P(X=k) s^{k} = \sum_{k=0}^{n} \binom{n}{k} p^{k} (1-p)^{n-k} s^{k} = \left( (1-p) + ps \right)^{n}.
    \end{align}
    This is the generating function of the binomial distribution. Clearly,
    \begin{align*}
        EX &= \cP '(1) = np,\\
        \Var X &= \cP ''(1) + \cP '(1) - (\cP'(1))^{2} = n(n-1)p^{2} + np - n^{2}p^{2} = np(1-p).
    \end{align*}
    Note that using this generating function, we can also show that $\text{Bin}(n,p) + \text{Bin}(m,p) = \text{Bin}(m+n,p)$ when the former terms are independent.
\end{example}
\begin{example}
    We look at $X \sim \text{Poisson}(\lambda)$. We have
    \begin{align}
        \cP (s) = \sum_{k=0}^{\infty} e^{-\lambda} \frac{\lambda^{k}}{k!} s^{k} = e^{-\lambda} \sum_{k=0}^{\infty} \frac{(\lambda s)^{k}}{k!} = e^{-\lambda + \lambda s}.
    \end{align}
    For this, we can als verify $EX = \Var X = \lambda$. We can also show that $\text{Poisson}(\lambda) + \text{Poisson}(\mu) = \text{Poisson}(\lambda + \mu)$ when the former terms are independent.
\end{example}
\begin{example}
    We look at $X \sim \text{Geo}(p)$. Denote $1-p$ as $q$. The generating function is given as
    \begin{equation}
        \cP (s) = \sum_{k=0}^{\infty} pq^{k}s^{k} = \frac{p}{1-qs}.
    \end{equation}
    As an extension, let $X_{k}$ denote the number of failures between the $(k-1)^{\text{th}}$ and $k^{\text{th}}$ successes. If we denote $S_{r} = X_{1} + X_{2} + \ldots + X_{r}$, we find that $S_{r} \sim \text{NB}(p,r)$. From direct computation, we know that
    \begin{equation*}
        P(S_{r} = k) = \binom{r+k-1}{k} q^{k} p^{r} \text{ for } k = 0,1,\ldots
    \end{equation*}
    Let us compute this in another way; $S_{r}$ is the sum of independent geomtric random variables with parameter $p$. We have
    \begin{equation}
        \cP^{(S_{r})}(s) = \left( \frac{p}{1-qs} \right)^{r} = p^{r}(1-qs)^{-r} = p^{r} \sum_{k=0}^{\infty} \binom{-r}{k} (-qs)^{k}
    \end{equation}
    which tells us that
    \begin{equation}
        P(S_{r}=k) = p^{r}\binom{-r}{k} (-q)^{k}.
    \end{equation}
\end{example}
\section{Random Walks, with Generating Functions}
Here, we consider the paths that have a right step with probability $p$ and a left step with probability $q = 1-p$.
We first look at the waiting time for the first gain, that is, the event $\{S_{1}\leq 0, S_{2} \leq 0, \ldots, S_{n-1} \leq 0, S_{n} = 1\}$ (Event ($\ast$)). Denote the probability of this event by $\phi_{n}$, and its generating function by $\Phi (s)$. Note that $\phi_{0} = 0$ and $\phi_{1} = p$ lead to trivial cases. We focus on $n > 1$.

We must have $S_{1} = -1$ (Event (1)). Denote, by $\nu < n$, the first return to $0$ (Event (2)). $\nu$ only depends on $X_{0},X_{1},\ldots,X_{\nu}$. We need another $n-\nu$ steps to reach $1$; this depends on $X_{\nu+1},X_{\nu+2},\ldots,X_{n}$ (Event (3)). For some $n > 1$, Event ($\ast$) occurs if and only Event (1) $\cap$ Event (2) $\cap$ Event (3) occurs for some $\nu < n$. The point here is that the three events are independent. For some fixed $\nu < n$,
\begin{equation}
    P(\text{Event (1)}) = q, \; P(\text{Event (2)}) = \phi_{\nu -1}, \; P(\text{Event (3)}) = \phi_{n-\nu}.
\end{equation}
Thus,
\begin{equation}
    \phi_{n} = \sum_{\nu = 2}^{n-1} q \phi_{\nu-1} \phi_{n-\nu}.
\end{equation}
We have
\begin{align}
    \Phi (s) - ps &= \sum_{n=2}^{\infty} \phi_{n}s^{n} = q\sum_{n=2}^{\infty} (\phi_{1}\phi_{n-2} + \ldots + \phi_{n-2}\phi_{1}) s^{n} = qs \sum_{n=1}^{\infty} \phi_{n}^{2 \ast} s^{n} = qs(\Phi(s))^{2} \\
    \implies \Phi (s) - ps &= qs(\Phi(s))^{2}.
\end{align}
This is a standard quadratic; solving gives us
\begin{align}
    \Phi (s) = \dfrac{1 \pm \sqrt{1-4pqs^{2}}}{2qs}.
\end{align}
The solution with the `$+$' is rejected; if it was valid, then plugging in $s < 1$ would give us $\Phi (s) > 1$, which is impossible. We expand this using the binomial theorem,

\begin{align}
    \Phi (s) = \frac{1}{2qs} \left( 1 - \sum_{k=0}^{\infty} \binom{\frac{1}{2}}{k} (-4pqs^{2})^{k}  \right) = \sum_{k=1}^{\infty} \binom{\frac{1}{2}}{k} \frac{(-1)^{k-1} (4pq)^{k}}{2q} s^{2k-1}
\end{align}
which tells us that
\begin{align}
    \phi_{2k-1} = \frac{(-1)^{k-1}}{2q} \binom{\frac{1}{2}}{k} (4pq)^{k}, \; \phi_{2k} = 0.
\end{align}
Thus,
\begin{equation*}
    \Phi (1) = \sum \phi_{n} = \frac{1-\sqrt{1-4pq}}{2q} = \frac{1-\abs{p-q}}{2q} = \begin{cases}
        \frac{p}{q} &\text{ if } p < q,\\
        1 &\text{ if } p \geq q.
    \end{cases}
\end{equation*}
This gives the probability that, at some point of the random walk, the displacement 1 is reached.

Similarly, for displacement $S_{n}$, we have
\begin{equation*}
    P(S_{n} \leq 0 \; \forall n) = \begin{cases}
        \frac{q-p}{p} &\text{ if } p < q,\\
        0 &\text{ if } p \geq q.
    \end{cases}
\end{equation*}
\textit{January 28th.}

Recall that we used $u_{k}$ denote the probability that the random walk returns to zero at step $k$. For unequal left-right step probabilities,
\begin{equation*}
    u_{k} = P(S_{k}=0) = \begin{cases}
        0 &\text{ if $k$ is odd},\\
        \binom{2k}{k} p^{n} q^{n} &\text{ if } k = 2n.
    \end{cases}
\end{equation*}
Thus, the generating function for this is
\begin{align}
    U(s) = \sum_{n=0}^{\infty} u_{2n}s^{2n} = \sum_{n=0}^{\infty} \binom{2n}{n} (pqs^{2})^{n} = \sum_{n=0}^{\infty} \binom{-\frac{1}{2}}{n} (-4pqs^{2})^{n} = \frac{1}{\sqrt{1-4pqs^{2}}}.
\end{align}
Denote, by $f_{2n}$, the probability that the first return to zero occurs at step $2n$, for some $n \geq 1$. In fact, it consists of subevents; if $X_{1} = 1$, denote it by $f_{2n}^{+}$ and if $X_{1} = -1$, denote it by $f_{2n}^{-}$. If we also recall the definition of our $\phi_{n}$,
\begin{align}
    f_{2n}^{-} = P(X_{1}=-1,S_{2}<0,S_{3}<0,\ldots,S_{2n-1}<0,S_{2n}=0) = q \phi_{2n-1}.
\end{align}
The generating function of $\{f_{2n}^{-}\}$ will be given as
\begin{align}
    F^{-}(s) = \sum_{n=1}^{\infty} f_{2n}^{-} s^{2n} = q\sum_{n=1}^{\infty} \phi_{2n-1}s^{2n} = qs \sum_{n=1}^{\infty} \phi_{2n-1} s^{2n-1} = qs \Phi(s) = \frac{1}{2}(1-\sqrt{1-4pqs^{2}}).
\end{align}
It can be shown that $f_{2n}^{+}$ is just $f_{2n}^{-}$ with the probabilities reversed (check!). The generating function of $\{f_{2n}^{+}\}$ is given as
\begin{align}
    F^{+}(s) = \sum_{n=0}^{\infty} f_{2n}^{+}s^{2n} = \frac{1}{2}(1-\sqrt{1-4pqs^{2}}).
\end{align}
Adding both of these, we get
\begin{align}
    F(s) &= F^{+}(s) + F^{-}(s) = 1-\sqrt{1-4pqs^{2}} = 1 - \sum_{n=0}^{\infty} \binom{\frac{1}{2}}{n} (-4pqs^{2})^{n}\\
    \implies f_{2n} &= (-1)^{n+1} \binom{\frac{1}{2}}{n} (4pq)^{n}.
\end{align}
$F(1)$ gives us the probability that walk eventually returns to zero,
\begin{align}
    F(1) = \sum_{n=0}^{\infty} f_{2n} = 1-\sqrt{1-4pq} = 1-\abs{p-q}.
\end{align}
$F'(1)$ gives us the expected time of return to zero,
\begin{align}
    F'(s) = -\frac{1}{2}(1-4pqs^{2})^{-\frac{1}{2}}(-8pqs).
\end{align}
If $p = q = \frac{1}{2}$, then
\begin{equation*}
    F'(1) = \lim_{s \to 1^{-}} F'(s) = \infty.
\end{equation*}
The basic lemma can be proved using the generating functions.

\section{Simple Random Walks in Higher Dimensions}
Consider the walk in the dimension $d$. A walker starts at the origin in the lattice $\Z^{d}$. The random variables $X_{1},X_{2},\ldots$ are independent and identically distributed with probabilities
\begin{equation*}
    P(X_{i} = -e_{d}) + \ldots + P(X_{i} = -e_{2}) + P(X_{i} = -e_{1}) + P(X_{i} = e_{1}) = P(X_{i} = e_{2}) + \ldots + P(X_{i} = e_{d}) = \frac{1}{2d}.
\end{equation*}
for all valid $i$. The random walk here is defined as $S_{n} = X_{1} + \ldots + X_{n}$. We ask the probabilitiy that $S_{n}$ returns to the origin. Denote by $u_{2n}$ the probability that $S_{2n} = 0$, and denote by $f_{2n}$ the probability that the first return to the origin occurs at time $2n$. By conditioning,
\begin{align}
    u_{2n} = \sum_{k=0}^{n} f_{2k} u_{2n-2k}.
\end{align}
If $U(s)$ and $F(s)$ are the appropriate generating functions, then we can show that
\begin{align}
    U(s)-1 = F(s)U(s) \implies U(s) = \frac{1}{1-F(s)}.
\end{align}
Both $U(s)$ and $F(s)$ are covergent for $\abs{s} < 1$. For each $N$,
\begin{equation}
    \sum_{n=0}^{N} u_{2n} \leq \lim_{s \to 1^{-}} U(s) \leq \sum_{n=0}^{\infty} u_{2n}.
\end{equation}

\begin{lemma}
    A random walk on $\Z^{d}$ return to the origin with probability 1 if and only if $\sum u_{2n} = \infty$.
\end{lemma}
\begin{proof}
    Suppose $F(1) < 1$. Then, $\lim{s \to 1^{-}} U(s) < \infty$ and, consequently, $\sum_{n=0}^{\infty} u_{2n} < \infty$. The converse can be proved by reversing the steps.
\end{proof}
The lemma tells us that to see the probability that the random walk returns to the origin, we only need to compute $\sum_{n=0}^{\infty} u_{2n}$.

For $d = 2$, we need the number of $e_{i}$ jumps to be equal to the number of $-e_{i}$ jumps for $i = 1,2$. We have
\begin{align}
    u_{2n} &= \frac{1}{4^{2n}} \sum_{j=0}^{n} \binom{2n}{j} \binom{2n-j}{j} \binom{2n-2j}{n-j} \binom{n-j}{n-j} = \frac{1}{4^{2n}} \binom{2n}{n} \sum_{j=0}^{n} \binom{n}{j}^{2} = \frac{1}{4^{2n}} \binom{2n}{n}^{2} \notag \\
    &\sim \frac{2}{2\pi} \frac{n^{4n+1}}{n^{4n+2}} = \frac{1}{\pi n}.
\end{align}
Since this is any asymptotic relationship, $u_{2n} \geq \frac{(1-\varepsilon)}{\pi n}$ for large $n$. Thus, we can show $\sum u_{2n} = \infty$.

For $d = 3$,
\begin{align}
    u_{2n} &= \frac{1}{6^{2n}} \sum_{j,k = 0; j+k \leq n}^{n} \frac{(2n)!}{j! j! k! k! (n-j-k)! (n-j-k)!} = \frac{1}{6^{2n}} \sum_{j,k=0l j+k \leq n}^{\infty} \frac{(2n)!}{(j!)^{2}(k!)^{2}((n-j-k)!)^{2}} \notag \\
    &= \frac{1}{2^{2n}} \binom{2n}{n} \sum_{j,k;j+k \leq n} \left( \frac{n!}{j!k!(n-j-k)!} \frac{1}{3^{n}} \right)^{2}.
\end{align}
$\frac{1}{2^{2n}} \binom{2n}{n}$ behaves asymptotically as $\frac{1}{\sqrt{\pi n}}$. For the rest of the term,
\begin{align}
    \sum_{j,k; j+k \leq n} \left( \frac{n!}{j!k!(n-j-k)!} \frac{1}{3^{n}} \right)^{2} \leq t_{n} \sum_{j,k;j+k \leq n} \frac{n!}{j!k!(n-j-k)!} \frac{1}{3^{n}}
\end{align}
where $t_{n} = \max_{j,k;j+k \leq n} \frac{n!}{j!k!(n-j-k)!}$. The maximum is attained roughly when $j,k \approx \frac{n}{3}$. Also, the summation behaving as the upper bound is just unity. Thus,
\begin{align}
    \sum_{j,k; j+k \leq n} \left( \frac{n!}{j!k!(n-j-k)!} \frac{1}{3^{n}} \right)^{2} \leq t_{n} \approx \frac{n!}{((\frac{n}{3})!)^{3} 3^{n}} \sim \frac{C}{n}
\end{align}
for some constant $C$. Therefore,
\begin{equation}
    u_{2n} \leq \frac{C^{\ast}}{n^{\frac{3}{2}}} \implies \sum u_{2n} < \infty \implies F(1) < 1.
\end{equation}

\begin{theorem}[\eax{Polya}]
    A random walk in 1 or 2 dimensions will always return to the origin with probability 1. A random walk in more than 2 dimensions has a positive probability of never returning to the origin.
\end{theorem}

\section{.}
\textit{January 31st.}\\
Recall that in the first course, we studied that if $X_{n} \sim \text{Bin}(n,p_{n})$ with $np_{n} \to \lambda$ as $n \to \infty$, then
\begin{equation}
    \lim_{n \to \infty} P(X_{n} = k) = P(\text{Poisson}(\lambda) = k) \text{ for } k \geq 0. 
\end{equation}
We now extend upon this idea.
\begin{theorem}[\eax{Continuity theorem}]
    Suppor for each $n$ the sequence $a_{0,n}, a_{1,n}, \ldots$ is a probability distribution, that is,
    \begin{equation}
        a_{k,n} \geq 0 \text{ for all } k \text{ and } \sum_{k=0}^{\infty} a_{k,n} = 1.
    \end{equation}
    Let $A^{(n)}(s)$ denote the generating function for $\{a_{k,n}\}_{k \geq 0}$, that is,
    \begin{equation}
        A^{(n)}(s) = \sum_{k=0}^{\infty} a_{k,n} s^{k} \text{ for all } n.
    \end{equation}
    Then $a_{k} = \lim_{n \to \infty} a_{k,n}$ exists for all $k$ (\emph{statement $\star$}) if and only if $A(s) = \lim_{n \to \infty} A^{(n)}(s)$ exists for all $0 < s < 1$ (\emph{statement $\star \star$}). In this case, $A(s) = \sum_{k = 0}^{\infty} a_{k} s^{k}$.
\end{theorem}
\begin{proof}
    Assume statement $\star$. Thus, $\abs{a_{k,n} - a_{k}} \leq 1$ for all $n$ large enough. If we now fix $0 < s < 1$, then for some $K$ and a fixed $\varepsilon > 0$, we have
    \begin{align}
        \abs{A^{(n)}(s) - A(s)} &= \abs{\sum_{k=0}^{\infty} a_{k,n} s^{k} - \sum_{k=0}^{\infty} a_{k} s^{k}} \\
        &= \abs{\sum_{k=0}^{K} a_{k,n} s^{k} + \sum_{k=K+1}^{\infty} a_{k,n} s^{k} - \sum_{k=0}^{K} a_{k}s^{k} - \sum_{k=K+1}^{\infty} a_{k}s^{k}} \notag \\
        &\leq \abs{\sum_{k=0}^{K} a_{k,n} s^{k} - \sum_{k=0}^{K} a_{k}s^{k}} + \abs{\sum_{k=K+1}^{\infty} (a_{k,n}-a_{k}) s^{k}} \notag \\
        &\leq \abs{\sum_{k=0}^{K} a_{k,n} s^{k} - \sum_{k=0}^{K} a_{k}s^{k}} + \frac{s^{K+1}}{1-s}.
    \end{align}
    We can choose $K$ such that the second term becomes less than $\varepsilon$, and we can choose $N$ such that for all $n \geq N$, the first term becomes smaller than $\varepsilon$. Therefore, the entire term becomes less than $2\varepsilon$.

    For the converse, assume statement $\star \star$. $A(s)$ is monotonic in $s$; $A(0) = \lim_{s \to 0^{-}} A(s)$. We sandwich as follows---
    \begin{align}
        a_{0,n} &\leq A^{(n)}(s) \leq a_{0,n} + \frac{s}{1-s} \notag \\
        \implies A^{(n)}(s) - \frac{s}{1-s} &\leq a_{0,n} \leq A^{(n)}(s) \notag.
    \end{align}
    Letting $n$ grow to infinity,
    \begin{align}
        A(s) - \frac{s}{1-s} &\leq \liminf_{n \to \infty} a_{0,n} \leq \limsup_{n \to \infty} a_{0,n} \leq A(s).
    \end{align}
    If $s \to 0$, note that $\lim_{n \to \infty} a_{0,n} = A(0)$. Now define
    \begin{align}
        B^{(n)}(s) = \frac{A^{(n)}(s) - a_{0,n}}{s} \to \frac{A(s)-A(0)}{s} \to A'(0).
    \end{align}
    Working similarly,
    \begin{align}
        a_{1,n} &\leq B^{(n)}(s) \leq a_{1,n} + \frac{s}{1-s} \\
        \implies B^{(n)}(s) - \frac{s}{1-s} &\leq a_{1,n} \leq B^{(n)}(s).
    \end{align}
    If we again proceed as shown, we will get $B(0) = \lim_{n \to \infty} a_{1,n}$ and $a_{1,n} \to A'(0) = a_{1}$. Thus, induction is in play here.
\end{proof}
\begin{example}
    Let us work with the binomial distribution example given before. We have $X_{n} \sim \text{Bin}(n,p_{n})$ with $np_{n} \to \lambda$. We have
    \begin{align}
        A^{(n)}(s) &= \sum_{k=0}^{\infty} P(X_{n} = k) s^{k} = ((1-p_{n}) + p_{n}s)^{n} = (1+p_{n}(s-1))^{n} \notag \\
        \implies \lim_{n \to \infty} A^{(n)}(s) &= \lim_{n \to \infty} \left( 1 + \frac{np_{n}}{n}(s-1) \right)^{n} = e^{\lambda (s-1)} = E[s^{\text{Poisson}(\lambda)}].
    \end{align}
    Thus, we have shown the prior statement.
\end{example}
\begin{example}
    We have $X_{1}^{(n)}, X_{2}^{(n)}, \ldots, X_{n}^{(n)}$ independent, with $X_{i}^{(n)} \sim \text{Ber}(p_{i}^{(n)})$ for $1 \leq i \leq n$. Let $S_{n} = X_{1}^{(n)} + X_{2}^{(n)} + \ldots X_{n}^{(n)}$. We have
    \begin{align}
        E[s^{S_{n}}] = \prod_{i=1}^{n} E[s^{X_{i}^{(n)}}] = \prod_{i=1}^{n} \left( (1-p_{i}^{(n)}) + p_{i}^{(n)} s \right) = \exp \left( \ln \prod_{i=1}^{n} (\ldots) \right).
    \end{align}
    Assume that $\lim_{n \to \infty} \sum_{i=1}^{n} p_{i}^{(n)} = \lambda$ and $\lim_{n \to \infty} \max_{i} p_{i}^{(n)} = 0$. Thus,
    \begin{align}
        \exp \left( \sum_{i=1}^{n} \ln (1 + p_{i}^{(n)} (s-1)) \right) &= \exp \left( \sum_{i=1}^{n} p_{i}^{(n)}(s-1) - \frac{(p_{i}^{(n)}(s-1))^{2}}{2} + \ldots \right) \notag \\
        &= \exp \left( (s-1)\sum_{i=1}^{n} p_{i=1}^{n} - \sum_{i=1}^{n} o(p_{i}^{(n)}(s-1))  \right) \\
        &\to e^{\lambda (s-1)}.
    \end{align}
\end{example}
\begin{example}
    Let $X^{(n)} \sim \text{NB}(r_{n},p)_{n}$, the number of successes before the $r_{n}^{\text{th}}$ success in trials with success probability $p_{n}$. Let $p_{n} \to 1$ and $r_{n} \to \infty$ such that $r_{n}(1-p_{n}) \to \lambda$, where $\lambda$ is fixed. We would then have $P(X^{(n)} = k) \to P(\text{Poisson}(\lambda) = k)$.
\end{example}

\section{Gambler's Ruin}
We take a look at a gambler, who has starting capital $z$. His probability of a success $(+1)$ is $p$, and of a failure $(-1)$ is $q$. We ask the probability $q_{z}$ that the gambler reaches $0$ before $a$ when he starts at capital $z$. Note that $q_{z}$ satisfies
\begin{equation}
    q_{z} = pq_{z+1}+qq_{z-1} \text{ for } 1 < z < a-1\; (\text{statement }\star), \text{ with } q_{0} = 1,\; q_{a} = 0\; (\text{statement }\star \star).
\end{equation}
We look at two cases, beginning with the case when $p \neq q$. Note that $q_{z} = 1$ for $1 \leq z \leq a-1$ solves for statement $\star$, ignoring statement statement $\star \star$ and ignoring probaility for now. $q_{z} = (\frac{q}{p})^{z}$ for $1 \leq z \leq a-1$ also solves for statement $\star$. Therefore, $A+B(\frac{q}{p})^{z}$ solves statement $\star$. Now, we plug in the boundary conditions given by statement $\star \star$. Solving the equations $A+B = 1$ and $A + B(\frac{q}{p})^{a} = 0$ gives us
\begin{equation}
    B = \frac{1}{1-(\frac{q}{p})^{a}},\; A = 1 - \frac{1}{1-(\frac{q}{p})^{a}}.
\end{equation}
Plugging this in, gives us
\begin{equation}
    q_{z} = \frac{(\frac{q}{p})^{a} - (\frac{q}{p})^{z}}{(\frac{q}{p})^{a} - 1}.
\end{equation}
Note that we were working the case when $p \neq q$. For $p = q$, this solution does not work.

We work the case for when $p = q = \frac{1}{2}$. Again, $q_{z} = 1$ for $1 \leq z \leq a-1$ satisfies statement $\star$. We also find that $q_{z} = z$ for $1 \leq z \leq a-1$ also satisfies this statement. Hence, we look for $A+Bz$ which satisfies boundary condition given by statement $\star \star$. Solving, this gives us
\begin{align}
    q_{z} = 1 - \frac{z}{a}.
\end{align}
Note that we are yet to show $p_{z} + q_{z} = 1$. If we instead focus on a \textit{second} gambler playing against our gambler, we would have a gambler with capital $a-z$, and probability of success $q$ and probability of failure $p$. Replacing $z$ by $a-z$ and $q$ by $p$ and $p$ by $q$ in our formed equations would give us $p_{z} + q_{z} = 1$.

Let us intuitively look at our equations with a table of examples.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c}
        \textbf{$p$} & \textbf{$q$} & \textbf{$z$} & $a$ & $q_{z}$ \\ 
        \hline
        0.45 & 0.55 & 9 & 10 & 0.21 \\
        0.45 & 0.55 & 90 & 100 & 0.866 \\
        0.45 & 0.55 & 99 & 100 & 0.182 \\
        0.5 & 0.5 & 9 & 10 & 0.1 \\
        0.5 & 0.5 & 90 & 100 & 0.1 \\
        0.5 & 0.5 & 99 & 100 & 0.01
    \end{tabular}
    \caption{Probability of ruin ($q_{z}$) given initial parameters.}
\end{table}

Note that the expected net gain is given by
\begin{equation}
    (a-z)(1-q_{z}) - zq_{z} = a(1-q_{z})-z.
\end{equation}
If we plug in this into our first three rows of our table, we would have $-1.1,\; -77,\; -18$. If one is gambling under such condition, we must start with big capital $z$ and low target $a-z$.

\subsection{Duration of the Game}
We look at $D_{z}$, the expected duration of a game starting at $z$; the expected time before the gambler hits $a$ or $0$. The linear recurrance satisfied here is
\begin{equation}
    D_{z} = pD_{z+1} + qD_{z-1} + 1 \text{ with boundary conditions } D_{0} = 0,\; D_{a} = 0.
\end{equation}
For $p \neq q$,
\begin{equation}
    D_{z} = \frac{z}{q-p} - \frac{a}{q-p} \left( \frac{1-(\frac{q}{p})^{z}}{1-(\frac{q}{p})^{n}} \right).
\end{equation}
For $p = q = \frac{1}{2}$,
\begin{equation}
    D_{z} = z(a-z).
\end{equation}

\chapter{JOINT CONTINUOUS DISTRIBUTIONS}

\section{Introduction}
\textit{February 4th.}

Recall that $X:\Omega \to \R$ is \eax{continuous random variable} if it has a probability density function $f_{X}:\R \to \R_{\geq 0}$. In this case, if $A \subseteq \R$, then
\begin{equation}
    P(X \in A) = \int_{A} f_{X}(x) dx.
\end{equation}
For a minute $dx$,
\begin{equation}
    P(X \in [x,x+dx]) \approx f_{X}(x) dx.
\end{equation}
Two random variables $X$ and $Y$ are termed \eax{jointly continuous} if there exists a function $f:\R^{2} \to R$ such that for $A \subseteq \R^{2}$,
\begin{equation}
    P((X,Y) \in A) = {\iint}_{A} f(x,y) dx dy.
\end{equation}
In this case, $f$ is termed the \eax{joint probability density function} of $X$ and $Y$. In particular, if $B,C \subseteq \R$, then
\begin{equation}
    P(X \in C, \; Y \in B) = P((X,Y) \in C \times B) = \int_{B} \int_{C} f(x,y) dx dy.
\end{equation}
The \eax{joint cumulative density function} is given as
\begin{equation}
    F(a,b) = P(X \leq a, \; Y \leq b) = P((X,Y) \in (-\infty, a] \times (-\infty, b]) = \int_{-\infty}^{b} \int_{-\infty}^{a} f(x,y) dx dy.
\end{equation}
There is, again, joint analagous versions of the single random variables cases;
\begin{equation}
    \frac{\partial^{2}}{\partial{a} \partial{b}} F(a,b) = f(a,b)
\end{equation}
and
\begin{equation}
    P(X \in [a,a+da], \; Y \in [b,b+db]) \approx f(a,b) da db.
\end{equation}
Note that $(X,Y)$ being jointly continuous implies that both $X$ and $Y$ are continuous. Indeed, if $A \subseteq \R$, then
\begin{equation}
    P(X \in A) = P(X \in A, \; Y \in \R) = \int_{A} \int_{\R} f(x,y) dy dx = \int_{A} f_{X}(x) dx.
\end{equation}
In this case, the inner intergral becomes the probability density funciton of $X$.

\begin{example}
    We are given the joint probability density function of $X$ and $Y$ as
    \begin{equation*}
        f(x,y) = \begin{cases}
            2e^{-x}e^{-2y} &\text{ if } 0 < x < \infty, \; 0 < y < \infty,\\
            0 &\text{ if otherwise.}
        \end{cases}
    \end{equation*}
    We are to compute $P(X>1,Y<1)$, $P(X<Y)$ and $P(X<a)$. This is left as an exercise.
\end{example}

\begin{example}
    Suppose $(X,Y)$ represents a random points inside a circle of radius $R$. The probability density function is given by
    \begin{equation*}
        f(x,y) = \begin{cases}
            \frac{1}{\pi R^{2}} &\text{ if } x^{2}+y^{2} \leq R^{2},\\
            0 &\text{ if otherwise.}
        \end{cases}
    \end{equation*}
    Compute $f_{X},f_{Y}$ and $f_{D}$ where $D = \sqrt{X^{2}+Y^{2}}$.
\end{example}
We call $X$ and $Y$ indepedent if, for $A,B \subseteq \R$,
\begin{equation}
    P(X \in A, Y \in B) = P(X \in A) \cdot P(Y \in B).
\end{equation}
If we take $A = (-\infty, a], B = (-\infty, b]$, then
\begin{equation*}
    F(a,b) = F_{X}(a) F_{Y}(b) \implies f(a,b) = f_{X}(a) f_{Y}(b).
\end{equation*}
In fact, all three conditions are equivalent. Note that everything discussed so far may be extended to more than two random variables. If $A \subseteq \R^{n}$ and $(X_{1},X_{2},\ldots,X_{n})$ are jointly continuous, then
\begin{equation}
    P((X_{1},X_{2},\ldots,X_{n}) \in A) = \idotsint_{A} f(x_{1},x_{2},\ldots,x_{n}) dx_{1} \cdots dx_{n}
\end{equation}
where $f: \R^{n} \to \R_{\geq 0}$.

\begin{example}[\eax{Buffin's needle problem}]
Suppose we have a table of width $D$ and length sufficient. We throw a needle of length $L \leq D$ randomly (and necessarily) on this table. We are to find the probability that the needle will hand over either side of the table separated by width $D$. Denote by $X$ the distance of the midpoint of the needle from the nearest edge, and denote by $\theta$ the angle is subtends with respect to the vertical.

Note that $X \in [0,\frac{D}{2}]$, and $\theta \in [0,\frac{\pi}{2}]$. By the procedure, one can assume these are independent. We look at the probability that the length of half the needle is greater than the length of the hypotenuse determined by $X$ and $\theta$---
\begin{equation}
    P(\frac{L}{2} > \frac{X}{\cos \theta}) = \int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{L}{2} \cos \theta} \frac{2}{D} \cdot \frac{2}{\pi} dx d\theta = \frac{2L}{D \pi} \int_{0}^{\frac{\pi}{2}} \cos \theta d \theta = \frac{2L}{D \pi}.
\end{equation}
\end{example}

\begin{proposition}
    The jointly continuous random variables $X$ and $Y$ are independent if and only the probability density function can be factored into functions of $x$ and $y$ respectively.
\end{proposition}

Suppose that $X$ and $Y$ are independent and jointly continuous random variables. Then $f(x,y) = f_{X}(x) f_{Y}(y)$. Let us compute the cumulative density function of $X+Y$.
\begin{equation}
    P(X+Y \leq a) = \int_{-\infty}^{\infty} \int_{-\infty}^{a-x} f_{X}(x) f_{Y}(y) dydx = \int_{-\infty}^{\infty} f_{X}(x) F_{Y}(a-x) dx = \int_{-\infty}^{\infty} f_{Y}(y) F_{X}(a-y) dy.
\end{equation}
Differentiating to give the probability density function of $X+Y$,
\begin{equation}
    \frac{d}{da} F_{X+Y}(a) = \int_{-\infty}^{\infty} f_{X}(x) f_{Y}(a-x) dx = \int_{-\infty}^{\infty} f_{X}(a-y) f_{Y}(y) dy.
\end{equation}
This is the convolution of $f_{X}$ and $f_{Y}$.\\ \\
\textit{February 11th.}

Suppose we have $U_{1},U_{2},\ldots,U_{n}$ which are independent and have the distribution $\text{Uniform}(0,1)$. From $\{1,2,\ldots,n\}$, if we choose a subset of size $k$, then each subset of size $k$ has probability $\frac{1}{\binom{n}{k}}$. We will generate $I_{1},I_{2},\ldots,I_{n}$, where exactly $k$ of them are 1. $I_{1}$ is defined as
\begin{equation}
    I_{1} = \begin{cases}
        1 &\text{ if } U_{1} < \frac{k}{n},\\
        0 &\text{ if otherwise}.
    \end{cases}
\end{equation}
Once $I_{1},I_{2}<\ldots,I_{i}$ are determined, we can define
\begin{equation}
    I_{i+1} = \begin{cases}
        1 &\text{ if } U_{i+1} < \frac{k-(I_{1}+I_{2}+\ldots+I_{i})}{n-1},\\
        0 &\text{ if otherwise}.
    \end{cases}
\end{equation}
Notice that if $I_{1}+I_{2}+\ldots+I_{i} = k$, then $I_{i+1},I_{i+2},\ldots = 0$. Note that $P(I_{1}=1) = \frac{k}{n}$, and
\begin{equation}
    P(I_{i+1}=1|I_{1},I_{2},\ldots,I_{i}) = \frac{k-\sum_{r=1}^{i} I_{r}}{n-i}, \text{ for } 1 < i \leq n.
\end{equation}
If we do induction on $k+n$, we know the case $k+n = 2$ to be true ($k=1,n=1$). We assume for all the cases $k+n \leq l$, and suppose that $k+n = l+1$. Consider any subset of size $k$, say, $i_{1} \leq i_{2} \leq \ldots \leq i_{k}$.
\begin{itemize}
    \item Case I, where $i_{1} = 1$. Here,
    \begin{align}
        &P(I_{i_{1}}=I_{i_{2}}=\ldots=I_{i_{k}} = 1, I_{j} = 0 \text{ otherwise}) \notag \\ \notag
        =\; &P(I_{1}=1) \cdot P(I_{i_{2}}=\ldots=I_{i_{k}} = 1, I_{j} = 0 \text{ otherwise} |I_{1}=1) \notag \\
        =\; &\frac{k}{n} \frac{1}{\binom{n-1}{k-1}} = \frac{1}{\binom{n}{k}}. \notag
    \end{align}

    \item Case II, where $i_{1} > 1$. In this case,
    \begin{align}
        &P(I_{i_{1}}=0,I_{i_{2}}=\ldots=I_{i_{k}} = 1, I_{j} = 0 \text{ otherwise}) \notag \\ \notag
        =\; &P(I_{1}=0) \cdot P(I_{i_{2}}=\ldots=I_{i_{k}} = 1, I_{j} = 0 \text{ otherwise} |I_{1}=0) \notag \\
        =\; &\left(1-\frac{k}{n}\right) \frac{1}{\binom{n-1}{k}} = \frac{1}{\binom{n}{k}}. \notag
    \end{align}
\end{itemize}

\section{Some Distributions}
\subsection{Gamma Random Variable}
We have $X \sim \text{Gamma}(t,\lambda)$ where $\lambda > 0$ and $t > 0$. The probability density function is defined as
\begin{equation}
    f_{X}(x) = \frac{\lambda e^{-\lambda y} (\lambda y)^{t-1}}{\Gamma(t)} \text{ for } 0 < y < \infty.
\end{equation}
Here, $\Gamma$ is the gamma function.
\begin{proposition}
    If $X \sim \emph{Gamma}(s,\lambda)$ and $Y \sim \emph{Gamma}(t,\lambda)$, then $X+Y \sim \emph{Gamma}(s+t,\lambda)$.
\end{proposition}
\begin{proof}
    Via convolution, we know that
    \begin{align}
        f_{X+Y}(a) &= \int_{0}^{a} f_{X}(a-y)f_{Y}(y) dy \notag \\
        &= \frac{1}{\Gamma(t) \Gamma(s)} \int_{0}^{a} \lambda e^{-\lambda(a-y)}(\lambda(a-y))^{t-1} \lambda e^{-\lambda y} (\lambda y)^{s-1} dy \notag \\
        &= \frac{\lambda e^{-\lambda a} (\lambda a)^{s+t-1}}{\Gamma(t+s)} \frac{\Gamma(t+s)}{\Gamma(t) \Gamma(s)} \int_{0}^{a} \frac{(a-y)^{t-1}y^{s-1}}{a^{s+t-1}} dy \notag \\
        &= \frac{\lambda e^{-\lambda a} (\lambda a)^{s+t-1}}{\Gamma(t+s)} \frac{\Gamma(t+s)}{\Gamma(t) \Gamma(s)} \int_{0}^{1} (1-u)^{t-1}u^{s-1} du \notag.
    \end{align}
    If we integrate this probability distribution function, we must have
    \begin{align}
        \int_{0}^{\infty} f_{X+Y}(a)da &= 1 \\
        \implies \frac{\Gamma(s+t)}{\Gamma(s)\Gamma(t)} \int_{0}^{1} (1-u)^{t-1}u^{s-1} du = 1.
    \end{align}
    This gives rise to the Beta function, defined as $\int_{0}^{1} (1-u)^{t-1}u^{s-1}du = B(t,s) = \frac{\Gamma(s)\Gamma(t)}{\Gamma(s+t)}$.
\end{proof}
One corollary that can be inferred from here is that if $X_{1},X_{2},\ldots,X_{n}$ are independent $\text{Gamma}(t_{i},\lambda)$ distributions, then $\sum_{i=1}^{n} X_{i} \sim \text{Gamma}(\sum_{i=1}^{n} t_{i}, \lambda)$. We also notice that $X \sim \text{Exp}(\lambda) = \text{Gamma}(1,\lambda)$. If we take the $X_{i}$'s to be all the exponential distribution, then $S_{n} = X_{1} + X_{2} + \ldots + X_{n} \sim \text{Gamma}(n,\lambda)$. The density of $S_{n}$ is given by
\begin{equation}
    g_{n}(y) = \frac{\lambda(\lambda y)^{n-1}e^{-\lambda y}}{\Gamma(n)} = \frac{\lambda(\lambda y)^{n-1}e^{-\lambda y}}{(n-1)!} \text{ for } 0 < y < \infty
\end{equation}
with the cumulative distribution function
\begin{equation}
    G_{n}(y) = \frac{1}{(n-1)!} \int_{0}^{y} \lambda (\lambda a)^{n-1} e^{-\lambda a} da.
\end{equation}

\begin{example}
    Suppose we have buses that each take time $X_{i} \sim \text{Exp}(\lambda)$. Suppose we fix a time $t$, and define $N(t)$ to be the number of buses seen up to time $t$. We ask the probability $P(N(t)=n)$.
    \begin{align}
        P(N(t)=n) &= P(X_{1}+X_{2}+\ldots+X_{n} \leq t, X_{1}+X_{2}+\ldots+X_{n+1}>t) \\
        &= P(X_{1}+\ldots+X_{n} \leq t) - P(X_{1}+\ldots+X_{n} \leq t, X_{1}+\ldots+X_{n+1}\leq t) \notag \\
        &= P(X_{1}+\ldots+X_{n} \leq t) - P(X_{1} + \ldots + X_{n+1} \leq t) \notag \\
        &= G_{n}(t) - G_{n+1}(t) = e^{-\lambda t} \frac{(\lambda t)^{n}}{n!}.
    \end{align}
\end{example}
Notice that there is a unique $k$ for which $S_{k-1} < t \leq S_{k}$. Define $X_{k}$ to be $S_{k} - S_{k-1}$.
\begin{proposition}
    The $X_{k}$ satisfying $S_{k-1} < t \leq S_{k}$ has density
    \begin{equation}
        v_{t}(x) = \begin{cases}
            \lambda^{2}xe^{-\lambda x} &\text{ if } 0 < x \leq t,\\
            \lambda(1+\lambda t) e^{-\lambda x} &\text{ if } x > t.
        \end{cases}
    \end{equation}
\end{proposition}
\begin{proof}
    Let $k$ denote the index for which $S_{k-1} < t \leq S_{k}$. Define $L_{t} = S_{k} - S_{k-1}$. Let us first compute the cumulative distribution function.
    \begin{itemize}
        \item Case I, when $x < t$. Note that $L_{t} \leq x \iff$ there exist $n,y$ such that $t-x \leq y < t$, $S_{n} = y$, $t-y \leq X_{n+1} \leq x$.
        \begin{align}
            P(L_{t} \leq x) &= \sum_{n=1}^{\infty} \int_{t-x}^{t} \int_{t-y}^{x} f_{S_{n},X_{n+1}}(y,z) dz dy \notag \\
            &= \sum_{n=1}^{\infty} \int_{t-x}^{t} \int_{t-y}^{x} g_{n}(y) f_{X_{n+1}}(z) dz dy \notag \\
            &= \sum_{n=1}^{\infty} \int_{t-x}^{t} g_{n}(y) [e^{-\lambda(t-y)} - e^{-\lambda x}] dy \notag \\
            &= \lambda \int_{t-x}^{t} [e^{-\lambda(t-y)}-e^{-\lambda y}] dy.
        \end{align}
        
        \item Case II, when $x > t$. Note that
        \begin{equation}
            \{L_{t} \leq x\} = \{t < S_{1} \leq x \} \cup \bigcup_{n=1}^{\infty} \{\text{bus $n$ arrives at $y < t$ and $t-y < X_{n+1}\leq x$}\}.
        \end{equation}
        These are all disjoint events. $P(t < S_{1} \leq x) = e^{-\lambda t} - e^{-\lambda x}$, and
        \begin{equation}
            P(\text{bus $n$ arrives at $y < t$ and $t-y < X_{n+1}\leq x$}) = \int_{0}^{t} \int_{t-y}^{x} g_{n}(y) f_{X_{n+1}}(z) dz dy.
        \end{equation}
        Adding up the probabilities of the disjoint events, we have
        \begin{align}
            P(L_{t} \leq x) &= e^{-\lambda t} - e^{-\lambda x} + \sum_{n=1}^{\infty} \int_{0}^{t} g_{n}(y) [e^{-\lambda(t-y)} - e^{-\lambda x}] dy \notag \\
            &= e^{-\lambda t} - e^{-\lambda x} + \lambda \int_{0}^{t} [e^{-\lambda(t-y)}-e^{-\lambda x}]dy \notag \\
            &= e^{-\lambda t} - e^{-\lambda x} + 1 - e^{-\lambda t} - \lambda t e^{-\lambda x}.
        \end{align}
    \end{itemize}
\end{proof}

\section{Conditional Distribution}
\textit{February 25th.}

Let $X$ and $Y$ hae a joint probability density function given by $f(x,y)$. The \eax{conditional probability density function} of $X$ given $Y=y$ is
\begin{align}
    f_{X|Y}(x|y) = \frac{f(x,y)}{f_{Y}(y)}
\end{align}
and it is defined for $f_{Y}(y) > 0$. The motivation behind defining this is as follows---
\begin{align}
    f_{X|Y}(x|y)dx = \frac{f(x,y)dxdy}{f_{Y}(y)dy} = \frac{P(x \leq X \leq x + dx, y \leq Y \leq y + dy)}{P(y \leq Y \leq y + dy)} = P(x \leq X \leq x + dx | y \leq Y \leq y + dy).
\end{align}
If one is given the conditional probability density function, we can do the following computation as well,
\begin{align}
    P(X \in A | Y = y) = \int_{A} f_{X|Y}(x|y) dx.
\end{align}
We can also make sense of a \eax{conditional cumulative distribution function} of $X$ given $Y=Y$.
\begin{align}
    F_{X|Y}(a|y) = P(X \leq a | Y = y) = \int_{-\infty}^{a} f_{X|Y}(x|y)dx
\end{align}

\begin{remark}
    If $X$ and $Y$ are independent, then the joint density factorizes into the product of the marginals which results in
    \begin{align}
        f_{X|Y}(x|y) = \frac{f(x,y)}{f_{Y}(y)} = \frac{f_{X}(x)f_{Y}(y)}{f_{Y}(y)} = f_{X}(x).
    \end{align}
\end{remark}

\begin{example}
    Suppose the joint density of $X$ and $Y$ is given as
    \begin{align}
        f(x,y) = \begin{cases}
            e^{-\frac{x}{y}}e^{-y}y^{-1} &\text{ if } 0 < x < \infty, 0 < y < \infty\\
            0 &\text{ if otherwise.}
        \end{cases}
    \end{align}
    Let us compute $P(X > 1 | Y = y)$ for $0 < y < \infty$.
    \begin{align}
        f_{X|Y}(x|y) = \frac{f(x,y)}{f_{Y}(y)} = \frac{e^{-\frac{x}{y}}e^{-y}}{y \int_{0}^{\infty}\frac{e^{-\frac{x}{y}}e^{-y}}{y}dx} = \frac{1}{y}e^{-\frac{x}{y}} \text{ for } 0 < x < \infty.
    \end{align}
    Thus,
    \begin{align}
        P(X > 1 | Y = y) = \int_{1}^{\infty} \frac{1}{y} e^{-\frac{x}{y}} dx = e^{-\frac{1}{y}}.
    \end{align}
\end{example}

\subsection{The $t$-distribution}
Suppose we have $Y \sim \chi_{n}^{2} \equiv \text{Gamma}(\frac{n}{2},\frac{1}{2})$ and $Z \sim N(0,1)$ with both independent. Then the \eax{$t$-distribution} with $n$ degrees of freedom is defined as
\begin{align}
    T = \frac{Z}{\sqrt{Y/n}} = \sqrt{n} \frac{Z}{\sqrt{Y}}.
\end{align}
If a $Y = y$ is fixed, then $T = \sqrt{\frac{n}{y}}Z \sim N(0, \frac{n}{y})$. Thus,
\begin{align}
    f_{T|Y}(t|y) &= \frac{1}{\sqrt{2\pi \frac{n}{y}}} e^{-\frac{1}{2}\frac{t^{2}y}{n}} \\
    \implies f_{T,Y}(t,y) &= f_{T | Y}(t|y)f_{Y}(y) = \frac{1}{\sqrt{2\pi \frac{n}{y}}} e^{-\frac{1}{2}\frac{t^{2}y}{n}} \left( \frac{e^{-\frac{y}{2}}y^{\frac{n}{2}-1}}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})} \right) \text{ for } t \in \R, y > 0.
\end{align}
Thus, the probability density function for $T$ can be found out as
\begin{align}
    f_{T}(t) = \int_{0}^{\infty} f_{T|Y}(t|y)dy = \int_{0}^{\infty} \frac{1}{\sqrt{2\pi n}} \cdot \frac{e^{-\frac{1}{2}(1+\frac{t^{2}}{n})y}y^{\frac{n-1}{2}}}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}dy.
\end{align}
If we let $c = \frac{1}{2}(1+\frac{t^{2}}{n})$ and make the change of variable from $x$ to $cy$, the intergral transforms as
\begin{align}
    \frac{c^{-\frac{n+1}{2}}}{\sqrt{\pi n} 2^{\frac{n+1}{2}} \Gamma(\frac{n}{2})} \int_{0}^{\infty} e^{-x} x^{\frac{(n-1)}{2}} dx = \frac{\Gamma(\frac{n+1}{2})}{\sqrt{\pi n} \Gamma(\frac{n}{2})} \left( 1+\frac{t^{2}}{n} \right)^{-\frac{n+1}{2}}.
\end{align}
Note that as $n \to \infty$,
\begin{align}
    f_{T}(t) \to \frac{1}{\sqrt{2\pi}} e^{-\frac{t^{2}}{2}}.
\end{align}

\subsection{The Bivariate Normal Distribution}
Five parameters are used here--- $\mu_{x},\mu_{y} \in \R$, $\sigma_{x}, \sigma_{y} > 0$, and $-1 < \rho < 1$. The joint probability density function is given by
\begin{align}
    f(x,y) = \frac{1}{2\pi \sigma_{x}\sigma_{y}\sqrt{1-\rho^{2}}} \exp \left( -\frac{1}{2(1-\rho^{2})} \left( \left( \frac{x-\mu_{x}}{\sigma_{x}} \right)^{2} + \left( \frac{y-\mu_{y}}{\sigma_{y}} \right)^{2} -2\rho \left( \frac{(x-\mu_{x})(y-\mu_{y})}{\sigma_{x} \sigma_{y}} \right) \right) \right).
\end{align}
We find the conditional density of $X$ given $Y = y$.
\begin{align}
    f_{X|Y}(x|y) &= \frac{f(x,y)}{f_{Y}(y)} = c_{1}(y)f(x,y) \\
    &= c_{2}(y) \exp \left( -\frac{1}{2(1-\rho^{2})} \left( \left( \frac{x-\mu_{x}}{\sigma_{x}} \right)^{2} - 2\rho \frac{x(y-\mu_{y})}{\sigma_{x}\sigma_{y}} \right)  \right) \notag\\
    &= c_{2}(y) \exp \left( -\frac{1}{2(1-\rho^{2}) \sigma_{x}^{2}} \left( x^{2}-2x\mu_{x} + \mu_{x}^{2} - 2\frac{\rho \sigma_{x}}{\sigma_{y}} x(y-\mu_{y}) \right)  \right) \notag\\
    &= c_{3}(y) \exp \left( -\frac{1}{2(1-\rho^{2}) \sigma_{x}^{2}} \left( x^{2} -2x (\mu_{x} + \rho \frac{\sigma_{x}}{\sigma_{y}} (y - \mu_{y})) \right)  \right) \notag\\
    &= c_{4}(y) \exp \left( -\frac{1}{2(1-\rho^{2}) \sigma_{x}^{2}} \left( x - \left( \mu_{x} + \rho \frac{\sigma_{x}}{\sigma_{y}} (y - \mu_{y}) \right) \right)^{2}  \right).
\end{align}
The integral of the last term must be 1, so we can conclude that it is the probability density function must be that of the normal distribution.
\begin{align}
    X|Y=y &\sim N\left( \mu_{x} + \rho \frac{\sigma_{x}}{\sigma_{y}}(y-\mu_{y}), \sigma_{x}^{2}(1-\rho^{2}) \right),\\
    Y|X=x &\sim N\left( \mu_{y} + \rho \frac{\sigma_{y}}{\sigma_{x}}(x-\mu_{x}), \sigma_{y}^{2}(1-\rho^{2}) \right).
\end{align}
Note that $X$ and $Y$ are independent if and only if $\rho = 0$. Also,
\begin{align}
    f_{X}(x) = \frac{f(x,y)}{f_{Y|X}(y|x)} = \frac{1}{\sqrt{2\pi \sigma_{x}^{2}}} \exp \left(  -\frac{1}{2\sigma_{x}^{2}} (x-\mu_{x})^{2} \right).
\end{align}

\section{Order Statistics}
\textit{February 28th.}

Suppose we have $X_{1},X_{2},\ldots,X_{n}$ independent and identically distributed continuous random variables with common probability density function $f$ and cumulative distribution function $F$. We order the $X_{i}$'s in such a way that $X_{(1)}<X_{(2)}<\ldots<X_{(n)}$. These are termed the \eax{order statistics} corresponding to $X_{1},X_{2},\ldots,X_{n}$, and $X_{(k)}$ is termed the $k^{\text{th}}$ order statistic. Note that $X_{(1)},X_{(2)},\ldots,X_{(n)}$ takes the values $x_{1}\leq x_{2} \leq \ldots \leq x_{n}$ if and only if for some permutation $(i_{1},i_{2},\ldots,i_{n})$ of $(1,2,\ldots,n)$ we have $X_{1} = x_{i_{1}},X_{2}=x_{i_{2}},\ldots,X_{n}=x_{i_{n}}$. Also, for sufficiently small $\varepsilon > 0$,
\begin{align}
    & P\left(x_{i_{1}}-\frac{\varepsilon}{2} < X_{1} < x_{i_{1}} + \frac{\varepsilon}{2}, x_{i_{2}} - \frac{\varepsilon}{2} < X_{2} < x_{i_{2}} + \frac{\varepsilon}{2}, \ldots, x_{i_{n}} - \frac{\varepsilon}{2} < X_{n} < x_{i_{n}} + \frac{\varepsilon}{2}\right) \notag \\
    &\approx \varepsilon^{n} f(x_{i_{1}}) f(x_{i_{2}}) \cdots f(x_{i_{n}}) = \varepsilon^{n} f(x_{1}) f(x_{2}) \cdots f(x_{n}).
\end{align}
Therefore, for $x_{1} < x_{2} < \ldots < x_{n}$, we have
\begin{align}
    & P\left(x_{1}-\frac{\varepsilon}{2} < X_{(1)} < x_{1} + \frac{\varepsilon}{2}, x_{2} - \frac{\varepsilon}{2} < X_{(2)} < x_{2} + \frac{\varepsilon}{2}, \ldots, x_{n} - \frac{\varepsilon}{2} < X_{(n)} < x_{n} + \frac{\varepsilon}{2}\right) \notag \\
    &\approx n! \varepsilon^{n} f(x_{1}) \ldots f(x_{n}) \\
    &\implies f_{(X_{(1)},X_{(2)},\ldots,X_{(n)})}(x_{1},x_{2},\ldots,x_{n}) = n! f(x_{1})f(x_{2}) \cdots f(x_{n}).
\end{align}

\begin{example}
    Three people are distributed on a 1 mile long rong, uniformly. Fix $d \leq \frac{1}{2}$. We are to find the probability that no two people are less distance $d$ apart. Note that the probability density function is $f_{(X_{(1)},X_{(2)},X_{(3)})}(x_{1},x_{2},x_{3}) = 6$ for $0 \leq x_{1} < x_{2} < x_{3} \leq 1$. The probability is then given us
    \begin{align}
        P(X_{(2)}-X_{(1)} \geq d, X_{(3)} - X_{(2)} \geq d) &= \int_{0}^{1} \int_{x_{1}+d}^{1} \int_{x_{2}+d}^{1} 6 dx_{3} dx_{2} dx_{1} = (1-2d)^{3}.
    \end{align}
\end{example}
The marginal density of $X_{(k)}$ is given as $f_{X_{(k)}}(y_{k}) = $
\begin{align}
    \idotsint_{x_{1} < \ldots < x_{k-1} < y_{k} < x_{k+1} < \ldots < x_{n}} f_{(X_{(1)},\ldots,X_{(n)})}(x_{1},\ldots,x_{k-1},y_{k},x_{k+1},\ldots,x_{n}) dx_{1} \cdots dx_{k-1} dx_{k+1} \cdots dx_{n}.
\end{align}
If we note that
\begin{align}
    F_{(X_{n})}(y) = P(X_{(n)} \leq y) = P(X_{1}\leq y, \ldots, X_{n} \leq y) = [F(y)]^{n}
\end{align}
then
\begin{align}
    f_{X_{(n)}}(y) = n[F(y)]^{n-1} f(y).
\end{align}
Similarly,
\begin{align}
    1-F_{X_{(1)}}(y) = P(X_{(1)} > y) = P(X_{1} > y, X_{2} > y, \ldots, X_{n} > y) = [1-F(y)]^{n}
\end{align}
gives us
\begin{align}
    f_{X_{(1)}}(y) = n[1-F(y)]^{n-1}f(y).
\end{align}
For the other marginal densities, we work as follows---
\begin{align}
    F_{X_{(k)}}(y) = P(X_{(k)}\leq y) &= P(\text{at least $k$ of }X_{1},\ldots,X_{n} \leq y) \notag \\
    &= \sum_{j=k}^{n} P(\text{exactly $j$ of }X_{1},\ldots,X_{n} \leq y) \notag \\
    &= \sum_{j=k}^{n} \binom{n}{j} F(y)^{j} (1-F(y))^{n-j}.
\end{align}
Differentiating this to find the probability density function is a tedious task. We work around this. Note that for $X_{(k)}$ to attain the value $y_k$, one of the $X_{1},\ldots,X_{k}$ should equal $y_{k}$, $k-1$ should be less than $y_{k}$, and $n-k$ should be greater than $y_{k}$. For a fixed partition to satisfy these conditions, we have
\begin{align}
    P(X_{i_{1}},X_{i_{2}},\ldots,X_{i_{k-1}}<y_{k}, X_{l_{1}}=y_{k},X_{j_{1}},X_{j_{2}},\ldots,X_{j_{n-k}} > y) = F(y)^{k-1}f(y)(1-F(y))^{n-k}.
\end{align}
Thus, we can choose any partition in the above mentioned way to get
\begin{align}
    f_{X_{(k)}}(y) = \binom{n}{k-1,n-k,1} F(y)^{k-1}f(y)(1-F(y))^{n-k}.
\end{align}
For $i < j$ the joint density function of $(X_{(i)},X_{(j)})$, $f_{X_{(i)}X_{(j)}}(y_{i},y_{j})$ exists for $y_{i}<y_{j}$. From the same intuitive reasoning as before, we have
\begin{align}
    f_{X_{(i)}X_{(j)}}(y_{i},y_{j}) = \binom{n}{i-1,1,j-i-1,1,n-j} F(y_{i})^{i-1} f(y_{i})(F(y_{j})-F(y_{i}))^{j-i-1}f(y_{j})(1-F(y_{j}))^{n-j}.
\end{align}
\begin{example}
    We are the find the cumulative distribution function of $R = X_{(n)}-X_{(1)}$, the range of the $X_{i}$'s. We simply have
    \begin{align}
        P(R \leq a) &= P(X_{(n)}-X_{(1)} \leq a) = \iint_{x_{n}-x_{1} \leq a} f_{X_{(1)}X_{(n)}}(x_{1},x_{n}) dx_{1} dx_{n} \\
        &= \int_{-\infty}^{\infty} \int_{x_{1}}^{x_{1}+a} \frac{n!}{(n-2)!}[F(x_{n})-F(x_{1})]^{n-2}f(x_{1})f(x_{n}) dx_{n} dx_{1} \notag
    \end{align}
    Making the substitution $y = F(x_{n}) - F(x_{1})$ with $dy = f(x_{n})dx_{n}$ will help compute the function. The computation is left as an exercise to the reader.
\end{example}

\textit{March 4th.}

Let $X_{1},X_{2},\ldots,X_{n}$ be independent and identically distributed as Exp$(\alpha)$. These will denote the service times of $n$ counters in a post office, commencing at time 0. Thus, $X_{(i)}$ will denote the time of the $i^{\text{th}}$ discharge. We have
\begin{align}
    P(X_{(1)} > t) = P(X_{1}>t,\ldots,X_{n}>t) = \prod_{i=1}^{n} P(X_{i} > t) = e^{-n\alpha t} \implies X_{(1)} \sim \text{Exp}(n \alpha).
\end{align}
Also,
\begin{align*}
    P(X_{(n)} \leq t) = P(X_{1} \leq t, \ldots, X_{(n)} \leq t) = \prod_{i=1}^{n} P(X_{i} \leq t) = (1-e^{-\alpha t})^{n}.
\end{align*}
Intuitively, from the memoryless property of the exponential distribution, we can infer a proposition.
\begin{proposition}
    The $n$ variables $X_{(1)},X_{(2)}-X_{(1)},\ldots,X_{(n)}-X_{(n-1)}$ are independent, and the density of $X_{(k+1)}-X_{(k)}$ is $(n-k)\alpha e^{-(n-k)\alpha t}$.
\end{proposition}
\begin{proof}
    We show the case for $n=3$. The proof can then be generalized. We have
    \begin{align}
        f_{X_{(1)}X_{(2)}X_{(3)}}(z_{1},z_{2},z_{3}) = 6 f_{X_{1},X_{2},X_{3}}(z_{1},z_{2},z_{3}) = 6\alpha^{3} e^{-\alpha} e^{-\alpha(z_{1}+z_{2}+z_{3})}.
    \end{align}
    Also,
    \begin{align}
        P(X_{(1)} > t_{1}, X_{(2)}-X_{(1)} > t_{2}, X_{(3)} - X_{(2)} > t_{3}) &= 6 \int_{t_{1}}^{\infty} \alpha e^{-\alpha z_{1}} \int_{z_{1}+t_{2}}^{\infty} \alpha e^{-\alpha z_{2}} \int_{z_{2}+t_{3}}^{\infty} \alpha e^{-\alpha t_{3}} dz_{3} dz_{2} dz_{1} \notag \\
        &= e^{-\alpha t_{3}} e^{-2\alpha t_{2}} e^{-3\alpha t_{1}}.
    \end{align}
    Therefore, $X_{(1)} \sim \text{Exp}(3\alpha)$, $X_{(2)}-X_{(1)} \sim \text{Exp}(2\alpha)$, and $X_{(3)}-X_{(2)} \sim \text{Exp}(\alpha)$.
\end{proof}

We ask a few questions; suppose $A$ and $B$ are currently being served at the office, and a third clerk $C$ enters. The probability that $C$ leaves last is one-half due to the memoryless property of the exponential distribution. To find the total time spent by $C$, we have $T = X_{(1)} + Z$, where $Z$ is the distriution of $C$'s time spent being served. The distribution of $T$ is given as
\begin{align*}
    f(t) = \int_{\R} f_{X_{(1)}}(x)f_{Z}(t-x) dx = \int_{0}^{t} 2\alpha e^{-2\alpha x} \alpha e^{-\alpha (t-x_{1})} dx = 2\alpha e^{-\alpha t} (1-e^{-\alpha t}).
\end{align*}
Let us also compute the distribution of the time of last departure, $\tilde{T}$. Note that $C$ enters only when one of $A$ or $B$ is served. The first discharge has distribution $Z \sim \text{Exp}(2\alpha)$. When $C$ enters, we have 2 independent $\text{Exp}(\alpha)$ random variables, where the last discharge is given as $X_{(2)}$. Note that $X_{1},X_{2}$ are independent of $Z$. From the previous quesiton, we know that $X_{(2)} = X_{(2)} - X_{(1)} + X_{(2)}$ has distribution $\text{Exp}(2\alpha) + \text{Exp}(\alpha)$, and density $2 \alpha e^{-\alpha t} (1-e^{-\alpha t})$. Therefore, the distribution of $\tilde{T}$ matches that of $Z + X_{(2)}$. Integrating gives us
\begin{align}
    f_{\tilde{T}}(\tilde{t}) = 4\alpha (e^{-\alpha \tilde{t}} - e^{-2 \alpha \tilde{t}} = \alpha \tilde{t} e^{-2\alpha \tilde{t}}).
\end{align}

Moving away from the exponential distribution, let us consider $X_{1},X_{2},\ldots,X_{n}$ to be independent and identically distributed to $\text{Unif}[0,1]$. Note that $X_{(1)},X_{(2)},\ldots,X_{(n)}$ partition the interval $[0,1]$ into subintervals of length $l_{1} = X_{(1)}, l_{2} = X_{(2)}-X_{(1)},\ldots,l_{n}=X_{(n)}-X_{(n-1)}$. The lengths $l_{1},l_{2},\ldots,l_{n}$ are not independent since they have to satisfy $\sum l_{i} = 1$. The distributions of $l_{i}$'s turn out to be identical.

As another exercise, let $X_{1},X_{2}$ be independent and randomly chosen uniformly on the unit circle. Let $r(x,y)$ denote the clockwise circular distance from points $x$ to $y$ on the unit circle. Note that $r(X_{1},X_{2})$ has the same distribution as $r(X_{2},X_{1})$, computed as $\text{Unif}[0,1]$.

\section{Joint Distribution of Functions of Random Variables}
\textit{March 7th.}

Suppose $(X_{1},X_{2})$ has joint probability density function $f_{X_{1}X_{2}}$, and suppose $Y_{1} = g_{1}(X_{1},X_{2})$ and $Y_{2} = g_{2}(X_{1},X_{2})$ for some functions $g_{1},g_{2}$. We assume that $g_{1}$ and $g_{2}$ satisfy the following conditions:
\begin{itemize}
    \item The equations $y_{1} = g_{1}(x_{1},x_{2})$ and $y_{2} = g_{2}(x_{1},x_{2})$ can be uniquely solves for $x_{1}$ and $x_{2}$ in terms of $y_{1}$ and $y_{2}$, say, $x_{1} = h_{1}(y_{1},y_{2})$ and $x_{2} = h_{2}(y_{1},y_{2})$.
    \item $g_{1}$ and $g_{2}$ have continuous partial derivatives at all $(x_{1},x_{2})$ and the determinant of the \eax{Jacobian matrix} is
    \begin{align}
        J(x_{1},x_{2}) = \det \begin{pmatrix}
            \frac{\partial g_{1}}{\partial x_{1}} & \frac{\partial g_{1}}{\partial x_{2}} \\ \frac{\partial g_{2}}{\partial x_{1}} & \frac{\partial g_{2}}{\partial x_{2}} 
        \end{pmatrix}
        = \frac{\partial g_{1}}{\partial x_{1}} \frac{\partial g_{2}}{\partial x_{2}} - \frac{\partial g_{1}}{\partial x_{2}}\frac{\partial g_{2}}{\partial x_{1}} \neq 0.
    \end{align}
\end{itemize}
Then $(Y_{1},Y_{2})$ has the joint density
\begin{align}
    f_{Y_{1}Y_{2}}(y_{1},y_{2}) = f_{X_{1}X_{2}}(x_{1},x_{2}) \abs{J(x_{1},x_{2})}^{-1}
\end{align}
where $x_{1} = h_{1}(y_{1},y_{2})$ and $x_{2} = h_{2}(y_{1},y_{2})$. The proof of this comes from multivariate analysis.

\begin{remark}
    The above can be extended to more than 2 random variables. Suppose that we have the probability density function $f_{X_{1}X_{2}\ldots X_{n}}(x_{1},x_{2},\ldots,x_{n})$ and we have $Y_{1} = g_{1}(X_{1},X_{2},\ldots,X_{n}), Y_{2} = g_{2}(X_{1},X_{2},\ldots,X_{n}),\ldots,Y_{n}=g_{n}(X_{1},X_{2},\ldots,X_{n})$. We have to assume that all the $g_{i}$'s have continuous partial derivatives, and we have to assume that
    \begin{align}
        J(x_{1},x_{2},\ldots,x_{n}) = \det \begin{pmatrix}
            \frac{\partial g_{1}}{\partial x_{1}} & \cdots & \frac{\partial g_{1}}{\partial x_{n}} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial g_{n}}{\partial x_{1}} & \cdots & \frac{\partial g_{n}}{\partial x_{n}} \neq 0 \text{ for all } (x_{1},x_{2},\ldots,x_{n}).
        \end{pmatrix}
    \end{align}
    We also suppose that that $y_{1} = g_{1}(x_{1},\ldots,x_{n}),\ldots,y_{n} = g_{n}(x_{1},\ldots,x_{n})$ has a unique solution given by $x_{1} = h_{1}(y_{1},\ldots,y_{n}), \ldots, x_{n}=h_{n}(y_{1},\ldots,y_{n})$. The joint density function of $(Y_{1},\ldots,Y_{N})$ is then given by
    \begin{align}
        f_{Y_{1}Y_{2}\ldots Y_{n}}(y_{1},y_{2},\ldots,y_{n}) = f_{X_{1}X_{2}\ldots X_{n}}(x_{1},x_{2},\ldots,x_{n}) \abs{J(x_{1},\ldots,x_{n})}^{-1}
    \end{align}
    where $x_{i} = h_{i}(y_{1},\ldots,y_{n})$.
\end{remark}

\subsection{Conditional Expectation and Variance}
Let $(X,Y)$ have a joint probability density function $f(x,y)$, and suppose $g:\R^{2} \to \R$ is a function.
\begin{proposition}
    The expectation of $g(X,Y)$ is given as
    \begin{align*}
        E[g(X,Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f(x,y) dy dx.
    \end{align*}
\end{proposition}
\begin{remark}
    By this proposition, if we pick $g(x,y) = x$ for all $(x,y) \in \R^{2}$, we get
    \begin{align*}
        EX = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f(x,y) dy dx = \int_{-\infty}^{\infty} x f_{X}(x)dx.
    \end{align*}
\end{remark}
\begin{proof}
    Assume that $g(x,y) \geq 0$ for all $(x,y) \in \R^{2}$. We have
    \begin{align}
        E[g(X,Y)] &= \int_{0}^{\infty} P(g(X,Y) > t) dt \notag \\
        &= \int_{0}^{\infty} \left( \iint_{\{(x,y) : g(x,y) > t\}} f(x,y) dy dx \right)dt \notag \\
        &= \iint \int_{0}^{g(x,y)} f(x,y) dt dy dx \notag \\
        &= \iint g(x,y)f(x,y) dy dx.
    \end{align}
    For a general $g$, we simply work with $g = g^{+} - g^{-}$ where $g^{+}(x,y) = \max \{g(x,y),0\}$ and $g^{-}(x,y) = \max \{-g(x,y),0\}$.
\end{proof}
If $(X,Y)$ have a joint probability density function $f(x,y)$, then it can be shown that $E[X+Y] = EX + EY$.

\begin{example}
    Suppose $X,Y \sim \text{Unif}[0,L]$. We compute $E\abs{X-Y}$. Noting that the joint probability density function is $f(x,y) = \frac{1}{L^{2}}$ for all $(x,y) \in [0,L] \times [0,L]$, we have
    \begin{align}
        E\abs{X-Y} &= \int_{0}^{L} \int_{0}^{L} \frac{\abs{x-y}}{L^2} dy dx \notag
        = \frac{1}{L^{2}} \int_{0}^{L} \left( \int_{0}^{x}(x-y)dy + \int_{x}^{L} (y-x)dy \right) \\
        &= \frac{1}{L^{2}} \int_{0}^{L} \left( \frac{x^{2}}{2} + \frac{(L-x)^{2}}{2} \right)dx = \frac{L}{3}.
    \end{align}
\end{example}

Recall that the conditional density of $X$ given $Y = y$ is given as
\begin{align*}
    f_{X|Y}(x|y) = \frac{f_{XY}(x,y)}{f_{Y}(y)}
\end{align*}
provided that $f_{Y}(y) > 0$.

\begin{definition}
    The conditional expectation is given as
    \begin{align*}
        E[X|Y=y] = \int_{-\infty}^{\infty} x f_{X|Y}(x|y) dx
    \end{align*}
    provided that $f_{Y}(y) > 0$.
\end{definition}

\begin{example}
    We compute $E[X|Y=y]$ for $f(x,y) = \frac{e^{-\frac{x}{y}}e^{-y}}{y}$ for $x,y>0$. It can be shown that the marginal density $f_{Y}(y)$ is $f_{Y}(y) = e^{-y}$. Thus, the conditional density is
    \begin{align}
        f_{X|Y}(x|y) = \frac{f(x,y)}{f_{Y}(y)} = \frac{e^{-\frac{x}{y}}e^{-y}}{y e^{-y}} = \frac{1}{y} e^{-\frac{x}{y}}.
    \end{align}
    The conditional expectation is then given as
    \begin{align*}
        E[X|Y=y] = \int_{0}^{\infty} \frac{xe^{-\frac{x}{y}}}{y} dx = y.
    \end{align*}
\end{example}

It can be shown that
\begin{align}
    E[g(X)|Y=y] = \int g(x) f_{X|Y}(x|y) dx.
\end{align}

\begin{proposition}
    We have
    \begin{align*}
        EX = E[E[X|Y]].
    \end{align*}
\end{proposition}
\begin{proof}
    Since $E[X|Y=y] = \int x f_{X|Y}(x|y) dx$ is a function of $y$, we have
    \begin{align*}
        E[E[X|Y]] &= \int_{-\infty}^{\infty} f_{Y}(y) \left(  \int_{-\infty}^{\infty} x f_{X|Y}(x|y) dx\right) dy \notag 
        = \int_{-\infty}^{\infty} f_{Y}(y) \int_{-\infty}^{\infty} x \frac{f(x,y)}{f_{Y}(y)} dx dy \\
        &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f(x,y) dy dx = \int_{-\infty}^{\infty} x f_{X}(x) dx = EX.
    \end{align*}
\end{proof}

The conditional variance is also given as
\begin{align}
    \Var(X|Y=y) = E[X^{2}|Y=y] - (E[X|y=y])^{2} = \int x^{2} f_{X|Y}(x|y) dx - \left( \int x f_{X|Y}(x|y) dx \right)^{2}.
\end{align}

Also recall the conditional variance formula---
\begin{align}
    \Var(X) &= E[(X-EX)^{2}] = E[(X-E[X|Y]+E[X|Y]-EX)^{2}] \notag \\
    &= E[(X-E[X|Y])^{2} + (E[X|Y]-EX)^{2} + 2(X-E[X|Y])(E[X|Y]-EX)] \notag \\
    &= E[\text{Var}(X|Y)] + \text{Var}(E[X|Y]) + 0.
\end{align}

\begin{example}
    We look at the bivariate normal distribution. The probability density function is given by
    \begin{align}
        f(x,y) = \frac{1}{2\pi \sigma_{x}\sigma_{y} \sqrt{1-\rho^{2}}} \exp \left( -\frac{1}{2(1-\rho^{2})} \left( \left( \frac{x-\mu_{x}}{\sigma_{x}} \right)^{2} + \left( \frac{y-\mu_{y}}{\sigma_{y}} \right)^{2} -2\rho \frac{(x-\mu_{x})(y-\mu_{y})}{\sigma_{x} \sigma_{y}} \right) \right).
    \end{align}
    It can be shown that the correlation between $X$ and $Y$ is indeed $\rho$, that is, $\frac{E(XY)-\mu_{x}\mu_{y}}{\sigma_{x}\sigma_{y}} = \rho$. $E(XY)$ is computed by considering $E(XY) = E[E[XY|Y]]$.
\end{example}

\begin{example}
    Suppose $(S,R)$ has the following distributions; $R|S=s \sim N(s,1)$ and $S \sim N(\mu,\sigma^{2})$. We wish to compute $E[S|R=r]$ and $\Var(S|R=r)$. The former can be found out by first computing
    \begin{align}
        f_{S|R}(s|r) &= \frac{f_{S,R}(s,r)}{f_{R}(s)} = \frac{f_{R|S}(r|s) f_{S}(s)}{f_{R}(r)}.
    \end{align}
\end{example}

\textit{March 18th.}\\
For random variables $X$ and $Y$ which are jointly distributed, the following hold true:
\begin{itemize}
    \item $E[(X-a)^{2}] \geq E[(X-EX)^{2}]$,
    \item $E[(Y-g(X))^{2}] \geq E[(Y-E[Y|X])^{2}]$.
\end{itemize}
The best predictor of $Y$ using $X$ is $E[Y|X]$.

\begin{example}
    Suppose $X$ is a continuous random variable that we wish to discretize. We first fix an increasing set of numbers $\ldots < a_{-2} < a_{-1} < a_{0} < a_{1} < a_{2} < \ldots$ such that $\lim_{i \to \infty} a_{i} = \infty$ and $\lim_{i \to -\infty} a_{i} = -\infty$. Let $Y$ be a random variable taking the value $y_{i}$ when $a_{i} < X \leq a_{i+1}$. We wish to choose $y_{i}$ that minimizes $E[(X-Y)^{2}]$. For these optimal $y_{i}$'s one can also show that $EY = EX$ and $\Var Y = \Var X - E[(X-Y)^{2}]$.

    Start with
    \begin{align}
        E[(X-Y)^{2}] &= \sum_{i} E[(X-Y)^{2} | a_{i} < X \leq a_{i+1}] P(a_{i} < X \leq a_{i+1}) \\
        &= \sum_{i} E[(X-y_{i})^{2} | a_{i} < X \leq a_{i+1}] P(a_{i} < X \leq a_{i+1}) \notag.
    \end{align}
    It is enough to minimize $E[(X-y_{i})^{2}|a_{i} < X \leq a_{i+1}]$. $y_{i}$ is then computed to be
    \begin{align}
        y_{i} = E[X|a_{i} < X \leq a_{i+1}] = \frac{E[X \cdot 1_{\{a_{i} < X \leq a_{i+1}\}}]}{P(a_{i} < X \leq a_{i+1})} = \frac{\int_{a_{i}}^{a_{i+1}} x f_{X}(x) dx}{\int_{a_{i}}^{a_{i+1}} f_{X}(x) dx}.
    \end{align}
    To show that the expectations of $Y$ and $X$ are equal, we have
    \begin{align}
        EY &= \sum_{i} y_{i} P(Y = y_{i}) = \sum_{i} E[X|a_{i} < X \leq a_{i+1}] P(a_{i} < X \leq a_{i+1}) = EX.
    \end{align}
    Define a random variable $I$ to be $I = i$ if $a_{i} < X \leq a_{i+1}$. Then
    \begin{align}
        \Var X = \Var(E[X|I]) + E[\Var (X|I)]
    \end{align}
    can be used. We can compute $E[X|I]$ as $y_{I} = Y$ and $\Var(X|I)$ as $E[(X-Y)^{2}|I]$. Thus, the variance equality follows.
\end{example}

\chapter{CONVERGENCE OF RANDOM VARIABLES}

\section{Types of Convergence}
We being with a probability space $(\Omega, P)$, and define a sequence of random variables $X_{n}:\Omega \to \R$ for $n \geq 1$ and also have another $X:\Omega \to \R$.

\subsection{Almost Sure Convergence}

\begin{definition}
    We first discuss the notion of \eax{almost sure convergence}. A sequence of random variables $X_{n}$ converges almost surely to $X$, that is, $X_{n} \toup{\text{a.s.}} X$ if
    \begin{align*}
        P(\{\omega: X_{n}(\omega) \to X(\omega)\}) = 1.
    \end{align*}
\end{definition}

\begin{example}
    Suppose that $(\Omega,P) = ([0,1],\text{Unif})$. Suppose that $q_{n}$ is an enumeration of the rationals in $[0,1]$. We define $X(\omega) = 1$ and $X_{n}(\omega) = 1$ if $\omega \in [0,1]\backslash\{q_{n}\}$ and $0$ otherwise. We find that
    \begin{align}
        A = \{\omega: X_{n}(\omega) \to X(\omega)\} = [0,1]
    \end{align}
    since for any rational $\omega$, further enough terms lead to $X_{n}(\omega) = 1$ and for any irrational $\omega$, we always have $X_{n}(\omega) = 1$. Thus, $P(A) = 1$.
\end{example}
\begin{example}
    As a continuation to the previous example, if we define the sequence of random variables as
    \begin{align}
        X_{n}(\omega) = \begin{cases}
            0 \text{ if } \omega \in \{q_{1},\ldots,q_{n}\},\\
            1 \text{ if otherwise},
        \end{cases}
    \end{align}
    we find that $\{\omega : X_{n}(\omega) \to X(\omega)\} = [0,1]\setminus \mathbb{Q}$. The sequence converges to 0 for rational $\omega$. But, in our probability space, $P([0,1]\setminus \mathbb{Q}) = 1$.
\end{example}

\begin{example}
    If we define the sequence of random variables as $X_{n} = n 1_{[0,\frac{1}{n}]}$, we find that $X_{n} \toup{\text{a.s.}} 0$. However, $EX_{n} = 1$ and $E[0] = 0$. The expectation is not preserved in almost sure convergence
\end{example}


\subsection{Convergence in Probability}
\begin{definition}
    We discuss the notion of \eax{convergence in probability}. We say that a sequence of random variables $X_{n}$ converges in probability to $X$, that is, $X_{n} \toup{p} X$ if for every $\varepsilon > 0$,
    \begin{align*}
        P(\abs{X_{n}-X} > \varepsilon) \to 0.
    \end{align*}
\end{definition}

To show that a sequence of random variables $X_{n}$ converges in probability to $X$, it is enough to show that $P(\abs{X_{n}-X} > \frac{1}{k}) \to 0$ for all $k \geq 1$.

\begin{theorem}
    $X_{n} \toup{\text{a.s.}} X$ implies $X_{n} \toup{p} X$.
\end{theorem}
\begin{proof}
    We have
    \begin{align}
        \{\omega:X_{n}(\omega) \to X(\omega)\} &= \{\omega: \text{ for all } k \geq 1, \exists N \text{ such that for all } n \geq N, \abs{X_{n}(\omega)-X(\omega)} \leq \frac{1}{k}\} \notag\\
        &= \bigcap_{k \geq 1} \bigcup_{N} \bigcap_{n \geq N} \{\abs{X_{n}-X} \leq \frac{1}{k}\}.
    \end{align}
    The probability of this set is 1 due to almost sure convergence. Since the intersection of these events has a probability of 1, each inside event must also have a probability of 1; thus,
    \begin{align}
        P(\bigcup_{N} \bigcap_{n \geq N} \{\abs{X_{n}-X} \leq \frac{1}{k}\}) &= 1.
    \end{align}
    We note that $\bigcap_{n \geq N} \{\abs{X_{n}-X} \leq \frac{1}{k}\}$ are events increasing in $N$. Therefore,
    \begin{align}
        P(\bigcap_{n \geq N} \{\abs{X_{n}-X} \leq \frac{1}{k}\}) \to 1 \implies P(\abs{X_{n}-X} \leq \frac{1}{k}) \to 1.
    \end{align}
\end{proof}
However, the converse is not true.\\
\textit{March 21st.}

\begin{theorem}
    Let $X_{1},X_{2},\ldots$ be a sequence of random variables. If $EX_{n} \to c$ and $\emph{\text{Var}}(X_{n}) \to 0$, then $X_{n} \toup{p}  c$.
\end{theorem}

\begin{proof}
    We have
    \begin{align}
        P(\abs{X_{n}-c} > \varepsilon) &= P(\abs{X_{n}-EX_{n}+EX_{n}-c} > \varepsilon) \leq P(\abs{X_{n}-EX_{n}} + \abs{EX_{n}-c} > \varepsilon).
    \end{align}
    Choose a natural $N$ such that $\abs{EX_{n}-c} < \frac{\varepsilon}{2}$, which gives us
    \begin{align}
        P(\abs{X_{n}-c} > \varepsilon) &\leq P(\abs{X_{n}-EX_{n}} > \frac{\varepsilon}{2}) \leq \frac{E[\abs{X_{n}-EX_{n}}^{2}]}{\varepsilon^{2}/4} = \Var X_{n} \frac{4}{\varepsilon^{2}} \to 0.
    \end{align}
\end{proof}
A consequence of this is the weak law of large numbers.
\begin{corollary}
    Let $X_{1},X_{2},\ldots$ be independent and identically distributed random variables, with $EX_{1} = \mu$ and $\emph{\text{Var}}X_{1} = \sigma^{2} < \infty$. Let $\overline{X}_{n} = \frac{X_{1}+X_{2}+\ldots+X_{n}}{n}$. Then $\overline{X}_{n} \toup{p} \mu$.
\end{corollary}
\begin{proof}
    We note that $E\overline{X}_{n} = \mu$ and $\Var \overline{X}_{n} = \frac{1}{n} \sigma^{2}$. The above theorem proves the law.
\end{proof}

\begin{example}
    We put $n$ balls randomly into $n$ boxes. Here, let $X_{n}$ denote the number of empty boxes. We are interested in $Y_{n} = \frac{X_{n}}{n}$, the proportion of empty boxes. We claim that $Y_{n}$ converges to $e^{-1}$. To compute the mean, we write $EX_{n} = E(\sum_{i=1}^{n} 1_{A_{i}})$, where $A_{i}$ is the event that the $i^{\text{th}}$ box is empty. Thus, this is $EX_{n} = \sum_{i=1}^{n} \frac{(n-1)^{n}}{n^{n}} = n(1-\frac{1}{n})^{n} \implies EY_{n} = (1-\frac{1}{n})^{n} \to \frac{1}{e}$.

    For the variance, $\Var X_{n} = \sum_{i,j=1}^{n} \text{Cov} (1_{A_{i}},1_{A_{j}})$. For $i = j$, this is
    \begin{align}
        \text{Cov}(1_{A_{i}},1_{A_{j}}) = E[1_{A_{i}}] - (E[1_{A_{i}}])^{2} = P(A_{i})(1-P(A_{i})) = \frac{(n-1)^{n}}{n^{n}}\left( 1- \frac{(n-1)^{n}}{n^{n}} \right).
    \end{align}
    For $i \neq j$, we have
    \begin{align}
        \text{Cov}(1_{A_{i}},1_{A_{j}}) = E[1_{A_{i}}1_{A_{j}}] - P(A_{i})P(A_{j}) = \left( \frac{n-2}{n} \right)^{n} - \left( \frac{n-1}{n} \right)^{2n}.
    \end{align}
    Thus, the final variance is calculated as
    \begin{align}
        \Var X_{n} &= n \left( \frac{n-1}{n} \right)^{n} \left( 1- \left( \frac{n-1}{n} \right)^{n} \right) + n(n-1) \left( \left( \frac{n-2}{n} \right)^{n} - \left( \frac{n-1}{2n} \right)^{n} \right) \\
        \implies \Var Y_{n} &= \frac{1}{n^{2}} \Var X_{n} \to 0.
    \end{align}
    Thus, $Y_{n} \toup{p} e^{-1}$.
\end{example}

\begin{example}
    We toss a $p$-coin, and look at $X_{n}$, the number of head runs in $n$ tosses. We claim that $Y_{n} = \frac{1}{n} X_{n} \toup{p} p(1-p)$. We rewrite $X_{n}$ as $X_{n} = \sum_{i=1}^{n} 1_{A_{i}}$ where $A_{i}$ is the event that a head run starts at position $i$. We have
    \begin{align}
        EX_{n} = \sum_{i=1}^{n} P(A_{i}) = P(A_{1}) + \sum_{i=2}^{n} P(A_{i}) = p + (n-1)(1-p)p \implies EY_{n} \to p(1-p).
    \end{align}
    For the variance,
    \begin{align}
        \Var X_{n} = \sum_{i,j=1}^{n} \text{Cov}(1_{A_{i}},1_{A_{j}}) = \sum_{i=1}^{n} \Var 1_{A_{i}} + 2 \sum_{i < j} \text{Cov}(1_{A_{i}},1_{A_{j}}).
    \end{align}
    The variance of a single indicator, $\Var 1_{A_{i}}$ is $p-p^{2}$ if $i = 1$, and $(1-p)p - ((1-p)p)^{2}$ if $i > 1$. For the covariance, if $\abs{i-j} \geq 2$, the events $A_{i}$ and $A_{j}$ are clearly independent, so $\Cov(1_{A_{i}},1_{A_{j}}) = 0$.
    \begin{align}
        \Var Y_{n} &= \frac{1}{n^{2}} \Var X_{n} = \frac{1}{n^{2}}(p-p^{2} + (n-1)((1-p)p - (1-p)^{2}p^{2})) + \frac{1}{n^{2}}(\cdots) \to 0.
    \end{align}
\end{example}

\begin{example}
    We revisit the coupon collector problem; there are $n$ types of coupons, and each purchase results in one of the $n$ types with equal probability. $T_{n}$ denotes the number of purchases to get $n$ types. The claim is that $\frac{1}{n \log n} T_{n} \toup{p} 1$.

    Clearly, $T_{1} = 1$, $T_{2} -T_{1} \sim \text{Geom}(\frac{n-1}{n})$, $T_{3}-T_{2} \sim \text{Geom}(\frac{n-2}{n})$, etc. These are all independent quantities, and the expectation of a geometric distribution if $\frac{1}{p}$. Thus,
    \begin{align}
        T_{n} &= T_{1} + (T_{2}-T_{1}) + \ldots + (T_{n}-T_{n-1}) \\
        \implies ET_{n} &= n \left( 1+\frac{1}{2} + \ldots + \frac{1}{n} \right) \notag \implies \frac{1}{n \log n} ET_{n} = \frac{1 + \frac{1}{2} + \ldots + \frac{1}{n}}{\log n} \to 1.
    \end{align}
    and the variance is
    \begin{align}
        \frac{1}{n^{2} (\log n)^{2}}\Var T_{n} = \frac{1}{(n\log n)^{2}} n \sum_{i=1}^{n} \frac{(i-1)}{(n-(i-1))^{2}} \leq \frac{1}{n^{2} (\log n)^{2}} n^{2} \sum_{i=1}^{n} \frac{1}{i^{2}} \to 0.
    \end{align}
\end{example}


\begin{theorem}[The \eax{strong law of large numbers}]
    Let $X_{1},X_{2},\ldots$ be independent and identicaly distributed random variables with $EX_{1} = \mu \in (-\infty,\infty)$. Let $\overline{X}_{n} = \frac{1}{n}(X_{1}+X_{2}+\ldots+X{n})$. Then $\overline{X}_{n} \toup{\emph{\text{a.s.}}} \mu$.
\end{theorem}

\begin{proof}
    We prove this assuming that $K = EX_{1}^{4} < \infty$. Without the loss of generality, let $\mu = 0$. Denote $S_{n} = \sum_{i=1}^{n} X_{i}$. The terms inside $ES_{n}^{4}$ are any one of the form $X_{i}^{4},X_{i}^{3}X_{j},X_{i}^{2}X_{j}^{2},X_{i}^{2}X_{j}X_{k},X_{i}X_{j}X_{k}X_{l}$, where $i,j,k,l$ are distinct. We have assumed that $\mu = 0$, so
    \begin{align}
        E[X_{i}^{3}X_{j}] = E[X_{i}^{3}]EX_{j} = 0,\; E[X_{i}^{2}X_{j}X_{k}] = E[X_{i}^{2}]EX_{j}EX_{k} = 0,\; E[X_{i}X_{j}X_{k}X_{l}] = 0.
    \end{align}
    Thus, the only remaining terms are
    \begin{align}
        ES_{n}^{4} &= nEX_{1}^{4} + \binom{n}{2} \binom{4}{2} (EX_{1}^{2})^{2} \notag \\
        \implies E\overline{X}_{n}^{4} &\leq \frac{1}{n^{3}}EX_{1}^{4} + \frac{3}{n^{2}}(EX_{1}^{2})^{2} \implies \sum_{n=1}^{\infty} E\overline{X}_{n}^{4} = \sum_{n=1}^{\infty} \frac{1}{n^{3}} EX_{1}^{4} + \frac{3}{n^{2}}(EX_{1}^{2})^{2} < \infty.
    \end{align}
    The expectation of the infinite sum is finite, which means
    \begin{align}
        E\left( \sum_{n=1}^{\infty} \overline{X}_{n}^{4} \right) < \infty \implies P\left( \sum_{n=1}^{\infty} \overline{X}_{n}^{4} < \infty \right) = 1 \implies P(\overline{X}_{n} \to 0) = 1.
    \end{align}
\end{proof}

\subsection{Convergence in Distribution}
\textit{March 28th.}

\begin{definition}
    We discuss the notion of \eax{convergence in distribution}. Suppose $X_{1},X_{2},\ldots$ are a sequence of random variables, and let $X$ be another random variables. Denote the cumulative distribution function of $X_{n}$ by $F_{n}$ and that of $X$ by $F$. We say that the sequence of random variables $X_{n}$ converges to $X$ if $\lim_{n \to \infty} F_{n}(x) = F(x)$ for all $x$ at which $F$ is continuous.
\end{definition}

We write $X_{n} \rightrightarrows X$ or $F_{n} \rightrightarrows F$, or even $X_{n} \toup{d} X$ if $X_{n}$ converges to $X$ in distribution.

\begin{remark}
    When $X$ has a denisty function $f$, then $F$ is continuous everywhere and we need the sequence to converge everywhere.
\end{remark}

When $F$ is continuous, we have $P(a < X_{n} \leq b) = P(X_{n} \leq b) - P(X_{n} \leq a) = F_{n}(b)-F_{n}(a) \to F(b)-F(a)$. Similarly, we can show that $P(a \leq X_{n} \leq b) \to P(a < X \leq b)$ since $P(X = a) = 0$; we wish to show that $P(X_{n} = a) = 0$ is $X_{n}$ converges to a $X$ with continuous $F$. This can be shown as
\begin{align}
    P(X_{n}=a) \leq P(a-\varepsilon < X_{n} \leq a) = F_{n}(a)-F_{n}(a-\varepsilon) \notag \implies \limsup_{n \to \infty} P(X_{n}=a) \leq F(a) - F(a-\varepsilon).
\end{align}
Letting $\varepsilon \downarrow 0$, we find $\limsup_{n \to \infty} P(X_{n}=a) = 0 \implies \lim_{n \to \infty} P(X_{n}=a) = 0$.

\begin{example}
    Suppose $X_{n} \sim \text{Bin}(n,\frac{\lambda}{n})$ and $X \sim \text{Poisson}(\lambda)$. We have already seen that $P(X_{n}=k) \to P(X=k)$. For $x \geq 0$, we have
    \begin{align}
        \lim_{n \to \infty} P(X_{n} \leq x) = \lim_{n \to \infty} \sum_{0 \leq k \leq x}P(X_{n} = k) = \sum_{0 \leq k \leq x} \lim_{n \to \infty} P(X_{n} = k) = \sum_{0 \leq k \leq x} P(X=k) = P(X \leq x).
    \end{align}
\end{example}

Suppose $X_{1},X_{2},\ldots$ are independent and identically distributed random variables with finite mean $\mu$ and finite variance $\sigma^{2}$. From the law of large numbers, we know that $\frac{S_{n}}{n} \to \mu$ where $S_{n}$ are the partial sums. The central limit theorem roughly states that $\frac{S_{n}}{n} = \mu + O(\frac{1}{\sqrt{n}})$. We state this more formally.

\subsubsection{The Central Limit Theorem}

\begin{theorem}[The \eax{central limit theorem}]
    Let $X_{1},X_{2},\ldots$ be independent and identically distributed random variables with with quantities defined as above. The central limit theorem states that
    \begin{align*}
        \frac{S_{n}-n\mu}{\sigma\sqrt{n}} \rightrightarrows N(0,1).
    \end{align*}
    For $-\infty < a \leq b \leq \infty$, we have
    \begin{align*}
        P\left(a \leq \frac{S_{n}-n\mu}{\sigma \sqrt{n}} \leq b  \right) \toup{n \to \infty} \int_{a}^{b} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^{2}}{2}} dx.
    \end{align*}
\end{theorem}

The left hand side may be rewritten as $P(n\mu + a\sigma\sqrt{n} \leq S_{n} \leq n\mu + b\sigma\sqrt{n})$ and the right hand side may be rewritten as $P(m\mu + a\sigma\sqrt{m} \leq Z_{1} + \cdots + Z_{m} \leq m\mu + b\sigma\sqrt{m})$ where $Z_{1},\ldots,Z_{m} \sim N(\mu,\sigma^{2})$ are independent.

\begin{example}
    Suppose a fair coin is tossed a million times, and denote $S$ to be the number of heads. This is the sum of a million independent and identically distributed $\text{Ber}(\frac{1}{2})$ variables. Here, $\mu = \frac{1}{2}$ and $\sigma^{2} = \frac{1}{4}$. In this case, $n\mu = 500,000$ and $\sigma \sqrt{n} = 500$. The central limit theorem states that $\frac{S-500,000}{500} \approx N(0,1)$. Using $a = -2.576$ and $b = 2.576$, we have
    \begin{align}
        P(498712 \leq S \leq 501288) \approx \int_{-2.576}^{2.576} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^{2}}{2}}dx \approx 0.99.
    \end{align}
\end{example}

Suppose $g:\R \to \R$ is defined by $g(x) = e^{-\frac{1}{x}}$ for $x > 0$, and $g(x) = 0$ for $x \leq 0$. For $x \neq 0$, clearly $g(x)$ is infinitely differentiable. We shall show that $g(x)$ is also infinitely differentiable at $x = 0$ with $g^{(k)}(0) = 0$ always. We have
\begin{align}
    \lim_{x \uparrow 0} \frac{g(x)-g(0)}{x-0} = \lim_{x \uparrow 0} \frac{0}{x} = 0
\end{align}
and
\begin{align}
    \lim_{x \downarrow 0} \frac{g(x)-g(0)}{x-0} = \lim_{x \downarrow 0} \frac{e^{-\frac{1}{x}}}{x} = 0.
\end{align}
Thus, $g(x)$ is differentiable at $0$, and there exists a $C$ such that $\abs{g'(x)} \leq C$ for all $x \in \R$. For any $k^{\text{th}}$ derivative of $g(x)$ for $x > 0$, we obtain $g^{(k)}(x) = e^{-\frac{1}{x}}p_{2k}(\frac{1}{x})$ where $p_{2k}$ is a polynomial of degree $2k$. Again, the only problem is at 0. By the induction hypothesis, $g^{(k-1)}(0) = 0$. We have
\begin{align}
    \lim_{x \downarrow 0} \frac{g^{(k-1)}(x)-g^{(k-1)}(0)}{x-0} = \lim_{x \downarrow 0} \frac{g^{(k-1)}(x)}{x} = \lim_{x \downarrow 0} \frac{e^{-\frac{1}{x}} p_{2(k-1)}(\frac{1}{x})}{x} = 0
\end{align}
and left hand limit is easily seen to be zero. Hence, $g^{(k)}(0) = 0$ and there always exists a $C_{k}$ such that $\abs{g^{(k)}(x)} \leq C_{k}$ for all $x \in \R$. Thus, $g \in C_{b}^{\infty}$, the set of functions on $\R$ that are infinitely differentiable with bounded derivatives, including the $0^{\text{th}}$ derivative.

For any $-\infty < a < b < \infty$, let $g_{a,b}(x)=g(x-a)g(b-x)$. We note that $g_{a,b} \equiv 0$ on $\R\setminus[a,b]$ and $g_{a,b} \in C_{b}^{\infty}$. Also, $g > 0$ on $(a,b)$. Define $C_{a,b}$ to be the integral $\int_{\R} g_{a,b}(x)dx > 0$. Lastly, defining $h_{a,b}(x) = \frac{1}{C_{a,b}}g_{a,b}(x)$ gives a probability density function. Denote the cumulative distribution function by $H_{a,b}$.
\begin{align}
    H_{a,b}(x) = \begin{cases}
        0 &\text{ if } x \in (-\infty,a],\\
        \text{ strictly increasing } &\text{ if } x \in (a,b),\\
        1 &\text{ if } x \in [b,\infty).
    \end{cases}
\end{align}
We may also note that $H_{a,b} \in C_{b}^{\infty}$. We have defined this distribution for the following lemma.

\begin{lemma}
    Let $X$ be a random variable and suppose $X_{1},X_{2},\ldots$ be a sequence of random variables such that for each $f \in C_{b}^{\infty}$ we have $Ef(X_{n}) \to Ef(X)$. Then $X_{n} \rightrightarrows X$.
\end{lemma}
\begin{proof}
    Let $F_{n}$ be the cumulative distribution function of $X_{n}$ and $F$ be that of $X$. Fix $t \in \R$ at which $F$ is continuous, and also fix $s < t$. We need to show $F_{n}(t) \toup{n \to \infty} F(t)$. Consider $H_{s,t}$ defined as above; $H_{s,t} \in C_{b}^{\infty}$. Look at $1-H_{s,t} \leq 1_{(-\infty,t]}$. We then have
    \begin{align}
        E[1-H_{s,t}(X_{n})] &\leq E[1_{(-\infty,t]}(X_{n})] = F_{n}(t) \notag \\
        \implies E[1-H_{s,t}(X)] = \lim_{n \to \infty} E[1-H_{s,t}(X_{n})] &\leq \liminf_{n \to \infty} F_{n}(t)
    \end{align}
    This gives us $F(s) \leq \liminf_{n \to \infty} F_{n}(t)$. Letting $s \uparrow t$, we get $F(t) \leq \liminf_{n \to \infty} F_{n}(t)$.

    Similarly, consider $H_{t,s}$ for $s > t$. We get
    \begin{align}
        \limsup_{n \to \infty} F_{n}(t) = \limsup_{n \to \infty} E[1_{(\infty,t]}(X_{n})] \leq \limsup_{n \to \infty} E[1-H_{t,s}(X_{n})] = E[1-H_{t,s}(X)] \leq E[1_{(-\infty,s]}(X)] = F(s).
    \end{align}
    Letting $s \downarrow t$, we have $\limsup_{n \to \infty} F_{n}(t) \leq F(t)$. Also considering $F(t) \leq \liminf_{n \to \infty} F_{n}(t)$, we obtain $F_{n}(t) \to F(t)$.
\end{proof}

\textit{April 1st.}\\
We now state a proof of the central limit theorem, under the assumption that $E\abs{X_{i}-\mu}^{3} < \infty$.

\begin{proof}[Proof of the central limit theorem]
    Fix $n$. Define $Y_{i} = \frac{X_{i}-\mu}{\sigma \sqrt{n}}$. Thus, the $Y_{1},\ldots,Y_{n}$ are also independet and identically distributed with $EY_{i} = 0$ and $\Var Y_{i} = \frac{1}{n}$. Thus, $T_{n} = \frac{S_{n}-n\mu}{\sigma\sqrt{n}} = \sum_{i=1}^{n} Y_{i}$. From the previous lemma, it is enough to show that $Ef(T_{n}) \toup{n \to \infty} Ef(Z)$ for each $f \in C_{b}^{\infty}$ where $Z \sim N(0,1)$. For now, fix $f \in C_{b}^{\infty}$. Let $Z_{1},Z_{2},\ldots,Z_{n} \sim N(0,\frac{1}{n})$ be independent. Note that $Z = Z_{1} + \cdots + Z_{n} \sim N(0,1)$. For $i = 0,1,\ldots,n$, define $A_{i} = Y_{1} + \cdots + Y_{i-1} + Y_{i} + Z_{i+1} + \cdots + Z_{n}$. Note that $A_{0} = Z$ and $A_{n} = T_{n}$. We then have
    \begin{align}
        f(T_{n})-f(Z) = f(A_{n})-f(A_{0}) = \sum_{i=1}^{n} f(A_{i})-f(A_{i-1}).
    \end{align}
    Define $B_{i} = Y_{1} + \cdots + Y_{i-1} + Z_{i+1} + \cdots + Z_{n}$ where we have omitted $Y_{i}$ from the summation. Then, $A_{i} = B_{i} + Y_{i}$ and $A_{i-1} = B_{i} + Z_{i}$. Since $f \in C_{b}^{\infty}$, there exists a real $C$ such that $\abs{f'''(x)} \leq C$ for all $x \in \R$. Via the Taylor series,
    \begin{align}
        \abs{f(A_{i})-f(B_{i})-Y_{i}f'(B_{i})-\frac{Y_{i}^{2}}{2}f''(B_{i})} \leq C \frac{\abs{Y_{i}}^{3}}{6}.
    \end{align}
    Using $\abs{EX} \leq E\abs{X}$, and noting that $B_{i}$ and $Y_{i}$ are independent, we get
    \begin{align}
        \abs{Ef(A_{i})-Ef(B_{i}) - E[Y_{i}f'(B_{i})]-E[\frac{Y_{i}^{2}}{2} f''(B_{i})]} &\leq C \frac{E\abs{Y_{i}}^{3}}{6} \notag \\
        \implies \abs{E\left[f(A_{i})-f(B_{i})-\frac{f''(B_{i})}{2n}\right]} &\leq C \frac{E\abs{Y_{i}^{3}}}{6}.
    \end{align}
    Similarly, expanding $A_{i-1}$ around $B_{i}$ gives
    \begin{align}
        \abs{E\left[f(A_{i-1})-f(B_{i})-\frac{f''(B_{i})}{2n}\right]} &\leq C \frac{E\abs{Z_{i}^{3}}}{6}.
    \end{align}
    Combining the last two equations, the triangle inequality gives
    \begin{align}
        \abs{Ef(A_{i})-Ef(A_{i-1})} \leq \frac{C}{6}[E\abs{Y_{i}}^{3}+E\abs{Z_{i}}^{3}].
    \end{align}
    We use this result to get
    \begin{align}
        \abs{Ef(T_{n})-Ef(Z)} &= \abs{\sum_{i=1}^{n} Ef(A_{i}) - Ef(A_{i-1})} \leq \sum_{i=1}^{n} \abs{Ef(A_{i})-Ef(A_{i-1})} \leq \frac{C}{6} \sum_{i=1}^{n} \abs{E\abs{Y_{i}}^{3} +E\abs{Z_{i}}^{3}}.
    \end{align}
    Explicitly computing the required expectations gives us
    \begin{align}
        E\abs{Y_{i}}^{3} &= E\abs{\frac{X_{1}-\mu}{\sigma\sqrt{n}}}^{3} = \frac{1}{\sigma^{3}n^{3/2}} E\abs{X_{1}-\mu}^{3},\\
        E\abs{Z_{1}}^{3} &= E\abs{\frac{Z_{1}}{1/\sqrt{n}} \cdot \frac{1}{\sqrt{n}}}^{3} = \frac{1}{n^{3/2}} E\abs{N(0,1)}^{3}.
    \end{align}
    Thus, the quantity $\frac{C}{6} \sum_{i=1}^{n} \abs{E\abs{Y_{i}}^{3} + E\abs{Z_{i}}^{3}}$ goes to 0, and the quantity $\abs{Ef(T_{n})-Ef(Z)}$ is minimized.
\end{proof}

\begin{definition}
    The \eax{moment generating function} of a random variable $X$ is $M_{X}:\R \to \R$ defined as $M_{X}(t) = E[e^{tX}]$ for all $t \in \R$.
\end{definition}

If $\{X_{n}\}$ is a sequence of random variables and $X$ is another random variables with $M_{X_{n}}(t) \toup{n \to \infty} M_{X}(t)$ for all $t \in \R$, then $X_{n} \rightrightarrows X$. For an alternative proof of the central limit theorem, we can show that
\begin{align}
    E\left[ \exp{\left(t \cdot \dfrac{S_{n}-n\mu}{\sigma\sqrt{n}} \right)} \right] \toup{n \to \infty} E[e^{tZ}].
\end{align}


\begin{appendices}

\titleformat{\chapter}[display]
    {\normalfont\Large\bfseries}
    {\chaptername\ \thechapter}{20pt}{\Huge}

\titlespacing*{\chapter}{0pt}{20pt}{40pt}

\chapter{Appendix}
Extra content goes here.

\printindex

\end{appendices}

\end{document}
