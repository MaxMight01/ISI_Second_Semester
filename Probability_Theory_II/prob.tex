
\documentclass[15pt,a4paper]{book}

\usepackage{amsmath, amsthm, amssymb} 
\usepackage{graphicx} % For including graphics
\usepackage{hyperref} % For clickable links
\usepackage{bookmark} % Better control over bookmarks
\usepackage{geometry} % Customize page layout
\usepackage{xcolor} % Colors for text and graphics
\usepackage{enumitem} % Customizable lists
\usepackage{fancyhdr} % Header and footer
\usepackage{titlesec} % Custom section/chapter titles
\usepackage[toc,page]{appendix} % For the appendix
\usepackage{longtable} % For tables spanning multiple pages
\usepackage{mathrsfs} % For script fonts in math mode
\usepackage{tocloft} % Custom table of contents
\usepackage{datetime2} % For dates
\usepackage{caption} % For better control over captions
\usepackage{float} % Fine control over figure/table placement
\usepackage{imakeidx} % For index

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\cftchapfont}{\normalfont} % Remove bold for chapter names
\renewcommand{\cftchappagefont}{\normalfont} % Remove bold for chapter page numbers
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\eax}[1]{\emph{#1}\index{#1}} % Macro for emphasis and index
\newcommand{\abs}[1]{\left| #1 \right|} % Absolute value
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor} % For flooor

% Custom Notation List Environment
\newlist{notationlist}{description}{1}
\setlist[notationlist]{font=\bfseries,labelsep=1em}

% Geometry Settings
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
}

% Hyperref Colors
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    citecolor=red
}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}

% Custom Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Chapter name on top left
\fancyhead[R]{\rightmark}  % section name on top right
\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Making index
\makeindex[intoc]

% Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries \centering}
  {\chaptername\ \thechapter}{20pt}{\Huge \centering}

\titlespacing*{\chapter}{0pt}{20pt}{100pt}

\begin{document}

\pagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vspace*{\fill}
    % Title in all caps
    {\Huge \textbf{\MakeUppercase{Probability Theory II}}\par}

    \vspace{0.5cm} % Adjust vertical spacing between title and subtitle
    % Subtitle in normal text, slightly enlarged
    {\Large Matthew Joseph, notes by Ramdas Singh\par}

    \vspace{0.5cm} % Additional spacing before the author
    % Author information
    {\large Second Semester\par}
    \vspace*{\fill}
    \end{center}
\end{titlepage}

\clearpage

\pagenumbering{roman}

\chapter*{List of Symbols}
\begin{notationlist}
    \item $\Omega$, a sample space.
    \item $\omega$, an element of a sample space.
    \item $EX$, the expectation of the random variable $X$.
    \item $\text{Var}X$, the variance of the random variable $X$.
    \item $N(\mu,\sigma^{2})$, a normal distribution with expectation $\mu$ and variance $\sigma^{2}$.
    \item $N_{n}(k)$, the number of paths from $(0,0)$ to $(n,k)$ in a simple random walk.
    \item $N_{n}^{+}(k)$, the number of paths from $(0,0)$ to $(n,k)$ through strictly positive values in a random walk.
    \item $p_{k}^{X}$, the probability mass function for a random variable $X$.
\end{notationlist}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
\pagenumbering{arabic}
\pagestyle{fancy}


%%-------------------------------------------------------------------------------------------------


\chapter{RANDOM WALKS AND MISC. RESULTS}


\textit{January 3rd.}

We first start with some initial statements.
Let $\Omega$ be a countable state space, and let each $\omega \in \Omega$ have a probabiltiy $P(\omega)$ associated with it.

\begin{lemma}
    For random variables $X,Y$ such that $X(\omega) \leq Y(\omega)$ for all $\omega \in \Omega$. Then, $EX \leq EY$.
\end{lemma}
\begin{proof}
    This can easily be seen by summing over all terms via the alternate definition of the expectation,
    \begin{equation}
        EX = \sum_{\omega \in \Omega} X(\omega) P(\omega) \leq \sum_{\omega \in \Omega} Y(\omega) P(\omega) = EY.
    \end{equation}
\end{proof}

We now state Markov's inequality. 

\begin{theorem}[\eax{Markov's inequality}]
    If $X$ is a non-negative randm variable, then for $a > 0$, we have
    \begin{equation}
        P(X > a) \leq \frac{EX}{a}.
    \end{equation}
\end{theorem}
\begin{proof}
    Define an indicator function $I_{a}(\omega)$ as 1 if $X(\omega) \geq a$, and 0 if otherwise. We then have
    \begin{align}
        I_{a}(\omega) &\leq \frac{X(\omega)}{a} \implies P(X \geq a) = EI_{a} \leq \frac{1}{a} EX.
    \end{align}
\end{proof}
\begin{remark}
    A better upper bound here may be found by starting with $I_{a}(\omega)X(\omega)$ instead of just $X(\omega)$.
\end{remark}
If we have $X \sim N(0,1)$, then we can find an upper bound for its probability density function.
\begin{equation}
    P(X > a) = \int_{a}^{\infty} \frac{1}{\sqrt{2\pi}} e^{\frac{-x^{2}}{2}} dx \leq \int_{a}^{\infty} \frac{1}{\sqrt{2\pi}} \frac{x}{a} e^{\frac{-x^{2}}{2}} dx = \frac{e^{\frac{-a^{2}}{2}}}{\sqrt{2\pi}a}.
\end{equation}

Note that $X$ here is a random variable over a continuous state space; the previous lemma and Markov's inequality also work here. We are to show them for the continuous case instead of the discrete one.

\begin{proof}
    Here, we have $0 \leq X(\omega) \leq Y(\omega)$ for all $\omega$ in our continuous state space $\Omega$. We see that $\{X > x\} \subseteq \{Y > x\} \implies P(X > x) \leq P(Y > x)$. Integrating both sides gives us $EX \leq EY$.
\end{proof}

\begin{theorem}[\eax{Chebyshev's inequality}]
    Let $X$ be a random variable with finite mean $\mu = EX$ and finite variance $\sigma^{2} = \text{Var}(X)$. Then for $a > 0$, 
    \begin{equation}
        P(\abs{X-\mu}>a) \leq \frac{\text{Var}(X)}{a^{2}}.
    \end{equation}
\end{theorem}
\begin{proof}
    Start with the proof of Markov's inequality, replacing the indiciator function with one that's unity when $\abs{X-\mu} \geq a$.
\end{proof}
\begin{example}
    Suppose $X_{1}, X_{2}, \ldots, X_{n}$ are $n$ independent and identically distributed random variables, with $EX_{i} = \mu$ and $\text{Var}X_{i} = \sigma^{2}$. If $S_{n} = \sum X_{i}$, we then have
    \begin{equation}
        P(\abs{S_{n}-n\mu} > a) \leq \frac{\text{Var}S_{n}}{a^{2}} = \frac{n\sigma^{2}}{a^{2}}.
    \end{equation}
    If we replace $a$ with $n^{\frac{1}{2}+\varepsilon}$, we then have
    \begin{equation}
        P(\abs{S_{n}-n\mu} > n^{\frac{1}{2}+\varepsilon}) \leq \frac{\sigma^{2}}{n^{2 \varepsilon}} \to 0 \text{ as } n \to \infty.
    \end{equation}
\end{example}
\begin{proposition}
    If $\text{Var}(X) = 0$, then $P(X=EX) = 1$.
\end{proposition}
\begin{proof}
    For all $\varepsilon > 0$, we have
    \begin{equation}
        P(\abs{X-EX} > \varepsilon) \leq \frac{\text{Var}X}{\varepsilon^{2}} = 0.
    \end{equation}
    Define $A_{n}$ as $\{\abs{X-EX} > \frac{1}{n}\}$. Taking $P(\bigcup A_{n}) = \lim_{n \to \infty} P(A_{n})$, the proof follows.
\end{proof}

\section{The Law of Large Numbers}

We start by stating the weak law of large numbers.
\begin{theorem}[\eax{Weak law of large numbers}]
    Let $\{X_{k}\}_{k \geq 1}$ be a sequence of independent and identically distributed random variables with $E\abs{X_{i}} < \infty$. Let $\mu = EX_{i}$. Then for any $a > 0$,
    \begin{equation}
        \lim_{n \to \infty} P\left(\abs{\frac{X_{1}+X_{2}+\ldots+X_{n}}{n} - \mu} > a \right) = 0.
    \end{equation} 
\end{theorem}
\begin{proof}
    For now, let us assume that $\Omega$ is countable. We begin with the case where the variance of $X_{i}$, $\sigma^{2}$, is finite. Fix $a > 0$, and let $S_{n} = X_{1} + X_{2} + \ldots + X_{n}$. Then,
    \begin{equation}
        P\left(\abs{\frac{S_{n}}{n} - \mu} > a\right) = P(\abs{S_{n}-n\mu} > na) \leq \frac{\text{Var}S_{n}}{n^{2}a^{2}} = \frac{n\sigma^{2}}{n^{2}a^{2}} \to 0 \text{ as } n \to \infty.
    \end{equation}
    We now focus the case when the variance, $\sigma^{2}$, is infinite. Assume that the expected value, $\mu$, is 0; if it were non-zero, we would then instead work with $X_{i}-\mu$. Let $\delta > 0$; we shall choose a particular $\delta$ later. For each $n$, define $n$ pairs of random variables, $U_{1},V_{1}, \ldots, U_{n},V_{n}$, as $U_{k} = X_{k}, V_{k} = 0$ if $\abs{X_{k}} \leq \delta n$, and $U_{k} = 0, V_{k} = X_{k}$ if $\abs{X_{k}} > \delta n$. $X_{k}$ can be rewritten as $U_{k} + V_{k}$. We then have
    \begin{align}
        \{\abs{X_{1} + \ldots + X_{n}} \geq na\} &\subseteq \{\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\} \cup \{\abs{V_{1} + \ldots + V_{n}} \geq \frac{na}{2}\} \\
        \implies P\left(\abs{X_{1} + \ldots + X_{n}} \geq na\right) &\leq P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) + P\left(\abs{V_{1} + \ldots + V_{n}} \geq \frac{na}{2}\right).
    \end{align}
    We focus on the first term on the right hand side. The $U_{i}$'s are independently and identically distributed, so
    \begin{align}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4E[\abs{U_{1}+\ldots+U_{n}}^{2}]}{a^{2}n^{2}} = \frac{4}{a^{2}n^{2}} \left( \text{Var}(U_{1} + \ldots + U_{n}) + (nEU_{i})^{2} \right).
    \end{align}
    For the variance, we have
    \begin{equation}
        \text{Var}(U_{1}+\ldots+U_{n}) = n \text{Var}U_{i} \leq n EU_{i}^{2} \leq n E[\abs{U_{i}}\abs{U_{i}}] \leq \delta n^{2} E[\abs{U_{i}}]
    \end{equation}
    which transforms the previous equation as
    \begin{equation}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4}{a^{2}n^{2}} \left( \delta n^{2} E[\abs{U_{i}}] + (nEU_{i})^{2} \right).
    \end{equation}
    A lemma (to be proven later) states that $E[\abs{U_{i}}] = E[\abs{X_{i}}]$ as $n \to \infty$, and $EU_{i} = EX_{i} = 0$ too. So,
    \begin{equation}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4}{a^{2}n^{2}} \left( \delta n^{2} E[\abs{U_{i}}] + (nEU_{i})^{2} \right) \leq \frac{4 \delta E[\abs{U_{i}}]}{a^{2}} + \frac{4}{a^{2}} (EU_{i})^{2}.
    \end{equation}

    For the second term on the right hand side, begin with
    \begin{align}
        P(V_{1}+ \ldots + V_{n} \neq 0) &\leq P(\{V_{1} \neq 0\} \cup \ldots \cup \{V_{n} \neq 0\}) \leq nP(V_{i} \neq 0) = n \sum_{\abs{x} > \delta n} P(X_{i} = x) \notag \\
        &\leq n \sum_{\abs{x} > \delta n} \frac{\abs{x}}{\delta n} P(X_{i} = x) = \frac{1}{\delta} E[\abs{V_{i}}]. 
    \end{align}
    The rightmost term here tends to 0 as $n \to \infty$. Now choose $\delta$ to be $\frac{\varepsilon a^{2}}{\abs{6 E{\abs{X_{i}}}}}$, and then choose $N$ to be large enough such that for all $n > N$, both the terms are smaller than $\frac{\varepsilon}{2}$.
\end{proof}
\textit{January 7th.}

We now prove the lemma called upon earlier.
\begin{lemma}
    If $X$ is a discrete random variable and takes values $y_{1},y_{2},\ldots,y_{k}$, and $E[\abs{X}] < \infty$, then $\lim_{n \to \infty} E[\abs{X}1_{\abs{X} \leq n}] = E[\abs{X}]$.
\end{lemma}
\begin{proof}
    Notice that the terms on the left hand side and right hand side are $\sum_{y_{k} : \abs{y_{k}} \leq n}$ and $\sum_{y_{k}} \abs{y_{k}} P(Y = y_{k})$. The condition for convergence may now be applied.
\end{proof}
The above equation, begin inside absolute braces, must imply that the term $E[X \cdot 1_{\abs{X} \leq n}]$ must also absolutely converge to $EX$.

\section{Simple Random Walk}
Let $X_{1},X_{2},\ldots$ be independent and identically distributed random variables, with $X_{i} = 1$ with probability $\frac{1}{2}$ and $X_{i}=-1$ with probability $\frac{1}{2}$. Now define $S_{0}=0$ and $S_{n} = \sum_{i=1}^{n} X_{i}$. The sequence $(S_{n})_{n \geq 0}$ is a \eax{simple random walk}.

Note that $S_{0}=k_{0}=0,S_{1}=k_{1},\ldots,S_{n}=k_{n}$ can occur if and only if $\abs{k_{i}-k_{i+1}} = 1$ for all $0 \leq i \leq n-1$. The sequence $(k_{n})_{n \geq 0}$ is a \eax{simple path} of the simple random walk. By the event $\{S_{n}=k\}$, we are concerned with the event that the random walk visits $k$ at step $n$. If $(k_{n})_{n \geq 0}$ is given we have $X_{i} = k_{i}-k_{i-1}$. Because the $X_{i}$'s are independent and identically distributed, each event $\{X_{1}=l_{1},X_{2}=l_{2},\ldots,X_{n}=l_{n}\}$, where $l_{i} = \pm 1$, is equally likely with probability $\frac{1}{2^{n}}$. Thus,
\begin{equation}
    P(S_{n}=k) = \frac{N_{n}(k)}{2^{n}}
\end{equation}
where $N_{n}(k)$ is defined as the number of distinct of path that start at $0$ and end at $k$ at step $n$. We also define $N_{n}^{+}(k)$ to be the number of distinct paths that end at $k$ at step $n$ and stay above the $x$-axis up to time $n-1$. The probability of the corresponding event is
\begin{equation}
    P(\{S_{1} > 0, S_{2} > 0, \ldots S_{n-1} > 0, S_{n} = k\}) = \frac{N_{n}^{+}(k)}{2^{n}}.
\end{equation}

\begin{lemma}
    Suppose $a,a',b,b'$ are integers, with $0 \leq a < a'$. Then the number of distinct path from $(a,b)$ to $(a',b')$ depends only on $a'-a = n$ and $b'-b = k$, and is given by $\binom{n}{\frac{n+k}{2}}$.
\end{lemma}
\begin{proof}
    Notice that we need $x$ $+1$'s and $y$ $-1$'s to appear, satisfying $x+y = a'-a$ and $x-y = b'-b$. Solving, we get $x = \frac{n+k}{2}$ and $y = \frac{n-k}{2}$. Thus, the number of paths is given by $\binom{n}{\frac{n+k}{2}}$.
\end{proof}
Using this lemma, we find that $N_{n}(k) = \binom{n}{\frac{n+k}{2}}$. The following convention is now followed; if $t$ is not an integer, then $\binom{n}{t} = 0$.

\begin{lemma}[The \eax{method of images}]
    Suppose $a,a',b,b'$ are integers, with $0 \leq a < a'$ and $b,b'>0$. Then the number of distinct paths from $(a,b)$ to $(a',b')$ that intersect the $x$-axis is equal to the number of paths from $(a,-b)$ to $(a',b')$.
\end{lemma}
\begin{proof}
    Consider any path $(b=k_{0},k_{1},\ldots,k_{n-1},k_{n}=b')$, from $(a,b)$ to $(a',b')$, that intersects the $x$-axis. Let $j$ be the smallest index for which $k_{j}=0$. For ease, denote $(a,b)$ by $A$, $(a',b')$ by $A'$, $(a+j,0)$ by $B$, and $(a,-b)$ by $A''$. Reflect the segment from $A$ to $B$ about the $x$-axis to obtain a `mirrored-path' from $A''$ to $B$; $(-b=-k_{0},-k_{1},\ldots,-k_{j-1},k_{j}=0,k_{j+1},\ldots,k_{n}=b')$. There is now a one-to-one correspondence between the paths from $A$ to $A'$ that intersect the $x$-axis, and the paths from $A''$ to $A'$.
\end{proof}
We can now easily compute $N_{n}^{+}(k)$; it simply the number of paths from $(1,1)$ to $(n,k)$ that do not intersect the $x$-axis.
\begin{theorem}[\eax{Ballot theorem}]
    The number of paths that progress from $(0,0)$ to $(n,k)$ through strictly positive values is given by $N_{n}^{+}(k) = \frac{k}{n}N_{n}(k)$.
\end{theorem}
\begin{proof}
    We have
    \begin{align}
        N_{n}^{+}(k) &= \text{ number of paths from $(1,1)$ to $(n,k)$} - \text{ number of such paths that intersect the $x$-axis} \notag \\
        &= N_{n-1}(k-1) - N_{n-1}(k+1) \notag \\
        &= \binom{n-1}{\frac{n+k}{2}-1} - \binom{n-1}{\frac{n+k}{2}} = \frac{k}{n}\binom{n}{\frac{n+k}{2}} = \frac{k}{n}N_{n}(k).
    \end{align}
\end{proof}
Suppose $n = 2\nu$. Define $u_{2\nu}$ to be $P(S_{2\nu}=0) = \frac{\binom{2\nu}{\nu}}{2^{n}}$. The question we ask is to compute the probability that the first return to 0, if at all, occurs after step $n$. It can be found out as
\begin{align}
    P(\text{first return to $0$...}) &= P(S_{1}\neq 0, S_{2} \neq 0, \ldots, S_{2\nu} \neq 0) \\
    &= P(S_{1}>0,\ldots,S_{2\nu}>0) + P(S_{1}<0,\ldots,S_{2\nu}<0) \notag \\
    &= 2P(S_{1}>0,\ldots,S_{2\nu}>0) \notag \\
    &= 2 \sum_{k \text{ even}, k>0} P(S_{1}>0,\ldots,S_{2\nu-1}>0,S_{2\nu}=k) \notag \\
    &= \frac{2}{2^{2\nu}} \sum_{k \text{ even}, k>0} N_{2\nu}^{+}(k) \notag \\
    &= \frac{2}{2^{2\nu}} \sum_{k \text{ even}, k>0} N_{2\nu-1}(k-1)-N_{2\nu-1}(k+1) \notag \\
    &= \frac{2}{2^{2\nu}} N_{2\nu-1}(1) = u_{2\nu}.
\end{align}
We state this down as a lemma.
\begin{lemma}[\eax{Basic lemma}]
    For $n$ even, the probability that the first return to 0, if at all, occurs after step $n$ is the same as the probability that the location at step $n$ is 0. For $n$ odd, it is the probability that the location at step $n-1$ is 0.
\end{lemma}
We ask another question; for a fixed $n$, where does the random walk achieve its first maximum upto time $n$? For this, denote by $M_{n}$ the index $m$ at which the walk $S_{0},S_{1},\ldots,S_{n}$, over $n$ steps, achieves its maximum for the first time.

For $0<m<n$, $M_{n}=m$ if and only if $S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}$ and $S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n}$. Notice that the first of these two conditions depends only on $X_{1},X_{2},\ldots,X_{m}$, and the second condition depends only on $X_{m+1},X_{m+2},\ldots,X_{n}$. So, $P(M_{n}=m) = P(S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}) \cdot P(S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n})$.

The key idea here is to consider the \eax{reversed walk}; define a new walk with $X_{1}'=X_{m},\; X_{2}'=X_{m-1},\ldots,X_{m}'=X_{1}$. Also define $S_{k}' = X_{1}' + \ldots + X_{k}'$. From here, we can deduce that $S_{m}>S_{m-i}$ is true if and only if $X_{m}+\ldots+X_{m-i}>0$ is true, which is true if and only if $S_{i}'>0$ is true. So, $P(S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}) = P(S_{1}'>0,\; S_{2}'>0, \ldots, S_{m}'>0)$. If we now define $S_{k}''=X_{m+1}+\ldots+X_{m+k}$, we have
\begin{align*}
    P(S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n}) &= P(X_{m+1}\leq 0,\; X_{m+1}+X_{m+2}\leq 0, \ldots, X_{m+1}+\ldots+X_{n}\leq 0) \\
    &= P(S_{1}''\leq 0,\; S_{2}''\leq 0,\ldots,S_{n-m}''\leq 0) \\
    &= P(S_{1}''\geq 0,\; S_{2}''\geq 0,\ldots,S_{n-m}''\geq 0)
\end{align*}
The first of the terms discussed, $P(S_{1}'>0,\; S_{2}'>0, \ldots, S_{m}'>0)$, can be computed for $m = 2\nu, 2\nu+1$; it is simply $\frac{1}{2}u_{2\nu}$. For the latter of these terms, we introduce a new random variable $\tilde{X}$ which has the same distribution as the $X_{i}$'s and is independent. Also define $\tilde{S_{i}}$ to be $\tilde{X}+X_{1}+\ldots+X_{i-1}$ and $\tilde{S_{0}}$ to be $0$. We then have
\begin{align}
    \frac{1}{2}P(S_{0} \geq 0, \ldots, S_{n-m} \geq 0) &= P(\tilde{X}=1) \cdot P(S_{0} \geq 0, \ldots, S_{n-m} \geq 0) \notag\\
    &= P(\tilde{X}=1,S_{0} \geq 0, S_{0} \geq 0, \ldots, S_{n-m} \geq 0) \notag\\
    &= P(\tilde{S_{1}}=1,\tilde{S_{2}}>0,\ldots,\tilde{S_{n-m+1}}>0) \notag\\
    &= P(S_{1}>0,S_{2}>0,\ldots,S_{n-m+1}>0).
\end{align}
Thus, we get
\begin{equation}
    P(M_{n}=m) = \frac{1}{2} u_{2k}u_{2\nu-2k}
\end{equation}
where $m$ is of the form $2k$ or $2k+1$, and $n$ is of the form $2\nu$, with $1 < k < \nu$.\\ \\
\textit{January 10th.}

Plugging in $m = 0$, we get $P(M_{n}=0) = P(S_{1} \leq 0, \ldots, S_{2 \nu} \leq 0) = \frac{1}{2}u_{2\nu}$. For $m=n$, we have $P(M_{n}=n) = P(S_{1}\leq 0, \ldots, S_{2\nu} \leq 0) = \frac{1}{2}u_{2 \nu}$. Let us first compute $u_{2k}$.
\begin{align}
    u_{2k} &= P(2k=0) = \frac{\binom{2k}{k}}{2^{2k}} = \frac{(2k)!}{(k!)^{2}2^{2k}} \notag \\
    &\sim \frac{(2k)^{2k+\frac{1}{2}} e^{-2k} \sqrt{2\pi}}{(\sqrt{2\pi} k^{k+\frac{1}{2}} e^{-k})^{2}2^{2k}} = \frac{1}{\sqrt{\pi k}}.
\end{align}
For $0 < a < b < 1$, we have
\begin{align}
    P(an \leq M_{n} \leq bn) &= \sum_{m = an}^{bn} P(M_{n}=m) = \sum_{k=a\nu}^{b\nu} u_{2k}u_{2\nu-2k} \notag \\
    &\sim \sum_{k=a\nu}^{b\nu}\frac{1}{\sqrt{\pi k}}\frac{1}{\sqrt{\pi (\nu-k)}} = \sum_{k=a\nu}^{b\nu} \frac{1}{\nu \sqrt{\pi \frac{k}{\nu}} \sqrt{\pi (1-\frac{k}{\nu})}}\notag \\
    &\to \frac{1}{\pi} \int_{a}^{b} \frac{dx}{\sqrt{x(1-x)}} = \frac{2}{\pi} (\arcsin{\sqrt{b}} - \arcsin{\sqrt{a}}).
\end{align}
In fact, this is the \eax{arcsin law for maxima}; for $0 \leq t \leq 1$, we have
\begin{equation}
    \lim_{n \to \infty} P\left( \frac{M_{n}}{n} \leq t\right) = \frac{2}{\pi} \arcsin{\sqrt{t}}.
\end{equation}
If we look at this as a cumulative density funtion, the probability density function becomes $\frac{d}{dt} \frac{2}{\pi} \arcsin{\sqrt{t}} = \frac{1}{\pi \sqrt{t(1-t)}}$.

We are now interested in $\tilde{M}_{n}$, the last time when maximum up to time $n$ is attained. We can just look at the walk backwards again; in this case, we get
\begin{equation}
    P(\frac{\tilde{M}_{n}}{n}) = P\left(\frac{n-\tilde{M}_{n}}{n} \leq t\right) \to \frac{2}{\pi} \arcsin{\sqrt{t}}.
\end{equation}

We now ask the probability that the random walk of $n = 2\nu$ steps last visit $0$ at time $2k$. We denote by $K_{n}$ the location of the last return to $0$ in a walk of $n$ steps. Now look at
\begin{align}
    \alpha_{2k,2\nu} &= P(K_{n}=2k) = P(S_{2k}=0,S_{2k+1}\neq 0,\ldots,S_{2\nu}\neq 0) \notag
    \\ &= P(S_{2k}=0) \cdot P(X_{2k+1}\neq 0, \ldots, X_{2k+1}+\ldots+X_{2\nu} \neq 0) \notag \\
    &= P(S_{2k}=0) \cdot P(S_{1} \neq 0, \ldots, S_{2\nu-2k} \neq 0) = u_{2k}u_{2\nu-2k}.
\end{align}
We can also state an \eax{arcsin law for last visit} here; for $0 < t < 1$
\begin{equation}
    \lim_{n \to \infty} P(K_{n} \leq tn) = \frac{2}{\pi} \arcsin{\sqrt{t}}.
\end{equation}
If we set the an additional limit that says $t$ tends to $0$, replacing $t$ by an arbitrary $\varepsilon > 0$, we have
\begin{equation}
    \lim_{n \to \infty} P(K_{n}=0) = 0.
\end{equation}
Given enough time, a simple random walk must return to 0.

Denote by $f_{2n}$ the probability that the first return to 0 occurs at time $2n$.
\begin{align}
    f_{2n} &= P(S_{1} \neq 0, \ldots, S_{2n-1} \neq 0, S_{2n} = 0) \notag \notag \\
    &= P(S_{1} \neq 0, \ldots, S_{2n-1} \neq 0) - P(S_{1} \neq 0, \ldots, S_{2n} \neq 0) \notag \\
    &= P(S_{1} \neq 0, \ldots, S_{2n-2} \neq 0) - P(S_{1} \neq 0, \ldots, S_{2n} \neq 0) \notag \\
    &= u_{2n-2} - u_{2n} = \frac{1}{2n-1} u_{2n}.
\end{align}
\begin{lemma}
    With the usual notation,
    \begin{equation}
        u_{2n} = f_{2}u_{2n-2} + f_{4}u_{2n-4} + \ldots + f_{2n} u_{0}.
    \end{equation}
\end{lemma}
\begin{proof}
    We have
    \begin{align}
        P(S_{2n} = 0) &= \sum_{k=1}^{n} P(S_{2n}=0, \text{ first return at $2k$}) \notag\\
        &= \sum_{k=1}^{n} P(\text{first return at 2k}) \cdot P(S_{2n} = 0 \mid \text{first return at 2k}) \notag\\
        \implies P(S_{n} = 0) &= \sum_{k=1}^{n} f_{2k} u_{2n-2k}.
    \end{align}
\end{proof}
\begin{theorem}
    The probability that in the time interval $0$ to $n=2\nu$, the random walk spends $2k$ amount of time on the positive side and $2\nu-2k$ amount of time on the negative side is $\alpha_{2k,2\nu}$.
\end{theorem}
\begin{corollary}
    For $0 < t < 1$,
    \begin{equation}
        P(\text{random walk spends less than $tn$ time on positive side}) \to \frac{2}{\pi} \arcsin{\sqrt{t}}.
    \end{equation}
\end{corollary}
\begin{proof}
    This is the proof of the theorem. We introduce $b_{2k,2\nu}$; it is defined as the probability that the random walk of length $2\nu$ and $2k$ sides above the $x$-axis. We need to show that $b_{2k,2\nu} = \alpha_{2k,2\nu}$. We have
    \begin{align}
        b_{2\nu,2\nu} &= P(S_{1} \geq 0, S_{2} \geq 0, \ldots, S_{2\nu} \geq 0) = u_{2\nu}, \\
        b_{0,2\nu} &= P(S_{1} \leq 0, \ldots, S_{2\nu} \leq 0) = u_{2\nu}.
    \end{align}
    We are left to prove it for $1 \leq k \leq \nu-1$. Assume that exactly $2k$ out of $2\nu$ time are spent above the $x$-axis, with $1 \leq k \leq \nu-1$. Suppose first return to 0 occurs at time $2r < 2\nu$. We deal in cases.
    \begin{itemize}
        \item Case I: $2r$ time units upto first return are on the positive side. Then, $r \leq k \leq \nu-1$. The time from $2r$ to $2\nu$ has to be above the $x$-axis, $2k-2\nu$ time. The number of such paths is $(\frac{1}{2}2^{2r}f_{2r}) (2^{2\nu-2r} b_{2k-2r,2\nu-2r})$.
        \item The $2r$ time units upto the first return are on the negative side. The nubmer of such paths is $(\frac{1}{2}2^{2r}f_{2r})(2^{2\nu-2r}b_{2k,2\nu-2r})$. Also, $\nu-r \geq k$.
    \end{itemize}
    Thus, we have
    \begin{equation}
        b_{2k,2\nu} = \frac{1}{2}\sum_{r=1}^{k} f_{2r} b_{2k-2r,2\nu-2r} + \frac{1}{2} \sum_{r=1}^{\nu-k} f_{2r} b_{2k,2\nu-2r}.
    \end{equation}
    We now proceed with induction on $\nu$. We have already shown this for $\nu = 1$; assume that this is true for $\nu \leq V - 1$. By induction,
    \begin{align}
        b_{2k,2V} &= \frac{1}{2}\sum_{r=1}^{k} f_{2r} \alpha_{2k-2r,2V-2r} + \frac{1}{2} \sum_{r=1}^{V - k} f_{2r} \alpha_{2k,2V-2r} \notag \\
        &= \frac{1}{2} u_{2V-2k} \sum_{r=1}^{k} f_{2r} u_{2k-2r} + \frac{1}{2} u_{2k} \sum_{r=1}^{V-k} f_{2r} u_{2V-2k-2r} \notag \\
        &= u_{2k}u_{2\nu-2k} = \alpha_{2k,2\nu}.
    \end{align}
\end{proof}

\textit{January 17th.}
\begin{theorem}[\eax{Weirstrass's polynomial approximation}.]
    Let $f:[0,1] \to \R$ be a continuous function. Then for every $\varepsilon > 0$, there is a polynomial $P$, dependent on $f$ and $\varepsilon$, such that
    \begin{equation}
        \abs{f(x)-P(x)} < \varepsilon \text{ for all } x \in [0,1].
    \end{equation}
\end{theorem}
\begin{remark}
    Any continuous function $f:[0,1]\to\R$ is bounded and uniformly continuous. This fact will be useful in proving the previous theorem.
\end{remark}
\begin{proof}
    Start with $X_{1},X_{2},\ldots$ which are independent and identically disitributed Bernoulli random variables, $\text{Ber}(x)$. Let $S_{n} = X_{1} + X_{2} + \ldots + X_{n}$. From the weak law of large numbers, we know that $\frac{S_{n}}{n}$ is approximately $x$. We can expect that $f(x)$ will also be approximately $f(\frac{S_{n}}{n})$. We now have
    \begin{align}
        f_{n}(x) &= Ef(\frac{S_{n}}{n}) = \sum_{j=0}^{n} f(\frac{j}{n})P(S_{n}=j) \notag \\
        &= \sum_{j=0}^{n} f(\frac{j}{n}) \binom{n}{j} x^{j} (1-x)^{n-j}.
    \end{align}
    This is now a polynomial; we wish to see how close this is to $f$. Define $A_{\delta}$ to be $\{j : \abs{\frac{j}{n} - x} \leq \delta\}$
    \begin{align}
        \abs{f_{n}(x)-f(x)} &= \abs{\sum_{j=0}^{n} \left( f(\frac{j}{n}) - f(x) \right)} P(S_{n}=j) \notag \\
        &= \abs{\sum_{j \in A_{\delta}} \left( f(\frac{j}{n}) - f(x) \right) + \sum_{j \notin A_{\delta}} \left( f(\frac{j}{n}) - f(x) \right)} P(S_{n}=j) \notag \\
        &\leq \sum_{j \in A_{\delta}} \abs{ f(\frac{j}{n}) - f(x) } P(S_{n}=j) + \sum_{j \notin A_{\delta}} \abs{ f(\frac{j}{n}) - f(x) } P(S_{n}=j).
    \end{align}
    We have two terms to deal with now. For the first term, choose $\delta > 0$ such that $\abs{x-y} < \delta \implies \abs{f(x)-f(y)} < \varepsilon$; this $\delta$ can be chosen since $f$ is uniformly continuous. Similarly, also choose $M = \sup_{x \in [0,1]} \abs{f(x)}$. $M$ is finite since $f$ is bounded. Thus, we have
    \begin{align}
        \sum_{j \in A_{\delta}} \abs{f(\frac{j}{n})} P(S_{n}=j) \leq \sum_{j \in A_{\delta}} \varepsilon P(S_{n} = j) \leq \varepsilon
    \end{align}
    and
    \begin{align}
        \sum_{j \notin A_{\delta}} \leq 2M P(\abs{\frac{S_{n}}{n}-x} > \delta) \leq 2M \frac{\text{Var}(S_{n})}{n^{2}\delta^{2}} = \frac{2Mnx(1-x)}{n^{2}\delta^{2}}.
    \end{align}
    Combining the two, and choosing $n$ large enough, we have
    \begin{align}
        \abs{f_{n}(x)-f(x)} &\leq \varepsilon + \frac{2Mx(1-x)}{n\delta^{2}} \leq \varepsilon + \frac{M}{2n\delta^{2}} \leq 2\varepsilon.
    \end{align}
\end{proof}

\section{Erd\"os-Renyi Random Graph}
We first discuss the setup; start with $n$ vertices of an empty graph. For any pair of points $(i,j)$, with $i \neq j$, join these vertices with an edge with probability $p$ independently for all such pairs. Such a graph is denoted by $G_{n,p}$.

A collection of three points $S = \{i,j,k\}$ form a triangle if $G_{n,p}$ has the edges $\{i,j\}$, $\{j,k\}$, and $\{i,k\}$. We question the probability that such a graph has no formed triangles. Can we find $p = p_{n}$ such that triangles begin to appear at $p_{n}$? Let $S$ be any set of three vertices. Define $X_{S}$ to be the indicator function; 1 if $S$ forms a triangle, and 0 otherwise. We note that $X_{S} \sim \text{Ber}(p^{3})$. We note that
\begin{equation*}
    EX_{S} = p^{3}, \; \text{Var}X_{S} = p^{3}(1-p^{3}) \leq p^{3}.
\end{equation*}
Denote by $N$ the number of triangles in the graph $G_{n,p}$. Clearly,
\begin{equation*}
    N = \sum_{S: \abs{S} = 3} X_{S}, \;  EN = \binom{n}{3} p^{3} < n^{3}p^{3}, \; \text{Var}{N} = \sum_{S} \Var{X_{S}} + \sum_{S}\sum_{T \neq S} \Cov{(X_{S}X_{T})} \leq n^{3}p^{3} + n^{4}p^{5}
\end{equation*}
ALso, $P(N \geq 1) \leq EN < n^{3}p^{3}$. If $p = p_{n} << \frac{1}{n}$, then $P(N \geq 1) \to 0$ as $n \to \infty$. We discuss this for $p >> \frac{1}{n}$. We have
\begin{align}
    P(N = 0) \leq P(\abs{N-EN} \geq EN) \leq \frac{\text{Var}N}{(EN)^{2}} \leq \frac{(n^{3}p^{3}+n^{4}p^{5})}{\frac{n^{6}p^{6}}{100}} \leq \frac{100}{n^{3}p^{3}} + \frac{100}{n(np)} \to 0.
\end{align}
We can state this as a theorem.
\begin{theorem}
    Consider $G_{n,p_{n}}$. Let $E$ be the event that the graph is triangle free. We then have
    \begin{equation}
        P(E) \to \begin{cases}
        0 &\text{ if } \dfrac{p_{n}}{\frac{1}{n}} \to \infty,\\
        1 &\text{ if } \dfrac{p_{n}}{\frac{1}{n}} \to 0.
        \end{cases}
    \end{equation}
\end{theorem}
Now suppose that $\frac{np_{n}} \to C > 0$ as $n \to \infty$. Then we have
\begin{equation}
    N \approx \text{Poisson} \left( \frac{C^{3}}{6} \right).
\end{equation}


\textit{January 21st.}
\begin{remark}
    For this next `game', we will think of $X_{i}$'s as the winnings in game $i$ and $\mu$ to be the entrance fees for a game.
\end{remark}
\begin{definition}
    Suppose that $X_{1},X_{2},\ldots$ are independent, but not necessarily identically distributed. Let $S_{n} = X_{1} + \ldots + X_{n}$. We say a game with accumulated entrance fees $\{\alpha_{n},n\geq 1\}$ is fair if
    \begin{equation}
        P(\abs{\frac{S_{n}}{\alpha_{n}}-1} > \varepsilon) \to 0
    \end{equation}
    for all $\varepsilon > 0$.
\end{definition}
Using this definition of `fair', we look at an example.
\begin{example}
    This is the St.\ Petersburg's paradox. This is the game; toss a coin repeatedly until the first head is observed. If this head occurs at the $k^{\text{th}}$ toss, the amount paid out is $X = 2^{k}$. Let us find a fair accumulated entrance fees. In this case,
    \begin{equation}
        EX = \sum_{k=1}^{\infty} \frac{1}{2^{k}} 2^{k} = \infty.
    \end{equation}
    Suppose we play this game $n$ times. We are to find a fair accumulated sum $\{\alpha_{n}\}$ such that
    \begin{equation}
        P(\abs{S_{n}-\alpha_{n}} > \varepsilon \alpha_{n}) \to 0.
    \end{equation}
    To find this, we will define
    \begin{align}
        U_{j} &= X_{j} 1_{\{X_{j}\leq a_{n}\}}, \notag \\
        V_{j} &= X_{j} 1_{\{X_{j} > a_{n}\}}. \notag
    \end{align}
    $a_{n}$ shall be determined later. Note that $S_{n} = X_{1} + \ldots + X_{n} = U_{1} + \ldots + U_{n} + V_{1} + \ldots V_{n}$. Then,
    \begin{align}
        P(\abs{S_{n}-\alpha_{n}} > \varepsilon \alpha_{n}) \leq  P(\abs{U_{1}+\ldots+U_{n}-\alpha_{n}} > \frac{1}{2}\varepsilon \alpha_{n}) + P(\abs{V_{1} + \ldots + V_{n}} > \frac{1}{2}\varepsilon \alpha_{n}).
    \end{align}
    We first bound the second term on the right hand side. We have
    \begin{align}
        P(\abs{V_{1} + \ldots + V_{n}} > \frac{1}{2}\varepsilon \alpha_{n}) &\leq P(\bigcup_{i=1}^{n} \{V_{i} \neq 0\}) \leq nP(V_{1} \neq 0) = nP(X_{1} > a_{n}) \\
        &= \sum_{2^{k}>a_{n}} P(X=2^{k}) \leq \frac{2n}{a_{n}}.
    \end{align}
    Thus, we will require that $a_{n} >> n$. Also,
    \begin{equation}
        EU_{1} = \sum_{k \leq \log_{2}a_{n}} 2^{k} \cdot 2^{-k} = \floor{\log_{2}a_{n}}, \; \Var{U_{1}} \leq E[U_{1}^{2}] = \sum_{k \leq \log_{2}a_{n}} (2^{k})^{2} \cdot 2^{-k} = 2^{\floor{\log_{2}a_{n}}+1}-1 < 2a_{n}.
    \end{equation}
    $\frac{1}{n}(U_{1}+\ldots+U_{n}) \approx EU_{j} = \floor{\log_{2}a_{n}}$, so we should choose
    \begin{equation}
        \alpha_{n} = n EU_{j} = n \floor{\log_{2}a_{n}}.
    \end{equation}
    This gives us
    \begin{equation}
        P(\abs{U_{1}+\ldots+U_{n}-\alpha_{n}} > \frac{1}{2}\varepsilon \alpha_{n}) \leq \frac{n(2a_{n})}{\frac{1}{4}\varepsilon^{2}\alpha_{n}^{2}}.
    \end{equation}
    Thus, we have another condition where we require that $\frac{na_{n}}{\alpha_{n}^{2}} \to 0$. The conditions we require are
    \begin{equation*}
        \frac{n}{a_{n}} \to 0 \text{ and } \frac{na_{n}}{n^{2}(\log_{2}a_{n})^{2}} \to 0.
    \end{equation*}
    The sequence $\{a_{n}\}$ defined as $a_{n} = n \log_{2} n$ satisfies these properties. The sequnce $\alpha_{n}$ is thus
    \begin{equation}
        \alpha_{n} = n \log_{2} a_{n} = n \log_{2} n + n \log_{2} \log_{2} n.
    \end{equation}
\end{example}

\chapter{GENERATING FUNCTIONS}

\textit{January 24th.}

\begin{definition}
    For a sequence $\{a_{n}\}_{n \geq 0}$, the \eax{generating function} of $\{a_{n}\}$ is given as
    \begin{equation}
        A(s) = \sum_{n=0}^{\infty} a_{n}s^{n}
    \end{equation}
    for some $-s_{0}<s<s_{0}$.
\end{definition}
For this probability course, we will be interested in a particular form; for a random variable $X$ that takes values $k=0,1,\ldots$, the function we look at is
\begin{equation}
    \sum_{k=0}^{\infty}P(X=k) s^{k} \text{ for } -1 \leq s \leq 1.
\end{equation}
Suppose we have two sequences $\{a_{n}\}$ and $\{b_{n}\}$ with generating functions $A(s)$ and $B(s)$, respectively. If we define a new sequence $\{c_{n}\}$ as
\begin{equation}
    c_{n} = a_{0}b_{n} + a_{1}b_{n-1} + \ldots + a_{n-1}b_{1} + a_{n}b_{0} \text{ for all } n \geq 0,
\end{equation}
then the sequence $\{c_{n}\}$ is termed the \eax{convolution} of the sequences $\{a_{n}\}$ and $\{b_{n}\}$, and we shall denote it as
\begin{equation*}
    \{c_{n}\} = \{a_{n}\} \ast \{b_{n}\}.
\end{equation*}
Note that this convolution operation is both associative and commutative. We are now interested in finding the generating function of $\{c_{n}\}$. We have
\begin{align}
    C(s) &= \sum_{n=0}^{\infty} c_{n} s^{n} = \sum_{n=0}^{\infty} \left( \sum_{k=0}^{n} a_{k}b_{n-k} \right) s^{n} \notag \\
    &= \sum_{n=0}^{\infty} \sum_{k=0}^{n} a_{k}s^{k} b_{n-k}s^{n-k} = \sum_{k=0}^{\infty} \sum_{m=0}^{\infty} a_{k}s^{k} b_{m}s^{m} \notag \\
    \implies C(s) &= \left( \sum_{k=0}^{\infty} a_{k}s^{k} \right) \cdot \left( \sum_{m=0}^{\infty} b_{m}s^{m} \right) = A(s) \cdot B(s).
\end{align}
We state this down as a theorem.
\begin{theorem}
    $C(s) = A(s) \cdot B(s)$ when $\{c_{n}\} = \{a_{n}\} \ast \{b_{n}\}$.
\end{theorem}
Suppose $X$ takes values in $\Z_{+} = \{0,1,\ldots\}$. Denote $P(X=k)$ as $p_{k}$. The generating function is, thus,
\begin{equation*}
    \cP (s) = \sum_{k=0}^{\infty} p_{k} s^{k} = E[s^{X}].
\end{equation*}
Also,
\begin{align}
    \cP (1) &= 1,\\
    \cP'(1) &= \sum_{k=1}^{\infty} kp_{k} s^{k-1} |_{s=1} = EX.
\end{align}
Also note that
\begin{align}
    E[X^{2}] = \sum_{k=0}^{\infty} k^{2} p_{k} = \sum k(k-1) p_{k} + \sum k p_{k} = \cP''(1) + \cP'(1)
\end{align}
which gives us the variance of $X$ a
\begin{align}
    \Var X = E[X^{2}] - (EX)^{2} = \cP ''(1) + \cP'(1) - (\cP '(1))^{2}.
\end{align}
The individual probabilities of $X=k$ may also be found as
\begin{equation}
    p_{k} = P(X=k) = \frac{1}{k!} \cdot \frac{d^{k}}{ds^{k}} \cP (s) |_{s=0}.
\end{equation}

Now suppose that $X$ and $Y$ are two independent variables, taking values in $\Z_{+}$. Let $Z = X+Y$. We ask the probability that $Z$ equals $k$. We can find this as
\begin{equation}
    P(Z = k) = \sum_{m=0}^{k} P(X=m, Y = k-m) = \sum_{m=0}^{k} P(X = m) \cdot P(Y = k-m).
\end{equation}
Therefore, denoting $p_{k}^{(X)}$ to be the probability mass function of $X$, we have
\begin{equation}
    \{p_{k}^{(Z)}\} = \{p_{k}^{(X)}\} \ast \{p_{k}^{(Y)}\} \implies \cP^{(Z)}(s) = \cP^{(X)} (s) \cdot \cP^{(Y)} (s).
\end{equation}
There is an easier way to see the last equation; we could have started with $Es^{Z} = E[s^{X} \cdot s^{Y}] = E[s^{X}] E[s^{Y}]$.\\

If we have $S_{n} = X_{1} + X_{2} + \ldots + X_{n}$, where the $X_{i}$'s are independently distributed taking values in $\Z_{+}$, it can be shown that
\begin{equation}
    \{p_{k}^{(S_{n})}\} = \{p_{k}^{(X)}\}^{n \ast}
\end{equation}
\begin{example}
    Let us compute the generating function of $X \sim \text{Bin}(n,p)$. We have
    \begin{align}
        \cP (s) = \sum_{k=0}^{\infty} P(X=k) s^{k} = \sum_{k=0}^{n} \binom{n}{k} p^{k} (1-p)^{n-k} s^{k} = \left( (1-p) + ps \right)^{n}.
    \end{align}
    This is the generating function of the binomial distribution. Clearly,
    \begin{align*}
        EX &= \cP '(1) = np,\\
        \Var X &= \cP ''(1) + \cP '(1) - (\cP'(1))^{2} = n(n-1)p^{2} + np - n^{2}p^{2} = np(1-p).
    \end{align*}
    Note that using this generating function, we can also show that $\text{Bin}(n,p) + \text{Bin}(m,p) = \text{Bin}(m+n,p)$ when the former terms are independent.
\end{example}
\begin{example}
    We look at $X \sim \text{Poisson}(\lambda)$. We have
    \begin{align}
        \cP (s) = \sum_{k=0}^{\infty} e^{-\lambda} \frac{\lambda^{k}}{k!} s^{k} = e^{-\lambda} \sum_{k=0}^{\infty} \frac{(\lambda s)^{k}}{k!} = e^{-\lambda + \lambda s}.
    \end{align}
    For this, we can als verify $EX = \Var X = \lambda$. We can also show that $\text{Poisson}(\lambda) + \text{Poisson}(\mu) = \text{Poisson}(\lambda + \mu)$ when the former terms are independent.
\end{example}
\begin{example}
    We look at $X \sim \text{Geo}(p)$. Denote $1-p$ as $q$. The generating function is given as
    \begin{equation}
        \cP (s) = \sum_{k=0}^{\infty} pq^{k}s^{k} = \frac{p}{1-qs}.
    \end{equation}
    As an extension, let $X_{k}$ denote the number of failures between the $(k-1)^{\text{th}}$ and $k^{\text{th}}$ successes. If we denote $S_{r} = X_{1} + X_{2} + \ldots + X_{r}$, we find that $S_{r} \sim \text{NB}(p,r)$. From direct computation, we know that
    \begin{equation*}
        P(S_{r} = k) = \binom{r+k-1}{k} q^{k} p^{r} \text{ for } k = 0,1,\ldots
    \end{equation*}
    Let us compute this in another way; $S_{r}$ is the sum of independent geomtric random variables with parameter $p$. We have
    \begin{equation}
        \cP^{(S_{r})}(s) = \left( \frac{p}{1-qs} \right)^{r} = p^{r}(1-qs)^{-r} = p^{r} \sum_{k=0}^{\infty} \binom{-r}{k} (-qs)^{k}
    \end{equation}
    which tells us that
    \begin{equation}
        P(S_{r}=k) = p^{r}\binom{-r}{k} (-q)^{k}.
    \end{equation}
\end{example}
\section{Random Walks, with Generating Functions}
Here, we consider the paths that have a right step with probability $p$ and a left step with probability $q = 1-p$.
We first look at the waiting time for the first gain, that is, the event $\{S_{1}\leq 0, S_{2} \leq 0, \ldots, S_{n-1} \leq 0, S_{n} = 1\}$ (Event ($\ast$)). Denote the probability of this event by $\phi_{n}$, and its generating function by $\Phi (s)$. Note that $\phi_{0} = 0$ and $\phi_{1} = p$ lead to trivial cases. We focus on $n > 1$.

We must have $S_{1} = -1$ (Event (1)). Denote, by $\nu < n$, the first return to $0$ (Event (2)). $\nu$ only depends on $X_{0},X_{1},\ldots,X_{\nu}$. We need another $n-\nu$ steps to reach $1$; this depends on $X_{\nu+1},X_{\nu+2},\ldots,X_{n}$ (Event (3)). For some $n > 1$, Event ($\ast$) occurs if and only Event (1) $\cap$ Event (2) $\cap$ Event (3) occurs for some $\nu < n$. The point here is that the three events are independent. For some fixed $\nu < n$,
\begin{equation}
    P(\text{Event (1)}) = q, \; P(\text{Event (2)}) = \phi_{\nu -1}, \; P(\text{Event (3)}) = \phi_{n-\nu}.
\end{equation}
Thus,
\begin{equation}
    \phi_{n} = \sum_{\nu = 2}^{n-1} q \phi_{\nu-1} \phi_{n-\nu}.
\end{equation}
We have
\begin{align}
    \Phi (s) - ps &= \sum_{n=2}^{\infty} \phi_{n}s^{n} = q\sum_{n=2}^{\infty} (\phi_{1}\phi_{n-2} + \ldots + \phi_{n-2}\phi_{1}) s^{n} = qs \sum_{n=1}^{\infty} \phi_{n}^{2 \ast} s^{n} = qs(\Phi(s))^{2} \\
    \implies \Phi (s) - ps &= qs(\Phi(s))^{2}.
\end{align}
This is a standard quadratic; solving gives us
\begin{align}
    \Phi (s) = \dfrac{1 \pm \sqrt{1-4pqs^{2}}}{2qs}.
\end{align}
The solution with the `$+$' is rejected; if it was valid, then plugging in $s < 1$ would give us $\Phi (s) > 1$, which is impossible. We expand this using the binomial theorem,

\begin{align}
    \Phi (s) = \frac{1}{2qs} \left( 1 - \sum_{k=0}^{\infty} \binom{\frac{1}{2}}{k} (-4pqs^{2})^{k}  \right) = \sum_{k=1}^{\infty} \binom{\frac{1}{2}}{k} \frac{(-1)^{k-1} (4pq)^{k}}{2q} s^{2k-1}
\end{align}
which tells us that
\begin{align}
    \phi_{2k-1} = \frac{(-1)^{k-1}}{2q} \binom{\frac{1}{2}}{k} (4pq)^{k}, \; \phi_{2k} = 0.
\end{align}
Thus,
\begin{equation*}
    \Phi (1) = \sum \phi_{n} = \frac{1-\sqrt{1-4pq}}{2q} = \frac{1-\abs{p-q}}{2q} = \begin{cases}
        \frac{p}{q} &\text{ if } p < q,\\
        1 &\text{ if } p \geq q.
    \end{cases}
\end{equation*}
This gives the probability that, at some point of the random walk, the displacement 1 is reached.

Similarly, for displacement $S_{n}$, we have
\begin{equation*}
    P(S_{n} \leq 0 \; \forall n) = \begin{cases}
        \frac{q-p}{p} &\text{ if } p < q,\\
        0 &\text{ if } p \geq q.
    \end{cases}
\end{equation*}
\textit{January 28th.}

Recall that we used $u_{k}$ denote the probability that the random walk returns to zero at step $k$. For unequal left-right step probabilities,
\begin{equation*}
    u_{k} = P(S_{k}=0) = \begin{cases}
        0 &\text{ if $k$ is odd},\\
        \binom{2k}{k} p^{n} q^{n} &\text{ if } k = 2n.
    \end{cases}
\end{equation*}
Thus, the generating function for this is
\begin{align}
    U(s) = \sum_{n=0}^{\infty} u_{2n}s^{2n} = \sum_{n=0}^{\infty} \binom{2n}{n} (pqs^{2})^{n} = \sum_{n=0}^{\infty} \binom{-\frac{1}{2}}{n} (-4pqs^{2})^{n} = \frac{1}{\sqrt{1-4pqs^{2}}}.
\end{align}
Denote, by $f_{2n}$, the probability that the first return to zero occurs at step $2n$, for some $n \geq 1$. In fact, it consists of subevents; if $X_{1} = 1$, denote it by $f_{2n}^{+}$ and if $X_{1} = -1$, denote it by $f_{2n}^{-}$. If we also recall the definition of our $\phi_{n}$,
\begin{align}
    f_{2n}^{-} = P(X_{1}=-1,S_{2}<0,S_{3}<0,\ldots,S_{2n-1}<0,S_{2n}=0) = q \phi_{2n-1}.
\end{align}
The generating function of $\{f_{2n}^{-}\}$ will be given as
\begin{align}
    F^{-}(s) = \sum_{n=1}^{\infty} f_{2n}^{-} s^{2n} = q\sum_{n=1}^{\infty} \phi_{2n-1}s^{2n} = qs \sum_{n=1}^{\infty} \phi_{2n-1} s^{2n-1} = qs \Phi(s) = \frac{1}{2}(1-\sqrt{1-4pqs^{2}}).
\end{align}
It can be shown that $f_{2n}^{+}$ is just $f_{2n}^{-}$ with the probabilities reversed (check!). The generating function of $\{f_{2n}^{+}\}$ is given as
\begin{align}
    F^{+}(s) = \sum_{n=0}^{\infty} f_{2n}^{+}s^{2n} = \frac{1}{2}(1-\sqrt{1-4pqs^{2}}).
\end{align}
Adding both of these, we get
\begin{align}
    F(s) &= F^{+}(s) + F^{-}(s) = 1-\sqrt{1-4pqs^{2}} = 1 - \sum_{n=0}^{\infty} \binom{\frac{1}{2}}{n} (-4pqs^{2})^{n}\\
    \implies f_{2n} &= (-1)^{n+1} \binom{\frac{1}{2}}{n} (4pq)^{n}.
\end{align}
$F(1)$ gives us the probability that walk eventually returns to zero,
\begin{align}
    F(1) = \sum_{n=0}^{\infty} f_{2n} = 1-\sqrt{1-4pq} = 1-\abs{p-q}.
\end{align}
$F'(1)$ gives us the expected time of return to zero,
\begin{align}
    F'(s) = -\frac{1}{2}(1-4pqs^{2})^{-\frac{1}{2}}(-8pqs).
\end{align}
If $p = q = \frac{1}{2}$, then
\begin{equation*}
    F'(1) = \lim_{s \to 1^{-}} F'(s) = \infty.
\end{equation*}
The basic lemma can be proved using the generating functions.

\section{Simple Random Walks in Higher Dimensions}
Consider the walk in the dimension $d$. A walker starts at the origin in the lattice $\Z^{d}$. The random variables $X_{1},X_{2},\ldots$ are independent and identically distributed with probabilities
\begin{equation*}
    P(X_{i} = -e_{d}) + \ldots + P(X_{i} = -e_{2}) + P(X_{i} = -e_{1}) + P(X_{i} = e_{1}) = P(X_{i} = e_{2}) + \ldots + P(X_{i} = e_{d}) = \frac{1}{2d}.
\end{equation*}
for all valid $i$. The random walk here is defined as $S_{n} = X_{1} + \ldots + X_{n}$. We ask the probabilitiy that $S_{n}$ returns to the origin. Denote by $u_{2n}$ the probability that $S_{2n} = 0$, and denote by $f_{2n}$ the probability that the first return to the origin occurs at time $2n$. By conditioning,
\begin{align}
    u_{2n} = \sum_{k=0}^{n} f_{2k} u_{2n-2k}.
\end{align}
If $U(s)$ and $F(s)$ are the appropriate generating functions, then we can show that
\begin{align}
    U(s)-1 = F(s)U(s) \implies U(s) = \frac{1}{1-F(s)}.
\end{align}
Both $U(s)$ and $F(s)$ are covergent for $\abs{s} < 1$. For each $N$,
\begin{equation}
    \sum_{n=0}^{N} u_{2n} \leq \lim_{s \to 1^{-}} U(s) \leq \sum_{n=0}^{\infty} u_{2n}.
\end{equation}

\begin{lemma}
    A random walk on $\Z^{d}$ return to the origin with probability 1 if and only if $\sum u_{2n} = \infty$.
\end{lemma}
\begin{proof}
    Suppose $F(1) < 1$. Then, $\lim{s \to 1^{-}} U(s) < \infty$ and, consequently, $\sum_{n=0}^{\infty} u_{2n} < \infty$. The converse can be proved by reversing the steps.
\end{proof}
The lemma tells us that to see the probability that the random walk returns to the origin, we only need to compute $\sum_{n=0}^{\infty} u_{2n}$.

For $d = 2$, we need the number of $e_{i}$ jumps to be equal to the number of $-e_{i}$ jumps for $i = 1,2$. We have
\begin{align}
    u_{2n} &= \frac{1}{4^{2n}} \sum_{j=0}^{n} \binom{2n}{j} \binom{2n-j}{j} \binom{2n-2j}{n-j} \binom{n-j}{n-j} = \frac{1}{4^{2n}} \binom{2n}{n} \sum_{j=0}^{n} \binom{n}{j}^{2} = \frac{1}{4^{2n}} \binom{2n}{n}^{2} \notag \\
    &\sim \frac{2}{2\pi} \frac{n^{4n+1}}{n^{4n+2}} = \frac{1}{\pi n}.
\end{align}
Since this is any asymptotic relationship, $u_{2n} \geq \frac{(1-\varepsilon)}{\pi n}$ for large $n$. Thus, we can show $\sum u_{2n} = \infty$.

For $d = 3$,
\begin{align}
    u_{2n} &= \frac{1}{6^{2n}} \sum_{j,k = 0; j+k \leq n}^{n} \frac{(2n)!}{j! j! k! k! (n-j-k)! (n-j-k)!} = \frac{1}{6^{2n}} \sum_{j,k=0l j+k \leq n}^{\infty} \frac{(2n)!}{(j!)^{2}(k!)^{2}((n-j-k)!)^{2}} \notag \\
    &= \frac{1}{2^{2n}} \binom{2n}{n} \sum_{j,k;j+k \leq n} \left( \frac{n!}{j!k!(n-j-k)!} \frac{1}{3^{n}} \right)^{2}.
\end{align}
$\frac{1}{2^{2n}} \binom{2n}{n}$ behaves asymptotically as $\frac{1}{\sqrt{\pi n}}$. For the rest of the term,
\begin{align}
    \sum_{j,k; j+k \leq n} \left( \frac{n!}{j!k!(n-j-k)!} \frac{1}{3^{n}} \right)^{2} \leq t_{n} \sum_{j,k;j+k \leq n} \frac{n!}{j!k!(n-j-k)!} \frac{1}{3^{n}}
\end{align}
where $t_{n} = \max_{j,k;j+k \leq n} \frac{n!}{j!k!(n-j-k)!}$. The maximum is attained roughly when $j,k \approx \frac{n}{3}$. Also, the summation behaving as the upper bound is just unity. Thus,
\begin{align}
    \sum_{j,k; j+k \leq n} \left( \frac{n!}{j!k!(n-j-k)!} \frac{1}{3^{n}} \right)^{2} \leq t_{n} \approx \frac{n!}{((\frac{n}{3})!)^{3} 3^{n}} \sim \frac{C}{n}
\end{align}
for some constant $C$. Therefore,
\begin{equation}
    u_{2n} \leq \frac{C^{\ast}}{n^{\frac{3}{2}}} \implies \sum u_{2n} < \infty \implies F(1) < 1.
\end{equation}

\begin{theorem}[\eax{Polya}]
    A random walk in 1 or 2 dimensions will always return to the origin with probability 1. A random walk in more than 2 dimensions has a positive probability of never returning to the origin.
\end{theorem}

\begin{appendices}

\titleformat{\chapter}[display]
    {\normalfont\Large\bfseries}
    {\chaptername\ \thechapter}{20pt}{\Huge}

\titlespacing*{\chapter}{0pt}{20pt}{40pt}

\chapter{Appendix}
Extra content goes here.

\printindex

\end{appendices}

\end{document}
