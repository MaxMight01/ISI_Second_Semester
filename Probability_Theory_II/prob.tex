
\documentclass[15pt,a4paper]{book}

\usepackage{amsmath, amsthm, amssymb} 
\usepackage{graphicx} % For including graphics
\usepackage{hyperref} % For clickable links
\usepackage{bookmark} % Better control over bookmarks
\usepackage{geometry} % Customize page layout
\usepackage{xcolor} % Colors for text and graphics
\usepackage{enumitem} % Customizable lists
\usepackage{fancyhdr} % Header and footer
\usepackage{titlesec} % Custom section/chapter titles
\usepackage[toc,page]{appendix} % For the appendix
\usepackage{longtable} % For tables spanning multiple pages
\usepackage{mathrsfs} % For script fonts in math mode
\usepackage{tocloft} % Custom table of contents
\usepackage{datetime2} % For dates
\usepackage{caption} % For better control over captions
\usepackage{float} % Fine control over figure/table placement
\usepackage{imakeidx} % For index
\usepackage{mdframed}% For middle boxes styling
\usepackage{tcolorbox} % For colored boxes

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\cftchapfont}{\normalfont} % Remove bold for chapter names
\renewcommand{\cftchappagefont}{\normalfont} % Remove bold for chapter page numbers
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\eax}[1]{\emph{#1}\index{#1}} % Macro for emphasis and index
\newcommand{\abs}[1]{\left| #1 \right|} % Absolute value

\tcbset {
  base/.style={
    arc=0mm, 
    bottomtitle=0.5mm,
    boxrule=0mm,
    colbacktitle=black!10!white, 
    coltitle=blue!70!red, 
    fonttitle=\bfseries, 
    left=2.5mm,
    leftrule=1mm,
    right=3.5mm,
    title={#1},
    toptitle=0.75mm, 
  }
}

\definecolor{brandblue}{rgb}{0.7, 0.6, 1}
\newtcolorbox{mainbox}[1]{
  colframe=brandblue, 
  base={#1}
}

\newtcolorbox{subbox}[1]{
  colframe=black!10!white,
  base={#1}
}                  !% Custom Theorem Styles
                  % Call by \begin{mainbox}{Title} ... \end{mainbox}


% Custom Notation List Environment
\newlist{notationlist}{description}{1}
\setlist[notationlist]{font=\bfseries,labelsep=1em}

% Geometry Settings
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
}

% Hyperref Colors
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    citecolor=red
}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}

% Custom Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Chapter name on top left
\fancyhead[R]{\rightmark}  % section name on top right
\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Making index
\makeindex[intoc]

% Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries \centering}
  {\chaptername\ \thechapter}{20pt}{\Huge \centering}

\titlespacing*{\chapter}{0pt}{20pt}{100pt}

\begin{document}

\pagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vspace*{\fill}
    % Title in all caps
    {\Huge \textbf{\MakeUppercase{Probability Theory II}}\par}

    \vspace{0.5cm} % Adjust vertical spacing between title and subtitle
    % Subtitle in normal text, slightly enlarged
    {\Large Matthew Joseph, notes by Ramdas Singh\par}

    \vspace{0.5cm} % Additional spacing before the author
    % Author information
    {\large Second Semester\par}
    \vspace*{\fill}
    \end{center}
\end{titlepage}

\clearpage

\pagenumbering{roman}

\chapter*{List of Symbols}
\begin{notationlist}
    \item $\Omega$, a sample space.
    \item $\omega$, an element of a sample space.
    \item $EX$, the expectation of the random variable $X$.
    \item $\text{Var}X$, the variance of the random variable $X$.
    \item $N(\mu,\sigma^{2})$, a normal distribution with expectation $\mu$ and variance $\sigma^{2}$.
\end{notationlist}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
\pagenumbering{arabic}
\pagestyle{fancy}


%%-------------------------------------------------------------------------------------------------


\chapter{}


\textit{January 3rd.}

We first start with some initial statements.
Let $\Omega$ be a countable state space, and let each $\omega \in \Omega$ have a probabiltiy $P(\omega)$ associated with it.

\begin{lemma}
    For random variables $X,Y$ such that $X(\omega) \leq Y(\omega)$ for all $\omega \in \Omega$. Then, $EX \leq EY$.
\end{lemma}
\begin{proof}
    This can easily be seen by summing over all terms via the alternate definition of the expectation,
    \begin{equation}
        EX = \sum_{\omega \in \Omega} X(\omega) P(\omega) \leq \sum_{\omega \in \Omega} Y(\omega) P(\omega) = EY.
    \end{equation}
\end{proof}

We now state Markov's inequality. 

\begin{theorem}[\eax{Markov's inequality}]
    If $X$ is a non-negative randm variable, then for $a > 0$, we have
    \begin{equation}
        P(X > a) \leq \frac{EX}{a}.
    \end{equation}
\end{theorem}
\begin{proof}
    Define an indicator function $I_{a}(\omega)$ as 1 if $X(\omega) \geq a$, and 0 if otherwise. We then have
    \begin{align}
        I_{a}(\omega) &\leq \frac{X(\omega)}{a} \implies P(X \geq a) = EI_{a} \leq \frac{1}{a} EX.
    \end{align}
\end{proof}
\begin{remark}
    A better upper bound here may be found by starting with $I_{a}(\omega)X(\omega)$ instead of just $X(\omega)$.
\end{remark}
If we have $X \sim N(0,1)$, then we can find an upper bound for its probability density function.
\begin{equation}
    P(X > a) = \int_{a}^{\infty} \frac{1}{\sqrt{2\pi}} e^{\frac{-x^{2}}{2}} dx \leq \int_{a}^{\infty} \frac{1}{\sqrt{2\pi}} \frac{x}{a} e^{\frac{-x^{2}}{2}} dx = \frac{e^{\frac{-a^{2}}{2}}}{\sqrt{2\pi}a}.
\end{equation}

Note that $X$ here is a random variable over a continuous state space; the previous lemma and Markov's inequality also work here. We are to show them for the continuous case instead of the discrete one.

\begin{proof}
    Here, we have $0 \leq X(\omega) \leq Y(\omega)$ for all $\omega$ in our continuous state space $\Omega$. We see that $\{X > x\} \subseteq \{Y > x\} \implies P(X > x) \leq P(Y > x)$. Integrating both sides gives us $EX \leq EY$.
\end{proof}

\begin{theorem}[\eax{Chebyshev's inequality}]
    Let $X$ be a random variable with finite mean $\mu = EX$ and finite variance $\sigma^{2} = \text{Var}(X)$. Then for $a > 0$, 
    \begin{equation}
        P(\abs{X-\mu}>a) \leq \frac{\text{Var}(X)}{a^{2}}.
    \end{equation}
\end{theorem}
\begin{proof}
    Start with the proof of Markov's inequality, replacing the indiciator function with one that's unity when $\abs{X-\mu} \geq a$.
\end{proof}
\begin{example}
    Suppose $X_{1}, X_{2}, \ldots, X_{n}$ are $n$ independent and identically distributed random variables, with $EX_{i} = \mu$ and $\text{Var}X_{i} = \sigma^{2}$. If $S_{n} = \sum X_{i}$, we then have
    \begin{equation}
        P(\abs{S_{n}-n\mu} > a) \leq \frac{\text{Var}S_{n}}{a^{2}} = \frac{n\sigma^{2}}{a^{2}}.
    \end{equation}
    If we replace $a$ with $n^{\frac{1}{2}+\varepsilon}$, we then have
    \begin{equation}
        P(\abs{S_{n}-n\mu} > n^{\frac{1}{2}+\varepsilon}) \leq \frac{\sigma^{2}}{n^{2 \varepsilon}} \to 0 \text{ as } n \to \infty.
    \end{equation}
\end{example}
\begin{proposition}
    If $\text{Var}(X) = 0$, then $P(X=EX) = 1$.
\end{proposition}
\begin{proof}
    For all $\varepsilon > 0$, we have
    \begin{equation}
        P(\abs{X-EX} > \varepsilon) \leq \frac{\text{Var}X}{\varepsilon^{2}} = 0.
    \end{equation}
    Define $A_{n}$ as $\{\abs{X-EX} > \frac{1}{n}\}$. Taking $P(\bigcup A_{n}) = \lim_{n \to \infty} P(A_{n})$, the proof follows.
\end{proof}

\section{The Law of Large Numbers}

We start by stating the weak law of large numbers.
\begin{theorem}[\eax{Weak law of large numbers}]
    Let $\{X_{k}\}_{k \geq 1}$ be a sequence of independent and identically distributed random variables with $E\abs{X_{i}} < \infty$. Let $\mu = EX_{i}$. Then for any $a > 0$,
    \begin{equation}
        \lim_{n \to \infty} P\left(\abs{\frac{X_{1}+X_{2}+\ldots+X_{n}}{n} - \mu} > a \right) = 0.
    \end{equation} 
\end{theorem}
\begin{proof}
    For now, let us assume that $\Omega$ is countable. We begin with the case where the variance of $X_{i}$, $\sigma^{2}$, is finite. Fix $a > 0$, and let $S_{n} = X_{1} + X_{2} + \ldots + X_{n}$. Then,
    \begin{equation}
        P\left(\abs{\frac{S_{n}}{n} - \mu} > a\right) = P(\abs{S_{n}-n\mu} > na) \leq \frac{\text{Var}S_{n}}{n^{2}a^{2}} = \frac{n\sigma^{2}}{n^{2}a^{2}} \to 0 \text{ as } n \to \infty.
    \end{equation}
    We now focus the case when the variance, $\sigma^{2}$, is infinite. Assume that the expected value, $\mu$, is 0; if it were non-zero, we would then instead work with $X_{i}-\mu$. Let $\delta > 0$; we shall choose a particular $\delta$ later. For each $n$, define $n$ pairs of random variables, $U_{1},V_{1}, \ldots, U_{n},V_{n}$, as $U_{k} = X_{k}, V_{k} = 0$ if $\abs{X_{k}} \leq \delta n$, and $U_{k} = 0, V_{k} = X_{k}$ if $\abs{X_{k}} > \delta n$. $X_{k}$ can be rewritten as $U_{k} + V_{k}$. We then have
    \begin{align}
        \{\abs{X_{1} + \ldots + X_{n}} \geq na\} &\subseteq \{\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\} \cup \{\abs{V_{1} + \ldots + V_{n}} \geq \frac{na}{2}\} \\
        \implies P\left(\abs{X_{1} + \ldots + X_{n}} \geq na\right) &\leq P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) + P\left(\abs{V_{1} + \ldots + V_{n}} \geq \frac{na}{2}\right).
    \end{align}

    \begin{mainbox}
        {Why should we use $a_n$ instead of $\delta n$?}
        >> We must use $a_n$ because we don't have any idea about the growth rate that is needed and that is what we address here...
        \end{mainbox}
        \vspace{1em}
        \begin{tcolorbox}[title=Analysis on $a_n$...,colback=blue!2!white,colframe=blue!55!black]
        As $M=\int_{\mathbb{R}}|x| d F(x)$ is finite the integral tails must decay to zero. Thus, for any $\zeta>0$, we must have $\int_{|x|>a_{n}}|x| \operatorname{dF}(x)<\zeta$ as $n \rightarrow \infty$. It follows that
        
        \[\PP\left\{\left|X_{j}\right|>a_{n}\right\}=\int_{|x|>a_{n}} d F(x) \leq \int_{|x|>a_{n}} \frac{|x|}{a_{n}} d F(x)<\frac{\zeta}{a_{n}},
        \]
        eventually, for all sufficiently large $n$.We begin with the second of the terms on the right. As the occurrence of the event $\left\lvert\mathrm{V}_{1}+\cdots+\mathrm{V}_{\mathrm{n}}\right\rvert \geq \frac{1}{2} \epsilon \mathrm{n}$ certainly implies that at least one of the $V_{j}$ is non-zero, Boole's inequality implies
        \[\PP\left\{\left|\mathrm{V}_{1}+\cdots+\mathrm{V}_{n}\right| \geq \frac{1}{2} \epsilon n\right\} \leq \PP\left(\bigcup_{j=1}^{n}\left\{\mathrm{V}_{j} \neq 0\right\}\right) \leq \sum_{j=1}^{n} \PP\left\{\left|X_{j}\right|>a_{n}\right\}<\frac{\zeta n}{a_{n}}\]
        For the bound to be useful we conclude that $a_{n}$ must have a growth rate at least of the order of $n$.

        If we set $\mu_{n}=\EE\left(U_{j}\right)=\int_{|x| \leq a_{n}} x\; dF(x)$ then, 
$$
\left|\mu_{n}-\mu\right|=\left|\int_{|x|>a_{n}} x \;dF(x)\right| \leq \int_{|x|>a_{n}}|x| \;dF(x)<\zeta \quad .
$$

We can afford to be more cavalier in estimating the variance. As $x^{2} \leq a_{n}|x|$ if $|x| \leq a_{n}$, by monotonicity of expectation we have

\[
Var\left(\mathrm{U}_{j}\right) \leq \EE\left(\mathrm{U}_{j}^{2}\right)=\int_{|x| \leq a_{n}} x^{2} \;dF(x) \leq a_n \int_{|x| \leq a_{n}}|x| \;dF(x) \leq a_n \int_{-\infty}^{\infty}|x| \; dF(x)=M a_{n}
\]


The pieces are in place for an application of Chebyshev's inequality. We first centre the sum appropriately by a routine application of the triangle inequality,For any $0<\zeta<\frac{\epsilon}{4}$, Chebyshev's inequality now yields
$$
\begin{aligned}
    & \PP\left\{\left|\mathrm{U}_{1}+\cdots+\mathrm{U}_{n}-\mu n\right| \geq \frac{1}{2} \epsilon{n}\right\} \leq \PP\left\{\left|\mathrm{U}_{1}+\cdots+\mathrm{U}_{n}-\mu_{n} n\right| \geq \frac{1}{2} \epsilon{n}-\left|\mu_{n}-\mu\right| n\right\} \\ & \quad \leq \PP\left\{\left|\mathrm{U}_{1}+\cdots+\mathrm{U}_{n}-\mu_{n} n\right| \geq \frac{1}{2}(\epsilon-2 \zeta) n\right\} \leq \frac{4 M n a_{n}}{(\epsilon-2 \zeta)^{2} n^{2}} \leq \frac{16 M a_{n}}{\epsilon^{2} n} .
\end{aligned}
$$
The bound will diverge if $a_{n}$ has a growth rate faster than $n$ and so $a_{n}$ cannot increase faster than the order of $n$. From our two bounds we conclude that, if the approach is to be useful at all, $a_{n}$ must be chosen to be exactly of order $n$.Accordingly, set $a_{n}={n\delta}$ with $\delta$ to be specified.

 Pooling our bounds, we obtain
$$
\mathbf{P}\left\{\left|\frac{1}{n} S_{n}-\mu\right| \geq \epsilon\right\}<\frac{16 M a_{n}}{\epsilon^{2} n}+\frac{\zeta n}{a_{n}}=\frac{16 M\delta}{\epsilon^{2}}+\frac{\zeta}{\delta} .
$$

The parameters $\delta$ and $\zeta$ may be chosen at our discretion and we select them to make the right-hand side small.
        \end{tcolorbox}
        \vspace{1em}
    We focus on the first term on the right hand side. The $U_{i}$'s are independently and identically distributed, so
    \begin{align}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4E[\abs{U_{1}+\ldots+U_{n}}^{2}]}{a^{2}n^{2}} = \frac{4}{a^{2}n^{2}} \left( \text{Var}(U_{1} + \ldots + U_{n}) + (nEU_{i})^{2} \right).
    \end{align}
    For the variance, we have
    \begin{equation}
        \text{Var}(U_{1}+\ldots+U_{n}) = n \text{Var}U_{i} \leq n EU_{i}^{2} \leq n E[\abs{U_{i}}\abs{U_{i}}] \leq \delta n^{2} E[\abs{U_{i}}]
    \end{equation}
    which transforms the previous equation as
    \begin{equation}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4}{a^{2}n^{2}} \left( \delta n^{2} E[\abs{U_{i}}] + (nEU_{i})^{2} \right).
    \end{equation}
    A lemma (to be proven later) states that $E[\abs{U_{i}}] = E[\abs{X_{i}}]$ as $n \to \infty$, and $EU_{i} = EX_{i} = 0$ too. So,
    \begin{equation}
        P\left(\abs{U_{1} + \ldots + U_{n}} \geq \frac{na}{2}\right) \leq \frac{4}{a^{2}n^{2}} \left( \delta n^{2} E[\abs{U_{i}}] + (nEU_{i})^{2} \right) \leq \frac{4 \delta E[\abs{U_{i}}]}{a^{2}} + \frac{4}{a^{2}} (EU_{i})^{2}.
    \end{equation}

    For the second term on the right hand side, begin with
    \begin{align}
        P(V_{1}+ \ldots + V_{n} \neq 0) &\leq P(\{V_{1} \neq 0\} \cup \ldots \cup \{V_{n} \neq 0\}) \leq nP(V_{i} \neq 0) = n \sum_{\abs{x} > \delta n} P(X_{i} = x) \notag \\
        &\leq n \sum_{\abs{x} > \delta n} \frac{\abs{x}}{\delta n} P(X_{i} = x) = \frac{1}{\delta} E[\abs{V_{i}}]. 
    \end{align}
    The rightmost term here tends to 0 as $n \to \infty$. Now choose $\delta$ to be $\frac{\varepsilon a^{2}}{\abs{6 E{\abs{X_{i}}}}}$, and then choose $N$ to be large enough such that for all $n > N$, both the terms are smaller than $\frac{\varepsilon}{2}$.
\end{proof}
\vspace{1em}

\textit{January 7th.}

We now prove the lemma called upon earlier.
\begin{lemma}
    If $X$ is a discrete random variable and takes values $y_{1},y_{2},\ldots,y_{k}$, and $E[\abs{X}] < \infty$, then $\lim_{n \to \infty} E[\abs{X}1_{\abs{X} \leq n}] = E[\abs{X}]$.
\end{lemma}
\begin{proof}
    Notice that the terms on the left hand side and right hand side are $\sum_{y_{k} : \abs{y_{k}} \leq n}$ and $\sum_{y_{k}} \abs{y_{k}} P(Y = y_{k})$. The condition for convergence may now be applied.
\end{proof}
The above equation, begin inside absolute braces, must imply that the term $E[X \cdot 1_{\abs{X} \leq n}]$ must also absolutely converge to $EX$.

\section{Simple Random Walk}
Let $X_{1},X_{2},\ldots$ be independent and identically distributed random variables, with $X_{i} = 1$ with probability $\frac{1}{2}$ and $X_{i}=-1$ with probability $\frac{1}{2}$. Now define $S_{0}=0$ and $S_{n} = \sum_{i=1}^{n} X_{i}$. The sequence $(S_{n})_{n \geq 0}$ is a \eax{simple random walk}.

Note that $S_{0}=k_{0}=0,S_{1}=k_{1},\ldots,S_{n}=k_{n}$ can occur if and only if $\abs{k_{i}-k_{i+1}} = 1$ for all $0 \leq i \leq n-1$. The sequence $(k_{n})_{n \geq 0}$ is a \eax{simple path} of the simple random walk. By the event $\{S_{n}=k\}$, we are concerned with the event that the random walk visits $k$ at step $n$. If $(k_{n})_{n \geq 0}$ is given we have $X_{i} = k_{i}-k_{i-1}$. Because the $X_{i}$'s are independent and identically distributed, each event $\{X_{1}=l_{1},X_{2}=l_{2},\ldots,X_{n}=l_{n}\}$, where $l_{i} = \pm 1$, is equally likely with probability $\frac{1}{2^{n}}$. Thus,
\begin{equation}
    P(S_{n}=k) = \frac{N_{n}(k)}{2^{n}}
\end{equation}
where $N_{n}(k)$ is defined as the number of distinct of path that start at $0$ and end at $k$ at step $n$. We also define $N_{n}^{+}(k)$ to be the number of distinct paths that end at $k$ at step $n$ and stay above the $x$-axis up to time $n-1$. The probability of the corresponding event is
\begin{equation}
    P(\{S_{1} > 0, S_{2} > 0, \ldots S_{n-1} > 0, S_{n} = k\}) = \frac{N_{n}^{+}(k)}{2^{n}}.
\end{equation}

\begin{lemma}
    Suppose $a,a',b,b'$ are integers, with $0 \leq a < a'$. Then the number of distinct path from $(a,b)$ to $(a',b')$ depends only on $a'-a = n$ and $b'-b = k$, and is given by $\binom{n}{\frac{n+k}{2}}$.
\end{lemma}
\begin{proof}
    Notice that we need $x$ $+1$'s and $y$ $-1$'s to appear, satisfying $x+y = a'-a$ and $x-y = b'-b$. Solving, we get $x = \frac{n+k}{2}$ and $y = \frac{n-k}{2}$. Thus, the number of paths is given by $\binom{n}{\frac{n+k}{2}}$.
\end{proof}
Using this lemma, we find that $N_{n}(k) = \binom{n}{\frac{n+k}{2}}$. The following convention is now followed; if $t$ is not an integer, then $\binom{n}{t} = 0$.

\begin{lemma}[The \eax{method of images}]
    Suppose $a,a',b,b'$ are integers, with $0 \leq a < a'$ and $b,b'>0$. Then the number of distinct paths from $(a,b)$ to $(a',b')$ that intersect the $x$-axis is equal to the number of paths from $(a,-b)$ to $(a',b')$.
\end{lemma}
\begin{proof}
    Consider any path $(b=k_{0},k_{1},\ldots,k_{n-1},k_{n}=b')$, from $(a,b)$ to $(a',b')$, that intersects the $x$-axis. Let $j$ be the smallest index for which $k_{j}=0$. For ease, denote $(a,b)$ by $A$, $(a',b')$ by $A'$, $(a+j,0)$ by $B$, and $(a,-b)$ by $A''$. Reflect the segment from $A$ to $B$ about the $x$-axis to obtain a `mirrored-path' from $A''$ to $B$; $(-b=-k_{0},-k_{1},\ldots,-k_{j-1},k_{j}=0,k_{j+1},\ldots,k_{n}=b')$. There is now a one-to-one correspondence between the paths from $A$ to $A'$ that intersect the $x$-axis, and the paths from $A''$ to $A'$.
\end{proof}
We can now easily compute $N_{n}^{+}(k)$; it simply the number of paths from $(1,1)$ to $(n,k)$ that do not intersect the $x$-axis.
\begin{theorem}[\eax{Ballot theorem}]
    The number of paths that progress from $(0,0)$ to $(n,k)$ through strictly positive values is given by $N_{n}^{+}(k) = \frac{k}{n}N_{n}(k)$.
\end{theorem}
\begin{proof}
    We have
    \begin{align}
        N_{n}^{+}(k) &= \text{ number of paths from $(1,1)$ to $(n,k)$} - \text{ number of such paths that intersect the $x$-axis} \notag \\
        &= N_{n-1}(k-1) - N_{n-1}(k+1) \notag \\
        &= \binom{n-1}{\frac{n+k}{2}-1} - \binom{n-1}{\frac{n+k}{2}} = \frac{k}{n}\binom{n}{\frac{n+k}{2}} = \frac{k}{n}N_{n}(k).
    \end{align}
\end{proof}
Suppose $n = 2\nu$. Define $u_{2\nu}$ to be $P(S_{2\nu}=0) = \frac{\binom{2\nu}{\nu}}{2^{n}}$. The question we ask is to compute the probability that the first return to 0, if at all, occurs after step $n$. It can be found out as
\begin{align}
    P(\text{first return to $0$...}) &= P(S_{1}\neq 0, S_{2} \neq 0, \ldots, S_{2\nu} \neq 0) \\
    &= P(S_{1}>0,\ldots,S_{2\nu}>0) + P(S_{1}<0,\ldots,S_{2\nu}<0) \notag \\
    &= 2P(S_{1}>0,\ldots,S_{2\nu}>0) \notag \\
    &= 2 \sum_{k \text{ even}, k>0} P(S_{1}>0,\ldots,S_{2\nu-1}>0,S_{2\nu}=k) \notag \\
    &= \frac{2}{2^{2\nu}} \sum_{k \text{ even}, k>0} N_{2\nu}^{+}(k) \notag \\
    &= \frac{2}{2^{2\nu}} \sum_{k \text{ even}, k>0} N_{2\nu-1}(k-1)-N_{2\nu-1}(k+1) \notag \\
    &= \frac{2}{2^{2\nu}} N_{2\nu-1}(1) = u_{2\nu}.
\end{align}
We state this down as a lemma.
\begin{lemma}[\eax{Basic lemma}]
    For $n$ even, the probability that the first return to 0, if at all, occurs after step $n$ is the same as the probability that the location at step $n$ is 0. For $n$ odd, it is the probability that the location at step $n-1$ is 0.
\end{lemma}
We ask another question; for a fixed $n$, where does the random walk achieve its first maximum upto time $n$? For this, denote by $M_{n}$ the index $m$ at which the walk $S_{0},S_{1},\ldots,S_{n}$, over $n$ steps, achieves its maximum for the first time.

For $0<m<n$, $M_{n}=m$ if and only if $S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}$ and $S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n}$. Notice that the first of these two conditions depends only on $X_{1},X_{2},\ldots,X_{m}$, and the second condition depends only on $X_{m+1},X_{m+2},\ldots,X_{n}$. So, $P(M_{n}=m) = P(S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}) \cdot P(S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n})$.

The key idea here is to consider the \eax{reversed walk}; define a new walk with $X_{1}'=X_{m},\; X_{2}'=X_{m-1},\ldots,X_{m}'=X_{1}$. Also define $S_{k}' = X_{1}' + \ldots + X_{k}'$. From here, we can deduce that $S_{m}>S_{m-i}$ is true if and only if $X_{m}+\ldots+X_{m-i}$ is true, which is true if and only if $S_{i}'>0$ is true. So, $P(S_{m}>S_{0},\; S_{m}>S_{1},\ldots,S_{m}>S_{m-1}) = P(S_{1}'>0,\; S_{2}'>0, \ldots, S_{m}'>0)$. If we now define $S_{k}''=X_{m+1}+\ldots+X_{m+k}$, we have
\begin{align*}
    P(S_{m} \geq S_{m+1},\; S_{m} \geq S_{m+2}, \ldots, S_{m} \geq S_{n}) &= P(X_{m+1}\leq 0,\; X_{m+1}+X_{m+2}\leq 0, \ldots, X_{m+1}+\ldots+X_{n}\leq 0) \\
    &= P(S_{1}''\leq 0,\; S_{2}''\leq 0,\ldots,S_{n-m}''\leq 0) \\
    &= P(S_{1}''\geq 0,\; S_{2}''\geq 0,\ldots,S_{n-m}''\geq 0)
\end{align*}
The first of the terms discussed, $P(S_{1}'>0,\; S_{2}'>0, \ldots, S_{m}'>0)$, can be computed for $m = 2\nu, 2\nu+1$; it is simply $\frac{1}{2}u_{2\nu}$. For the latter of these terms, we introduce a new random variable $\tilde{X}$ which has the same distribution as the $X_{i}$'s and is independent. Also define $\tilde{S_{i}}$ to be $\tilde{X}+X_{1}+\ldots+X_{i-1}$ and $\tilde{S_{0}}$ to be $0$. We then have
\begin{align}
    \frac{1}{2}P(S_{0} \geq 0, \ldots, S_{n-m} \geq 0) &= P(\tilde{X}=1) \cdot P(S_{0} \geq 0, \ldots, S_{n-m} \geq 0) \\
    &= P(\tilde{X}=1,S_{0} \geq 0, S_{0} \geq 0, \ldots, S_{n-m} \geq 0) \\
    &= P(\tilde{S_{1}}=1,\tilde{S_{2}}>0,\ldots,\tilde{S_{n-m+1}}>0) \\
    &= P(S_{1}>0,S_{2}>0,\ldots,S_{n-m+1}>0).
\end{align}
Thus, we get
\begin{equation}
    P(M_{n}=m) = \frac{1}{2} u_{2k}u_{2\nu-2k}
\end{equation}
where $m$ is of the form $2k$ or $2k+1$, and $n$ is of the form $2\nu$.




\begin{appendices}

\titleformat{\chapter}[display]
    {\normalfont\Large\bfseries}
    {\chaptername\ \thechapter}{20pt}{\Huge}

\titlespacing*{\chapter}{0pt}{20pt}{40pt}

\chapter{Appendix}
Extra content goes here.

\printindex

\end{appendices}

\end{document}