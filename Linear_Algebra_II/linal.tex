
\documentclass[15pt,a4paper]{book}

\usepackage{amsmath, amsthm, amssymb} 
\usepackage{graphicx} % For including graphics
\usepackage{hyperref} % For clickable links
\usepackage{bookmark} % Better control over bookmarks
\usepackage{geometry} % Customize page layout
\usepackage{xcolor} % Colors for text and graphics
\usepackage{enumitem} % Customizable lists
\usepackage{fancyhdr} % Header and footer
\usepackage{titlesec} % Custom section/chapter titles
\usepackage[toc,page]{appendix} % For the appendix
\usepackage{longtable} % For tables spanning multiple pages
\usepackage{mathrsfs} % For script fonts in math mode
\usepackage{tocloft} % Custom table of contents
\usepackage{datetime2} % For dates
\usepackage{caption} % For better control over captions
\usepackage{float} % Fine control over figure/table placement
\usepackage{imakeidx} % For index
\usepackage{afterpage} % For blank page

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\cftchapfont}{\normalfont} % Remove bold for chapter names
\renewcommand{\cftchappagefont}{\normalfont} % Remove bold for chapter page numbers
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\eax}[1]{\emph{#1}\index{#1}} % Macro for emphasis and index
\newcommand{\abs}[1]{\left| #1 \right|} % Absolute value
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tr}{\text{tr}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\adj}{\text{adj}}
\newcommand{\ip}[1]{\langle #1 \rangle}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\nullsp}{\text{nullsp}}
\newcommand{\rowsp}{\text{rowsp}}
\newcommand{\colsp}{\text{colsp}}

% Custom Notation List Environment
\newlist{notationlist}{description}{1}
\setlist[notationlist]{font=\bfseries,labelsep=1em}

% Geometry Settings
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
}

% Hyperref Colors
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    citecolor=red
}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}

% Custom Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Chapter name on top left
\fancyhead[R]{\rightmark}  % section name on top right
\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Making index
\makeindex[intoc]

% Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries \centering}
  {\chaptername\ \thechapter}{20pt}{\Huge \centering}

\titlespacing*{\chapter}{0pt}{20pt}{100pt}

\begin{document}

\pagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vspace*{\fill}
    % Title in all caps
    {\Huge \textbf{\MakeUppercase{Linear Algebra II}}\par}

    \vspace{0.5cm} % Adjust vertical spacing between title and subtitle
    % Subtitle in normal text, slightly enlarged
    {\Large Anita Naolekar, notes by Ramdas Singh\par}

    \vspace{0.5cm} % Additional spacing before the author
    % Author information
    {\large Second Semester\par}
    \vspace*{\fill}
    \end{center}
\end{titlepage}

\clearpage


\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
\pagenumbering{arabic}
\pagestyle{fancy}


%%-------------------------------------------------------------------------------------------------


\chapter{PERMUTATION GROUPS}


\textit{January 3rd.}

Let $S_{n}$ denote the set of all bijections (permutations) on the set $\{1,2,\ldots,n\}$. If $\sigma, \tau \in S_{n}$, let us define $\sigma \tau$ to be the bijection defined as
\begin{equation}
    (\sigma \tau)(i) = \sigma(\tau(i)) \forall 1 \leq i \leq n.
\end{equation}

This gives us a binary operation on $S_{n}$ which is associative, and $S_{n}$ will then contain the identity permutation 1 such that $\sigma 1 = 1 \sigma = \sigma$ for all $\sigma \in S_{n}$. For every such $\sigma$, we can also find a $\sigma^{-1} \in S_{n}$ such that $\sigma \sigma^{-1} = \sigma^{-1}\sigma = 1$. The set $S_{n}$ equipped with this binary operation, thus, forms a group. In this case, we call $S_{n}$ as the \eax{symmetric group} of degree $n$. We now define a cycle in regards to permutations.

\begin{definition}
    A \eax{cycle} is a a string of positive integers, say $(i_{1},i_{2},\ldots,i_{k})$, which represents the permutation $\sigma \in S_{n}$ (with $k\leq n$) such that $\sigma(i_{j})=i_{j+1}$ for all $1 \leq j \leq k-1$, and $\sigma(i_{k}) = i_{1}$, and fixes all other integers. 
\end{definition}

We also note that $S_{3}$ is the smallest Abelian group possible, upto isomorphism. $S_{3}$ is one of the only two groups of order 6, and can be written as
\begin{equation}
    S_{3} = \{1, \sigma=(1,2,3), \sigma^{2}=(1,3,2), \tau=(1,2),\sigma\tau=(1,3),\tau\sigma=(2,3)\}.
\end{equation}
Some other observations arise. We find that $\sigma^{3} = \tau^{2} = 1$, and that $\tau\sigma = \sigma^{2}\tau$. We notice another fact via this $\sigma$;

\begin{remark}
    A \eax{k-cycle} $\sigma=(i_{1},i_{2},\ldots,i_{k})$ is of order $k$, that is, $\sigma^{k}=1$.
\end{remark}

\begin{definition}
    Two cycles in $S_{n}$ are called disjoint if they have no intger in common.
\end{definition}
We note that if $\sigma$ and $\tau$ are two disjoint cycles in $S_{n}$ then $\sigma$ and $\tau$ commute, that is, $\sigma \tau = \tau \sigma$.

\begin{proposition}
    Every $\sigma$ in $S_{n}$ can be written uniquely as a product of disjoint cycles.
\end{proposition}
Every cycle can be written as a product of 2-cycles. 2-cycles are called \eax{transpositions}. This can easily be seen as
\begin{equation}
    (a_{1},a_{2},\ldots,a_{n})=(a_{1},a_{n})(a_{1},a_{n-1})\cdots(a_{1},a_{3})(a_{1},a_{2}).
\end{equation}

\section{Even and Odd Permutations}
Let $x_{1},x_{2},\ldots,x_{n}$ be indeterminates, and let
\begin{equation}
    \Delta = \prod_{1 \leq i < j \leq n} (x_{i}-x_{j}).
\end{equation}
Let $\sigma \in S_{n}$, and define 
\begin{equation}
    \sigma(\Delta) = \prod_{1 \leq i < j \leq n} (x_{\sigma(i)}-x_{\sigma(j)}).
\end{equation}
We find that $\sigma(\Delta) = \pm \Delta$. Based on this, we classify permutations as odd or even.

\begin{definition}
    A permutation $\sigma$ is said to be an \eax{even permutation} if $\sigma(\Delta) = \Delta$, and is said to be an \eax{odd permutation} if $\sigma(\Delta) = -\Delta$. The sign of a permutation $\sigma$, denoted by $\epsilon(\sigma)$, is $+1$ if $\sigma$ is even, and is $-1$ if $\sigma$ is odd. So, $\sigma(\Delta) = \epsilon(\sigma)\Delta$.
\end{definition}

\begin{proposition}
    The map $\epsilon: S_{n} \to \{-1,+1\}$, where $\epsilon(\sigma)$ is the sign of $\sigma$, is a homomorphism, that is, $\epsilon(\sigma \tau) = \epsilon(\sigma)\epsilon(\tau)$ for all $\sigma, \tau \in S_{n}$.
\end{proposition}
\begin{proof}
    Start with $\tau(\Delta)$;
    \begin{equation}
        \tau(\Delta) = \prod_{1 \leq i < j \leq n} (x_{\tau(i)}-x_{\tau(j)}).
    \end{equation}
    Let there be $k$ factors of this polynomial where $\tau(i)>\tau(j)$ with $i<j$. We find that $\tau(\Delta) = (-1)^{k}\Delta$, and so, $\epsilon(\tau) = (-1)^{k}$. Now, $\sigma \tau(\Delta)$ has exactly $k$ factors of the form $x_{\sigma(j)}-x_{\sigma(i)}$, with $j > i$. Bringing out a factor $(-1)^{k}$, we find that $\sigma \tau (\Delta)$ has all factors of the form $x_{\sigma(i)}-x_{\sigma(j)}$, with $j > i$. Thus,
    \begin{equation}
        \epsilon(\sigma \tau)\Delta = \sigma \tau (\Delta) = (-1)^{k} \prod_{1 \leq i < j \leq n} (x_{\sigma(i)}-x_{\sigma(j)}) = (-1)^{k} \sigma(\Delta) = (-1)^{k} \epsilon(\sigma) \Delta = \epsilon(\tau) \epsilon(\sigma) \Delta.
    \end{equation}
    Cancelling out the $\Delta$, we find $\epsilon(\sigma \tau) = \epsilon(\sigma) \epsilon(\tau)$.
\end{proof}
$\epsilon$ is a homomorphism to an Abelian group, so $\epsilon(\sigma \tau) = \epsilon(\sigma) \epsilon(\tau) = \epsilon(\tau) \epsilon(\sigma)$.

\begin{proposition}
    If $\lambda = (i,j)$ is a transposition, then $\epsilon(\lambda) = -1$.
\end{proposition}
\begin{proof}
    If $\lambda = (1,2) \in S_{n}$, it is easy to show that
    \begin{equation}
        \lambda(\Delta) = (x_{1}-x_{2}) \cdots (x_{1}-x_{n}) (x_{2}-x_{3}) \cdots (x_{2}-x_{n}) \cdots = (-1)(\Delta).
    \end{equation}
    Now, if $\sigma = (i,j)$, with $(i,j) \neq (1,2)$, then $(i,j) = \lambda (1,2) \lambda$ where $\lambda$ interchanges $1$ and $i$, and interchanges $2$ and $j$. Using that fact that $\epsilon$ is a homomorphism, $\epsilon(\sigma) = -1$.
\end{proof}

A cycle $\sigma$ of length $k$ is an even permutation if and only if $k$ is odd. This is because it can be decomposed into $k-1$ transpositions, and we would then have $\epsilon(\sigma) = (-1)^{k-1} = 1$ (using the fact that $\epsilon$ is a homomorphism). Some more corollaries of the previous proposition include the fact that $\epsilon$ is a surjective map, and that $\epsilon(\sigma^{-1}) = \epsilon(\sigma)$.

If, for $\sigma \in S_{n}$, $\sigma$ can be decomposed as $\sigma_{1}\sigma_{2} \cdots \sigma_{k}$, where $\sigma_{i}$ is a $m_{i}$-cycle, then $\epsilon(\sigma_{i}) = (-1)^{m_{i}-1}$, and $\epsilon(\sigma) = (-1)^{(\sum m_{i}) - k}$.

\begin{proposition}
    $\sigma$ is an odd permutation if and only if the number of cycles of even length in its cycle decomposition is odd.
\end{proposition}

\section{The Determinant}

\begin{definition}
    If $A = (a_{ij})$ is a square matrix of order $n$, then the \eax{determinant} of $A$ is defined as
    \begin{equation}
        \det{A} = \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma(n)}.
    \end{equation}
\end{definition}
Using this definition of the determinant of a square matrix, one may derive the usual determinant properties with ease.\\ \\
\textit{January 7th.}

\begin{remark}
    The following properties may be inferred:
    \begin{itemize}
        \item If $A$ contains a row of zeroes, or a column of zeroes, then $\det{A} = 0$.
        \item $\det{I_{n}} = 1$.
        \item The determinant of a diagonal matrix is the product of the diagonal elements. This is because if $\sigma \in S_{N}$ is not the identity permutation, then there exists at least one element in the corresponding term where $i \neq \sigma(i)$, and $a_{i \sigma(i)}$ makes the term zero. For the identity transformation, it contains only those elements of the form $a_{ii}$.
    \end{itemize}
\end{remark}

Other non-trivial properties may also be shown with ease.

\begin{corollary}
    If $A$ is an upper triangular matrix, then $\det{A}$ is the product of the diagonal entries.
\end{corollary}
\begin{proof}
    If $a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \neq 0$, then $a_{n \sigma(n)} \neq 0$, that is, $\sigma(n) = n$, as $a_{ni} = 0 \; \forall \; i < n$. Again, $\sigma_{(n-1) \sigma(n-1)} \neq 0$ leads us to conclude that $\sigma(n-1) = n-1$ as $\sigma$ is a bijection and has to lead to a non-zero element. By similar logic, $\sigma(i) = i$ for all valid $i$. So, $\sigma$ is the identity permutation.
\end{proof}
\begin{corollary}
    If $A$ is a lower triangular matrix, then $\det{A}$ is the product of the diagonal entries.
\end{corollary}
\begin{proof}
    The proof of this is similar to the previous proof if we consider that the determinant of the tranpose of a matrix is equal to the determinant of said matrix.
\end{proof}

\begin{theorem}
    The determinant of a matrix is equal to the determinant of its transpose, that is, $\det{A} = \det{A^{t}}$ for a square matrix $A$.
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}


\begin{proposition}
    Let $B$ be obtained from $A$ by multiplying a row (or column) of $A$ by a non-zero scalar, $\alpha$. Then, $\det{B} = \alpha \det{A}$.
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{proposition}
    If $B$ is obtained from $A$ by interchanging any two rows (or columns) of $A$, then $\det{B} = -\det{A}$.
\end{proposition}
\begin{proof}
    Let $B$ be obtained from $A$ by interchanging the rows $k$ and $l$, with $k < l$. We then have
    \begin{align}
        \det{B} &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) b_{1 \sigma(1)} b_{2 \sigma(2)} \cdots b_{n \sigma(n)} \notag \\
        &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{(k-1) \sigma(k-1)} a_{l \sigma(k)} \sigma_{(k+1) \sigma(k+1)} \cdots a_{k \sigma(l)} \cdots a_{n \sigma(n)}.
    \end{align}
    As $\sigma$ runs through all elements in $S_{n}$, $\tau = \sigma(k, l)$ also runs through all $S_{n}$. Hence, via $\epsilon(\tau) = -\epsilon(\sigma)$, the equation now looks like
    \begin{align}
        \det{B} &= -\sum_{\tau \in S_{n}} \epsilon(\tau) a_{1 \tau(1)} \cdots a_{l \tau(l)} \cdots a_{k \tau(k)} \cdots a_{n \tau(n)} = -\det{A}.
    \end{align}
\end{proof}

\begin{proposition}
    If two rows (or columns) of $A$ are equal, then $\det{A} = 0$.
\end{proposition}
\begin{proof}
    Suppose that the rows $k$ and $l$ of $A$ are equal. Interchanging will alter the determinant by $-1$, so $\det{A} = -\det{A} \implies 2 \det{A} = 0 \implies \det{A} = 0$ if $2 \neq 0$ in the field $F$ from where the elements of $A$ arrive.

    If $2 = 0$ in $F$, that is, $F$ is of characteristic $2$, we pair the $\sigma$ term in the expression of $\det {A}$ with the term $\tau$ where $\tau = \sigma (k, l)$. The terms corresponding to $\sigma$ and $\tau$ in the expressions are the same, differing in only the sign. Hence, $\det{A} = 0$.
\end{proof}

\begin{theorem}
    For a fixed $k$, let the row $k$ of $A$ be the sum of the two row vectors $X^{t}$ and $Y^{t}$, that is, $a_{kj} = x_{j} + y_{j}$ for all $1 \leq j \leq n$. Then $\det{A} = \det{B} + \det{C}$ where $B$ is obtained from $A$ by replacing the row $k$ of $A$ by the row vector $X^{t}$, and $C$ is obtained from $A$ by replacing the row $k$ of $A$ by the row vector $Y^{t}$.
\end{theorem}
\begin{proof}
    We utilize the fact that $a_{kj} = x_{j} + y_{j}$. We have
    \begin{align}
        \det{A} &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{k \sigma(k)} \cdots a_{n \sigma(n)} \notag \\
        &= \left( \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots x_{\sigma(k)} \cdots a_{n \sigma(n)} \right) + \left( \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots y_{\sigma(k)} \cdots a_{n \sigma(n)} \right) \notag \\
        &= \det{B} + \det{C}. \notag
    \end{align}
\end{proof}

\begin{proposition}
    If a scalar multiple of a row (or column) is added to a row (or column) of a matrix, the determinant remains unchanged.
\end{proposition}
\begin{proof}
    The proof follows immediately from the previously proved properties.
\end{proof}
\textit{January 10th.}

\begin{definition}
    For $a_{ij} \in A$, the \eax{cofactor} of $a_{ij}$ is $A_{ij}=(-1)^{i+j} \det M_{ij}$, where $M_{ij}$ is the $(n-1) \times (n-1)$ matrix obtained from $A$ by deleting the $i^{\text{th}}$ row and $j^{\text{th}}$ column of $A$.
\end{definition}
\begin{lemma}
    Fix $k,j$. If $a_{kl} = 0$ for all $l \neq j$, then $\det{A} = a_{kj}A_{kj}$.
\end{lemma}
\begin{proof}
    Take $A$ to be a $n \times n$ matrix. We deal in cases.
    \begin{itemize}
        \item Case I: $k=j=n$. In the expansion of the determinant,
        \begin{equation}
            \det{A} = \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)}, \notag
        \end{equation}
        only those $\sigma$'s survive where $\sigma(n) = n$. These $\sigma$'s can be thought of as permutations of $S_{n-1}$ instead. The sign of $\sigma \in S_{n}$ and $\sigma \in S_{n-1}$ is the same as $n$ is fixed. Thus, we get
        \begin{equation}
            a_{nn} \sum_{\sigma \in S_{n-1}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{(n-1) \sigma(n-1)} = a_{nn} \det M_{nn} = (-1)^{n+n} a_{nn} A_{nn} = a_{nn} A_{nn}.
        \end{equation}
        \item Case II: $(k,j) \neq (n,n)$. We construct a matrix $B$ by interchanging $n-k$ rows and $n-j$ columns to bring $a_{ij}$ to the position $(n,n)$. Thus, we have $\det B = (-1)^{n-k+n-j} \det A = (-1)^{k+j} \det A$. But $B = a_{kj} \det M_{kj}$, so
        \begin{equation}
            \det A = (-1)^{k+j} a_{kj} \det M_{kj} = a_{kj} A_{kj}.
        \end{equation}
    \end{itemize}
\end{proof}
\begin{theorem}
    Let $A$ be a $n \times n$ matrix, and let $1 \leq k \leq n$. Then, $\det {A} = \sum \limits_{j=1}^{n} a_{kj} A_{kj}$, expansion by the $k^{\text{th}}$ row.
\end{theorem}
\begin{proof}
    Write out the $k^{\text{th}}$ row of $A$ as $x_{1}^{t}+\ldots+x_{n}^{t}$, where $x_{i} = (0,\ldots,0,a_{ki},0,\ldots,0)^{t}$, and all the other rows remaining are the same. Writing the matrix $A$ as the sum of $n$ matrices where each matrix is the same as $A$ but with a row that looks like $x_{i}^{t}$, we can easily show that $\det{A} = \sum_{j=1}^{n} a_{kj} A_{kj}$.
\end{proof}
\begin{example}
    Let $n \geq 1$, and let $A_{n} = \begin{pmatrix}
        a_{1}^{n-1} & a_{1}^{n-2} & \ldots & a_{1} & 1 \\
        a_{2}^{n-1} & a_{2}^{n-2} & \ldots & a_{2} & 1 \\
        \ldots & \ldots & \ldots & \ldots & \ldots \\
        a_{n}^{n-1} & a_{n}^{n-2} & \ldots & a_{n} & 1
    \end{pmatrix}$. Then, $\det {A_{n}} = \prod \limits_{1 \leq i \leq j \leq n} (a_{i}-a_{j})$.
\end{example}
\begin{proof}
    If $a_{i}=a_{j}$ for some $i \neq j$, then $\det{A_{n}} = 0$ as two rows are then identical. Hence, assume that the $a_{i}$'s are distinct. Now construct
    \begin{equation}
        B_{n} = 
        \begin{pmatrix}
            x_{1}^{n-1} & x_{1}^{n-2} & \ldots & x_{1} & 1 \\
            a_{2}^{n-1} & a_{2}^{n-2} & \ldots & a_{2} & 1 \\
            \ldots & \ldots & \ldots & \ldots & \ldots \\
            a_{n}^{n-1} & a_{n}^{n-2} & \ldots & a_{n} & 1
        \end{pmatrix}.
    \end{equation}
    Notice that $\det B_{n} \in F[x]$, where $F$ is the field, and $x$ is an indeterminate. $\det {B}$ is also of degree $(n-1)$; let us call this polynomial $f(x)$. Each of $a_{2},\ldots,a_{n}$ are roots of $f(x)$, so $f(x)$ must be of the form $f(x) = C(x-a_{2}) \ldots (x-a_{n})$. Equating coefficients of $x^{n-1}$, we get
    \begin{equation}
        C = \prod_{2 \leq i < j \leq n} (a_{i}-a_{j}) = \det \begin{pmatrix}
            a_{2}^{n-2} & \ldots & a_{2} & 1 \\
            \ldots & \ldots & \ldots & \ldots \\
            a_{n}^{n-2} & \ldots & a_{n} & a_{1}
        \end{pmatrix}.
    \end{equation}
    Thus, we must have
    \begin{align}
        f(x) &= \left( \prod_{2 \leq i < j \leq n} (a_{i}-a_{j}) \right) (x-a_{2}) \cdots (x-a_{n}) \\
        \implies \det A_{n} = f(1) &= \prod_{1 \leq i < j \leq n} (a_{i}-a_{j}).
    \end{align}
\end{proof}
\begin{example}
    Show that there exists a unique polynomial of degree $n$ that takes arbitrary prescribed values at the $(n+1)$ points $x_{0},x_{1},\ldots,x_{n}$.
\end{example}

\chapter{EIGENVECTORS AND EIGENVALUES}

\section{Linear Transformers and an Introduction}
Let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of vector space $V$ and $\mc{C} = (w_{1},\ldots,w_{n})$ be a basis of a vector space $W$. As these are bases, given a $v \in V$, there exists a unique $X \in F^{n}$ such that $v = \mc{B}X$, called the \eax{coordinate vector} of $v$ with respect to the basis $\mc{B}$. We note that since the mapping from a $v \in V$ to a $X \in F^{n}$ is linear in nature and is bijection, the vector spaces $V$ and $F^{n}$ are isomorphic to each other. Similarly, a mapping that takes $w \in W$ to $Y \in F^{m}$ shows that $W$ and $F^{m}$ are isomorphic to each other. 

Now suppose that there exists a linear map that takes $v \mapsto Tv$ with $v \in V$ and $Tv \in W$. This transformer $T$ is with respect to the bases $\mc{B}$ and $\mc{C}$ of $V$ and $W$, respectively. We construct the $m \times n$ matrix $A$ so that the $j^{\text{th}}$ column of $A$ is the coordinate vector of $Tv_{j}$ with respect to the basis $\mc{C}$. We will then have $T(\mc{B}) = \mc{C}A$. For any vector $v \in V$, we have
\begin{align}
    v &= \mc{B}X = v_{1}x_{1}+ \ldots v_{n}x_{n} \notag \\
    \implies T(v) &= T(v_{1})x_{1} + \ldots T(v_{n})x_{n} = (T(v_{1}),\ldots,T(v_{n})) \begin{pmatrix}
        x_{1} \\ x_{2} \\ \ldots \\ x_{n}
    \end{pmatrix} = T(\mc{B})X = (\mc{C}A) X \\
    &= (w_{1},\ldots,w_{m}) AX;
\end{align}
the coordinate vector of $Tv$ with respect to the basis $AX$. In fact, if we denote the isomorphism from $V$ to $F^{n}$ by $\phi_{\mc{B}}$ and the isomorphism from $W$ to $F^{m}$ by $\phi_{\mc{C}}$, we get $\phi_{\mc{C}} \circ T = (\text{mult. by $A$}) \circ \phi_{\mc{B}}$.

The next theorem will be divided into two parts.
\begin{theorem}
    \begin{enumerate}
        \item The vector space form. Let $T:V \to W$ be a linear mapping between finite dimensional vector spaces $V$ and $W$, of dimensions $n$ and $m$ respectively. There are bases $\mc{B}$ and $\mc{C}$ of $V$ and $W$ respectively such that the matrix of $T$ with respect to the bases $\mc{B}$ and $\mc{C}$ looks like $\begin{pmatrix}
            I_{r} & O_{r \times (n-r)} \\ O_{(m-r) \times r} & O_{(m-r) \times (n-r)}
        \end{pmatrix}_{m \times n}$.
        \item The matrix form. If $A$ is a $m \times n$ matrix, then there exists an invertible matrix $Q_{m \times m}$ and an invertible matrix $P_{n \times n}$ such that $Q^{-1}AP$ is of the form $\begin{pmatrix}
            I_{r} & 0 \\ 0 & 0
        \end{pmatrix}$, where $r$ is the rank of $A$.
        \item In fact, both these forms of the theorem are equivalent.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Let $(u_{1},\ldots,u_{n-r})$ be a basis of $\ker{T}$. We can extend this to a basis $\mc{B}$ by appending independent vectors that do not belong to the kernel of $T$, that is, $(v_{1},\ldots,v_{r},u_{1},\ldots,u_{n-r})$. Let $(Tv_{1},\ldots,Tv_{r})$ be a basis of $\text{Im}{T}$. We can extend this to a basis of $W$, say $\mc{C} = (w_{1},\ldots,w_{r},w_{r+1},\ldots,w_{m})$, where $w_{i} = Tv_{i}$ for $1 \leq i \leq r$. These bases are the desired ones.
        \item $P$ is a sequence of column operations, multipled to form a matrix, and $Q^{-1}$ is a sequence of row operations, multiplied to form a matrix, that get the matrix $A$ into the desired form. These are our desired $P$ and $Q$.
        \item Suppose the vector space form holds. Let $A$ be a $m \times n$ matrix over $F$, with $A:F^{n} \to F^{m}$ defined as $X \mapsto AX$. There then exists a basis $\mc{B}$ of $F^{n}$ and a basis $\mc{C}$ of $F^{m}$ such that the linear map $A$ with respect to ther bases $\mc{B}$ and $\mc{C}$ has the desired matrix. We then have $\mc{B} =I_{n}P_{n \times n}$ and $\mc{C} = I_{m}Q_{m \times m}$, with both $P$ and $Q$ invertible. We claim that the matrix of the linear mapping $A$ with respect to the bases $\mc{B}$ and $\mc{C}$ is $Q^{-1}AP$.
    \end{enumerate}
\end{proof}

\textit{January 16th.}
\begin{proposition}
    \begin{enumerate}
        \item Let $T:V \to W$ be a linear map, and $A$ the matrix of $T$ with respect to the bases $\mc{C}$ and $\mc{C}$ of $V$ and $W$ respectively. Let $\mc{B}'$ and $\mc{C}'$ be new bases of $V$ and $W$ respectively, and let the change of basis matrices be given by $\mc{B}' = \mc{B}P$ and $\mc{C}' = \mc{C}Q$. Then the matrix of $T$ with respect to $\mc{B}'$ and $\mc{C}'$ is $Q^{-1}AP$.
        \item If $A' = Q_{1}^{-1}AP_{1}$, where $P_{1}$ and $Q_{1}$ are $n \times n$ and $m \times m$ invertible matrices, respectively, then $A'$ is the matrix of $T$ with respect to the bases $\mc{B}P_{1}$ and $\mc{C}Q_{1}$.
    \end{enumerate}
    
\end{proposition}
\begin{proof}
    Let the coordinate vector of $v$ with respect to the basis $\mc{B}'$ be $X'$. We claim that the coordinate vector of $Tv$ with respect to the basis $\mc{C}'$ is $Y'$, where $Y' = (Q^{-1}AP)X'$. We assume that $\mc{B}' = \mc{B}P_{n \times n}$, $\mc{C}' = \mc{C}Q_{m \times m}$, and $T(\mc{B}) = \mc{C}A_{m \times n}$. If $v = \mc{B}X$, then $T(v) = \mc{C}(AX)$. If we let $v = \mc{B}'\mc{X}' = v_{1}'x_{1}' + \ldots + v_{n}'x_{n}'$, then
    \begin{equation}
        T(v) = \mc{C}'Y' = (\mc{C}Q)' = \mc{C}(QY') = \mc{C}(APX') \implies QY' = APX' \implies Y' = (Q^{-1}AP)X'
    \end{equation}
    To prove the second part, we will show that the first part implies it. Let $A_{m \times n}$ be a matrix. Let $T_{A}$ be the linear map from $\R^{n} \to \R^{m}$ given by multiplication by $A$, that is $T_{A} : \R^{n} \to \R^{m}$ given by $X \mapsto AX$. By the first part, there exist bases $P_{n \times n}$ and $Q_{m \times m}$, both invertible, such that with respect $P$ and $Q$, the matrix of $T_{A}$ looks like $\begin{pmatrix}
        I & O \\ O & O
    \end{pmatrix}$, that is, $Q^{-1}AP = \begin{pmatrix}
        I & O \\ O & O
    \end{pmatrix}$.
\end{proof}

\subsection{Linear Operators}
Let $T:V_{\mc{B}} \to V_{\mc{B}}$. Let $A$ be the matrix of $T$ with respect to the basis $\mc{B}$. The other matrices of $T$ with respect to new bases are $P^{-1}AP$, where $P_{n \times n}$ is invertible. Also, the fact that $T$ is bijective, one-one, or onto are all equivalent for a finite dimensional vector space $V$.

\subsection{Eigenvectors and Eigenvalues}
\begin{definition}
    A non-zero vector $v \in V$ is said to be an \eax{eigenvector} of $T$ if $T(v) = \lambda v$ for some $\lambda \in \F$. If $A$ is a $n \times n$ matrix, a non-zero column vector $X$ is said to be an eigenvector of $A$ if $AX = \lambda X$ for some $\lambda \in \F$. $\lambda$, in both these cases, is called the \eax{eigenvalue} of $v$ and $X$ respectively.
\end{definition}
Usually, we always disregard the zero vector being an eigenvector. If $v$ is an eigenvector of $T:V \to V$, and $v = \mc{B}X$ with respect to some basis $\mc{B}$ of $V$, then $X$ is an eigenvector of the matrix of $T$ with respect to the basis $\mc{B}$. In fact,
\begin{equation}
    \mc{B}(AX) = (\mc{B}A)X = T(\mc{B})X = T(\mc{B}X) = Tv = \lambda v = \lambda \mc{B} X = \mc{B} (\lambda X) \implies AX = \lambda X.
\end{equation}
The converse is also true; if $X$ is an eigenvector of $A_{n \times n}$, then $X$ is also an eigenvector of $T_{A} : \R^{n} \to \R^{n}$.

\begin{proposition}
    0 is an eigenvalue of $A_{n \times n}$ ($T:V \to V)$ if and only if $A$ ($T$) is non-invertible (not an isomorphism).
\end{proposition}
Suppose $v$ is an eigenvector of $T: V \to V$ with eigenvalue $\lambda$. Let $W$ be the subspace spanned by $v$. Then every vector $w \in W$ is an eigenvector of $T$ with eigenvalue $\lambda$. The proof of this is left as an exercise.

\begin{definition}
    Two matrices $A_{n \times n}'$ and $A_{n \times n}$ are called \eax{similar matrices} if there exists an invertible matrix $P_{n \times n}$ such that $P^{-1}AP = A'$.
\end{definition}

Again let $T: V \to V$ be a linear operator, and let $\mc{B} = (v_{1},\ldots,v_{n})$. Suppose, with respect to the basis $\mc{B}$, the matrix of $T$ is $\begin{pmatrix}
    \lambda_{1} & \ldots & \ldots & \ldots \\ 0 & \ldots & \ldots & \ldots \\ \ldots & \ldots & \ldots & \ldots \\ 0 & \ldots & \ldots & \ldots    
\end{pmatrix}$. Then $v_{1}$ is an eigenvector with eigenvalue $\lambda_{1}$.
\section{Finding Eigenvalues and Eigenvectors}
\textit{January 21st.}

Let $T:V \to V$ and let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of $V$. Then the matrix of $T$ with respect to the basis $\mc{B}$ is a diagonal matrix if and only if each of the basis elements is an eigenvector. An equivalent statement for matrices is that an $n \times n$ matrix $A$ is similar to a diagonal matrix if and only if $\F^{n}$ admits a basis consisting of eigenvectors of $A$. The proof of this is left as an exercise to the reader.

We can now discuss the computation. For a linear operator $T: V \to V$, $\lambda$ is an eigenvalue of $T$ if and only if there exists a non-zero vector $v$ such that $Tv = \lambda v$. This can be rearranged to give
\begin{equation}
    (\lambda I_{v} -T)v = 0.
\end{equation}
We can now consider $\lambda I_{v} - T: V \to V$ to be a linear operator which maps $v \mapsto \lambda v - Tv$. If eigenvalues exist, this operator is a singular operator, that is, it contains a non-trivial kernel. The matrix of the operator $\lambda I_{v} - T$ comes out to be $\lambda I_{n} - A$, where $A$ is the matrix of $T$ with respect to the basis $\mc{B}$. This matrix is now singular, so we must have
\begin{equation}
    \det(\lambda I_{n} - A) = 0.
\end{equation}
The equation $\det(\lambda I_{n} - A)$ is called the \eax{characteristic polynomial} of $A$, and also $T$(?). The roots of this polynomial in $\lambda$ which lie in $\F$ are the eigenvalues of $A$, and $T$ as well.

We would now like to show that similar matrices have the same eigenvalues, that is,
\begin{equation}
    \det(\lambda I_{n} - P^{-1}AP) = \det(\lambda I_{n} - A).
\end{equation}
This is simple to see as $\det(\lambda I_{n} - P^{-1}AP) = \det(P^{-1}(\lambda I_{n} - A)P) = \det P^{-1} \cdot \det(\lambda I_{n} - A) \cdot \det P = \det(\lambda I_{n} - A)$. The found out eigenvalues from this equation can then be put back and solved for $v$ to get the corresponding eigenvectors.

\begin{proposition}
    Let $\lambda_{1},\ldots,\lambda_{r}$ be distinct eigenvalues of $T:V \to V$ and let $v_{1},\ldots,v_{r}$ be the corresponding eigenvectors of $T$. Then $(v_{1},\ldots,v_{r})$ is a linearly independent set in $V$.
\end{proposition}
\begin{proof}
    We claim that this is true for $r = 1,2$. Using a form of induciton, we will assume the result for $r-1$. Begin with
    \begin{align}
        \alpha_{1}v_{1} + \ldots + \alpha_{r}v_{r} &= 0 \notag \\
        \implies \alpha_{1}Tv_{1} + \ldots + \alpha_{r}Tv_{r} &= 0 \notag \\
        \implies \alpha_{1}\lambda_{1}v_{1} + \ldots + \alpha_{r}\lambda_{r}v_{r} &= 0.
    \end{align}
    Multiplying the first equation by $\lambda_{1}$ and subtracting it from the current equation, we have
    \begin{align}
        (\alpha_{2}\lambda_{2}-\alpha_{2}\lambda_{1})v_{2} + (\alpha_{3}\lambda_{3}-\alpha_{3}\lambda_{1})v_{3} + \ldots + (\alpha_{r}\lambda_{r}-\alpha_{r}\lambda_{1})v_{r} &= 0 \notag \\
        \implies \alpha_{2}(\lambda_{2}-\lambda_{1}) + \alpha-{3}(\lambda_{3}-\lambda_{1})v_{3} + \ldots + \alpha_{r}(\lambda_{r}-\lambda_{1})v_{r} &= 0.
    \end{align}
    By hypothesis, $\alpha_{j}(\lambda_{j}-\lambda_{1}) = 0$. As the eigenvalues are distinct, we must have $\alpha_{j} = 0$ for $j = 2,3,\ldots,r$. We are left with $\alpha_{1}v_{1} = 0$, which gives us $\alpha_{1} = 0$.
\end{proof}
When the $n$ eigenvalues found of $A$ are distinct, the corresponding eigenvectors $v_{1},\ldots,v_{n}$ are linearly independent in $\F^{n}$, and hence $\mc{B} = (v_{1},\ldots,v_{n})$ is a basis of $\F^{n}$. The matrix $P^{-1}AP$ is the matrix of the linear operator $T_{A} : \F^{n} \to \F^{n}$ with respect to the basis $\mc{B}$, with the column of $P$ being the eigenvectors $v_{1},\ldots,v_{n}$. As $\mc{B}$ consists of only eigenvectors, $P^{-1}AP$ is a diagonal matrix with the diagonal entries being the $n$ eigenvalues.

We now define the determinant and trace for a linear operator. For such an operator $T$, $\text{tr}T = \text{tr}A$ where $A$ is a matrix of $T$ with respect to some abitrary basis. Note that since $\tr(P^{-1}AP) = \tr(APP^{-1}) = \tr{A}$, the choice of basis is not important. Similarly, we define $\det{T} = \det{A}$.

We can now have a closer look at the characteristic equation. To find the constant term of $\det(xI-A)$, we simply plug in $x=0$ to give us $\det(-A) = (-1)^{n} \det A$. The coefficient of $x^{n-1}$ in $\det(xI-A)$ is $-\tr{A}$ as the coefficients of $x^{n-1}$ come solely from the expansion of $(x-a_{11})(x-a_{22})\cdots(x-a_{nn})$. Clearly, we can conclude that the sum of the eigenvalues is $\tr{A}$ and the product of the eigenvalues is $\det{A}$.

\subsection{Eigenspace}
\textit{January 23rd.}

For ease, let us denote $\chi_{T}(x)$ to mean $\det(xI-A)$. The \eax{eigenspace} for a given eigenvalue $\lambda$ is defined as
\begin{equation}
    E_{\lambda} = \{v \in V : Tv = \lambda v\}.
\end{equation}
This is a subspace of the vector space $V$. The \eax{geometric multiplicity} of $\lambda$ is defined as the dimension of $E_{\lambda}$. This geometric multiplicity of $\lambda$ is always less than or equal to its algebraic multiplicity in $\chi_{T}(x)$. For recall, the \eax{algebraic multiplicity} of $\lambda$ is the highest power of $(x-\lambda)$ that divides $\chi_{T}(x)$.

\begin{theorem}
    Let $\lambda$ be an eigenvalue of $T:V \to V$. Then the geometric multiplicity of $\lambda$ is always less than or equal to its algebraic multiplicity.
\end{theorem}
\begin{proof}
    Let $k$ me the geometric multiplicity of $\lambda$. Let $(v_{1},\ldots,v_{k})$ be an ordered basis of $E_{\lambda}$. Extend this to a basis $\mc{B} = (v_{1},\ldots,v_{k},u_{1},\ldots,u_{n-k})$ of $V$. The matrix of $T$ with respect to the basis $\mc{B}$ is of the form $A = \begin{pmatrix}
        \lambda I_{k} & B \\ O & D
    \end{pmatrix}$. Thus, the characteristic polynomial looks like
    \begin{equation}
        \chi_{T}(x) = \det(xI_{n}-A) = \det \begin{pmatrix}
            (x-\lambda)I_{k} & -B \\ O & xI_{n-k}-D
        \end{pmatrix}
        = (x-\lambda)^{k} \cdot \det(XI_{n-k}-D).
    \end{equation}
    This shows that $(x-\lambda)^{k}$ divides $\chi_{T}(x)$, so we must have an algebraic multiplicity greater than or equal to this $k$.
\end{proof}

\section{Diagonalizability}
We first define what this means for a linear mapping from $V$ to $V$.
\begin{definition}
    A linear operator $T: V \to V$ is said to be a \eax{diagonizable linear operator} if there exists a basis of $V$ consisting of eigenvectors of $T$. This means that the matrix of $T$ with respect to this basis if a digaonal matrix and the matrix of $T$ with respect to any other basis is similar to this diagonal matrix.
\end{definition}
A similar definition works for matrices.
\begin{definition}
    An $n \times n$ matrix $A$ over $\F$ is said to be a \eax{diagonizable matrix} if $A$ is similar to a diagonal matrix. Equivalently, $\F^{n}$ then admits a basis consisting of eigenvectors of $A$, thinking of $T_{A}:\F^{n} \to \F^{n}$ as a linear operator.
\end{definition}

Now let us suppose that $T$ is diagonizable. Let $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$ be the distinct eigenvalues of $T$. There then exists an ordered basis consisting of eigenvectors of $T$ and with respect to this basis, the matrix of $T$ is a diagonal matrix with diagonal entries consisting solely of $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$.

If $\lambda_{i}$ is of algebraic multiplicity $d_{i}$, then the matrix of $T$ looks like $\begin{pmatrix}
    \lambda_{1}I_{d_{1}} & & & \\
     & \lambda_{2}I_{d_{2}} & & \\
     & & \ldots & \\
     & & & \lambda_{k}I_{d_{k}}
\end{pmatrix}$. Thus, the characteristic polynomial then looks like $(x-\lambda_{1})^{d_{1}} (x-\lambda_{2})^{d_{2}} \cdots (x-\lambda_{k})^{d_{k}}$.

The geometric multiplicity of $\lambda_{i}$ is the dimension of $E_{\lambda_{i}}$, that is, the nullity of the operator $(\lambda_{i}I_{n}-A)$. But here, $\ker(\lambda_{i}I_{n}-A) = d_{i}$, which is just the algebraic multiplicity of $\lambda_{i}$. Hence, if $T$ is diagonizable, then each eigenvalue of it has the same algebraic multiplicity and geometric multiplicity.

\begin{proposition}
    If $E_{\lambda_{1}},\ldots,E_{\lambda_{k}}$ are the eigenspaces corresponding to the distinct eigenvalues, say, $\lambda_{1},\ldots,\lambda_{k}$ of $T$, then $E = E_{\lambda_{1}} + \cdots + E_{\lambda_{k}}$ is a direct sum.
\end{proposition}
\begin{proof}
    It is enough to show that $E_{\lambda_{1}},\ldots,E_{\lambda_{k}}$ are independent. Let $v_{1}+v_{2}+\ldots+v_{k} = 0$, where $v_{i} \in E_{\lambda_{i}}$. As $v_{1},v_{2},\ldots,v_{k}$ come from distinct eigenspaces, they are linearly independent, and our equation must imply that $v_{1} = \ldots = v_{k} = 0$.
\end{proof}
\begin{proposition}
    If $T$ is a diagonizable operator, and if $\lambda_{1},\ldots,\lambda_{k}$ are the distinct eigenvalues of $T$, then
    \begin{equation}
        V = E_{\lambda_{1}} \oplus \ldots \oplus E_{\lambda_{k}}.
    \end{equation}
\end{proposition}
\begin{proof}
    As $T$ is diagonizable, the algebraic and geometric multiplicities are equal for all the eigenvalues $\lambda_{i}$. Denote $\dim E_{\lambda_{i}} = d_{i}$. As $\chi_{T}(x)$ completely factors into linear factors, due to $T$ being diagonizable, we have $n = d_{1} + \ldots + d_{k}$. Also, $E_{\lambda_{1}} + \ldots + E_{\lambda_{k}}$ is a direct sum, that is,
    \begin{equation}
        \dim(E_{\lambda_{1}}+\ldots+E_{\lambda_{k}}) = \dim E_{\lambda_{1}} + \ldots + \dim E_{\lambda_{k}} = n.
    \end{equation}
    This direct sum is a subspace of $V$ and has the dimension as $V$. This mut mean that the direct sum is exactly $V$.
\end{proof}

\begin{theorem}
    Let $T$ be a linear operator on a finite dimensional vector space $V$, and let $\lambda_{1},\ldots,\lambda_{k}$ be the distinct eigenvalues of $T$. Also let $E_{\lambda_{i}}$ be the eigenspace of $\lambda_{i}$. Then, the following are equivalent.
    \begin{itemize}
        \item $T$ is diagonizable,
        \item $\chi_{T}(x) = (x-\lambda_{1})^{d_{1}} \cdots (x-\lambda_{k})^{d_{k}}$ and $\dim E_{\lambda_{i}} = d_{i}$,
        \item $V = E_{\lambda_{1}} \oplus \ldots \oplus E_{\lambda_{k}}$.
    \end{itemize}
\end{theorem}

\section{Polynomials}
\textit{January 28th.}

Let $\F[x]$ denote the set of all polynomials with coefficients coming from the field $\F$. With respect to the addition, it is an Abelian group. The multiplication here is associative, commutative, and distributive; there also exists a multiplicative identity. This makes $\F[x]$ into a commutative ring. Note that $\F[x]$ is also an infinite dimensional vector space over $\F$, since scalar multiplication is also defined. Together, these combine to form an algebra over the field.

\begin{definition}
    Let $d \in \F[x]$ with $d \neq 0$. For $f \in \F[x]$, we say that $d$ divides $f$ if there exists a $q \in \F[x]$ such that $f = dq$ in $\F[x]$.
\end{definition}
\begin{corollary}
    For $f \in \F[x]$, $f(c) = 0$ if and only if $x-c$ divides $f(x)$.
\end{corollary}
\begin{corollary}
    A polynomial $f \in \F[x]$ of degree $n$ has at most $n$ roots in $\F$.
\end{corollary}
\begin{proof}
    The proof is by induction. Note that this is true for $n = 0, 1$. If $\alpha$ is a root, then $f(x) = (x-\alpha) \cdot q(x)$. As $q(x)$ is of degree $n-1$, and all roots of $q(x)$ are root of $f(x)$, this follows by hypothesis.
\end{proof}

\begin{definition}
    An \eax{ideal} of $\F[x]$ is a subspace of $\F[x]$, say $I$, such that if $f \in I$ and $g \in \F[x]$, then $fg \in I$.
\end{definition}
\begin{example}
    Let $f \in \F[x]$. Define $I_{f} = \langle f\rangle = \{fg : g \in \F[x]\}$. Note that $I_{f}$ is called a \eax{principal ideal}, that is, it is an ideal generated by a single element.
\end{example}
\begin{theorem}
    $\F[x]$ is a principal ideal domain, that is, every ideal in $\F[x]$ is a principal ideal.
\end{theorem}
\begin{proof}
    Let $d$ be a polynomial of least degree in the ideal $I$, where $I$ is a non-zero ideal. Let, without loss of generatlity, $d$ be monic (if not, simply multiply it by a sutitable scalar).

    Let $f \in I$. Then there exists $q,r \in \F[x]$ such that $f = dq + r$ and either $r = 0$ or $\deg r < \deg d$. Note that since $f,d \in I$, $dq \in I$, so $f-dq \in I \implies r \in I$. As $d$ was of minimal degree in $I$, we must have $r = 0$. Thus, $f = dq$ and, thus, $I = \langle d \rangle$.
\end{proof}
If $I$ is an ideal of $\F[x]$, then there exists a unique polynomial $d \in I$ such that $I = \langle d \rangle$.

\subsection{Interaction with Linear Operators}
Let $f \in \F[x]$, and let $T:V \to V$ be a linear mapping. If
\begin{equation*}
    f(x) = a_{0} + a_{1}x + \ldots + a_{k}x^{k}
\end{equation*}
with $a_{k} \neq 0$, we define
\begin{equation*}
    f(T) = a_{0}I_{n} + a_{1}T + \ldots + a_{k}T^{k}.
\end{equation*}
Note that $f(T)$ is also a linear mapping from $V$ to $V$. Let $I$ be the set of all $f \in \F[x]$ such that $f(T)$ is the zero operator. All such polynomials are called \eax{annihilator}s. $I$ satisfies the properties of a vector space; it is a subspace of the space of all polynomials. $I$ is also an ideal of $\F[x]$.

\begin{definition}
    The \eax{minimal polynomial} of the linear operator $T:V \to V$ is the generator of the ideal of annihilators.
\end{definition}
Denote the minimal polynomial by $m_{T}(x)$. So, $m_{T}(x)$ is
\begin{enumerate}
    \item monic,
    \item of least degree among all annihilators of $T$.
\end{enumerate}
If $A$ is a $n \times n$ matrix, the minimal polynomial of $A$ is defined as the unique monic polynomial $m_{A}(x)$ of least degree such that $m_{A}(A) = O_{n \times n}$. It can be verified that if $A$ is the matrix of a linear operator $T:V \to V$ and if $f \in \F[x]$, then the matrix of the operator $f(T) : V \to V$ is $f(A)$ with respect to the same basis. It follows that the minimal polynomial of $T$ is same as the minimal polynomial of a matrix of $T$.

Note that $T$ belongs to $\text{Hom}_{\F}(V, V)$, which is of dimension $n^{2}$. Thus, $I, T, T^{2}, \ldots, T^{n^{2}}$ is a linearly dependent set and there exist scalars $a_{0},a_{2},\ldots,a_{n^{2}}$ such that
\begin{equation}
    a_{0}I + a_{1} T + a_{2}T^{2} + \ldots + a_{n^{2}} T^{n} = O.
\end{equation}
So, an annihilator of $T$ is
\begin{equation*}
    f(x) = a_{0} + a_{1}x + a_{2}x^{2} + \ldots + a_{n^{2}} x^{n^{2}}
\end{equation*}
and we must have $\deg m_{T}(x) \leq n^{2}$.

\begin{theorem}
    Let $T: V \to V$ with $n$ the dimension of the space $V$. The characteristic polynomial of $T$ and the minimal polynomial of $T$ have the same roots, except (possibly) for the multiplicities.
\end{theorem}
\begin{proof}
    We claim that $m_{T}(c) = 0$ if and only if $c$ is an eigenvalue. Let $m_{T}(c) = 0$. Thus, $m_{T}(c) = (x-c) \cdot q(x)$, with $q \in \F[x]$ and $\deg q < \deg m$. Also, $q(T)$ is \textit{not} the zero operator. So, there exists a $u \in V$ (non-zero vector) such that $q(T)(u) = v \neq 0$. Then,
    \begin{align}
        0 = m(T)(u) = (T-cI) \cdot q(T)(u) = (T-cI)v
    \end{align}
    which shows that $v$ is an eigenvector of $T$ with eigenvalue $c$. So all roots of $m_{T}(x)$ are roots of the characteristic polynomial.

    Conversely, let $c$ be an eigenvalue of $T$. Say, $Tv = cv$ for some $v \neq 0$. Thus, $m_{T}(T)(v) = m(c)(v)$. But $m_{T}(T) = 0$ must mean that $0 = m(c)(v)$, and $m(c) = 0$. So every root of the characteristic polynomial is a root of the minimal polynomial.
\end{proof}

\textit{January 30th.}
\begin{proposition}
    If $\lambda$ is an eigenvalue of $T$, then $f(\lambda)$ is an eigenvalue of $f(T)$ for $f \in \F[x]$.
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}
\begin{proposition}
    Let $T: V \to V$ be a diagonizable operator. The minimal polynomial is the product of distinct linear factors, that is, if
    \begin{align*}
        \chi_{T}(x) = \prod_{i=1}^{k} (x-\lambda_{i})^{d_{i}}
    \end{align*}
    where the $\lambda_{i}$'s are the distinct eigenvalues, then
    \begin{align*}
        m_{T}(x) = \prod_{i=1}^{k} (x-\lambda_{i}).
    \end{align*}
\end{proposition}
\begin{proof}
    As $T$ is a diagonalizable operator, there exists a basis of $V$ consisting of eigenvectors of $T$, say $\mc{B} = (v_{1},v_{2},\ldots,v_{n})$. Note that $m_{T}(T)v_{i} = 0$ for all valid $i$. For each $v_{i} \in \mc{B}$, there exists a $\lambda_{i}$ such that $(T-\lambda_{i}I)v_{i} = 0$, which tells us
    \begin{equation}
        m_{T}(T) = (T-\lambda_{1}I)(T-\lambda_{2}I) \cdots (T-\lambda_{k}I) v_{i} = 0.
    \end{equation}
    Hence, $m_{T}(x)$ is an annihilator for $T$, and it is of minimal degree by the above theorem.
\end{proof}

\begin{theorem}[\eax{Cayley-Hamilton theorem}]
    Let $T: V \to V$ be a linear operator on a finite dimensional vector space $V$. If $\chi_{T}(x)$ is the characteristic polynomial of $T$, then $\chi_{T}(T) = 0$, that is, the characteristic polynomial annihilates $T$. Hence, the minimal polynomial of $T$ divides the characteristic polynomial.
\end{theorem}
\begin{proof}
    Let $\mc{B} = (v_{1},v_{2},\ldots,v_{n})$ be a basis of $V$, and let $A = (a_{ij})$ be the matrix of $T$ with respect to the basis $\mc{B}$. We have
    \begin{align}
        a_{1j}v_{1} + a_{2j}v_{2} + \ldots + a_{nj}v_{nj} &= Tv_{j} \notag \\
        \implies -a_{1j}v_{1}-a_{2j}v_{2} - \ldots + (T-a_{jj})v_{j} - a_{(j+1)(j)} v_{j+1} -\ldots - a_{nj}v_{n} &= 0.
    \end{align}
    This sysmte of equations can be written as
    \begin{equation}
        B_{n \times n} \begin{pmatrix}
            v_{1} \\ \vdots \\ v_{n}
        \end{pmatrix} = \begin{pmatrix}
            0 \\ \vdots \\ 0
        \end{pmatrix}
    \end{equation}
    where
    \begin{equation}
        B = \begin{pmatrix}
            T-a_{11}I & -a_{21} & \cdots & -a_{n1} \\
            -a_{12} & T-a_{22}I & \cdots & -a_{n2} \\
            \vdots & \vdots & \ddots & \vdots \\
            -a_{1n} & -a_{2n} & \cdots & T-a_{nn}I
        \end{pmatrix}.
    \end{equation}
    Therefore $\det{B} = \chi_{T}(T)$. It is enough to show that $\det{B} = 0$ as an operator, that is, to show $\det{B} (b_{i}) = 0$ for all $v_{i} \in \mc{B}$. Let $(\adj{B})_{ij} = c_{ij}$, and $(B)_{ij} = b_{ij}$. Note that
    \begin{equation*}
        \sum_{k=1}^{n} c_{ik} b_{kj} = \begin{cases}
            \det{B} &\text{ if } i = j,\\
            0 &\text{ if otherwise}.
        \end{cases}
    \end{equation*}
    Now,
    \begin{align}
        \sum_{j=1}^{n} b_{kj} v_{j} &= 0 \text{ for all } 1 \leq k \leq n \notag \\
        \implies \sum_{j=1}^{n} b_{kj} v_{j} &= 0 \notag.
    \end{align}
    Summing over all rows,
    \begin{align}
        \sum_{k=1}^{n} \left( \sum_{j=1}^{n} c_{ik} b_{kj} v_{j} \right) &= 0 \notag \\
        \implies \sum_{j=1}^{n} \left( \sum_{k=1}^{n} c_{ik} b_{kj} \right) v_{j} &= 0.
    \end{align}
    The left hand side is zero except for when $i = j$, in which case it is $\det{B}$---
    \begin{align}
        0 = \sum_{j=1}^{n} \left( \sum_{k=1}^{n} c_{ik} b_{kj} \right) v_{j} = (\det{B})v_{i}
    \end{align}
    which implies that the operator $\det{B}$ is zero on all the basis vectors, and hence it is the zero vecotr. Thus, since $\chi_{T}(T) = \det{B}$, $\chi_{T}(T)$ is also the zero operator.
\end{proof}

\textit{February 4th.}
\begin{proposition}
    If the minimal polynomial $m_{T}(x) \in \F[x]$ of a linear operator $T: V \to V$ splits into distinct linear factors, then $T$ is diagonalizable.
\end{proposition}
\begin{proof}
    Let $m_{T}(x) = (x-\lambda_{1})(x-\lambda_{2}) \cdots (x-\lambda_{k})$ where the $\lambda_{i}$'s are distinct. We are to show that $V = E_{\lambda_{1}} \oplus \ldots \oplus E_{\lambda_{k}}$. We wish to find polynomials $h_{1}(x), \ldots, h_{k}(x)$ such that
    \begin{enumerate}
        \item $h_{1}(x) + \ldots + h_{k}(x) = 1$,
        \item $(x-\lambda_{i}) \cdot h_{i}(x)$ is divisible by $m_{T}(x)$ for all $1 \leq i \leq k$.
    \end{enumerate}
    The second condition implies that $(T-\lambda_{i}I) \cdot h_{i}(T)$ is the zero operator. The first condition implies that $\sum_{i=1}^{k} h_{i}(T)(v) = v$. But the second condition again implies that $h_{i}(T)(v)$ is an eigenvector corresponding to $\lambda_{i}$, that is, $h_{i}(T)(v) \in E_{\lambda_{i}}$. If we can find these $h_{i}$'s satisfying the two conditions then we can say that $V$ is the direct sum of the eigenspaces.

    For $1 \leq i \leq k$, let $f_{i}(x) = \frac{m_{T}(x)}{(x-\lambda_{i})} = \prod_{j \neq i} (x-\lambda_{j})$. As the $\lambda_{i}$'s are distinct, the $f_{i}$'s are relatively prime, so there exist $g_{1}, \ldots, g_{k}$ such that
    \begin{equation}
        f_{1}(x)g_{1}(x) + \ldots + f_{k}(x) g_{k}(x) = 1.
    \end{equation}
    Let $h_{i}(x) = f_{i}(x) g_{i}(x)$ for all $1 \leq i \leq k$. Both the conditions hold, and the result follows.
\end{proof}

\begin{corollary}
    Let $T:V \to V$ be a linear operator on a finite dimensional complex vector space such that $T^{m} = I$ for some positive integral $m$. Then $T$ is diagonalizable.
\end{corollary}

\begin{proposition}
    Let $T:V \to V$ be  linear operator, and let $U$ be an invariant subspace of $T$, that is, $T(U) \subseteq U$ (or equivalently, $T(u) \in U$ for all $u \in U$). The minimal polynomial $m_{T|_{U}}(x)$ of the operator $T|_{U} : U \to U$ divides the minimal polynomial $m_{T}(x)$ of the operator $T: V \to V$ in $\F[x]$.
\end{proposition}
\begin{proof}
    Note that $m_{T}(T)(u) = 0$ for all $u \in U$ as $U \subseteq V$. Thus, $m_{T}(T) = 0$ on the subspace $U$. So $m_{T}(x)$ annihilates $T|_{U}$. So, as $m_{T|_{U}}(x)$ is the minimal polynomial of $T|_{U}$, it should divide all annihilators of $T|_{U}$ and thus divides $m_{T}(x)$ in $\F[x]$.
\end{proof}

\section{Triangularizability}
A similar definition works as in the case of diagonalizability.
\begin{definition}
    A linear operator $T: V \to V$ is said to be a \eax{triangularizable linear operator} if there exists a basis of $V$ with respect to which the matrix of $T$ is a triangular matrix, be it upper or lower.
\end{definition}
If our basis is $\mc{B} = (v_{1},v_{2},\ldots,v_{n})$, then we can show that $Tv_{k} \in \text{span}(v_{1},\ldots,v_{k})$.

\begin{theorem}
    A linear operator $T: V \to V$ is triangularizable if and only if the minimal polynomial splits into linear factors.
\end{theorem}
\begin{proof}
    Let $T:V \to V$ be triangularizable, that is, there exists a basis with respect to which the matrix of $T$ is a triangular matrix, with diagonal entries $\lambda_{1}, \ldots, \lambda_{n}$, say. Then the characteristic polynomial of $T$ is $(x-\lambda_{1})(x-\lambda_{2}) \cdots (x-\lambda_{n})$ where the $\lambda_{i}$'s are not necessarily distinct. But $m_{T}(x)$ divides $\chi_{T}(x)$, hence is again a product of linear factors.

    Conversely, let $m_{T}(x) = (x-\lambda_{1})(x-\lambda_{2}) \cdots (x-\lambda_{k})$ where the $\lambda_{i}$'s are not necessarily distinct. We prove by induction on the number of factors of $m_{T}(x)$. If $k = 1$, then $m_{T}(x) = x-\lambda_{1}$; as $m_{T}(T) = 0$, $T = \lambda_{1}I$, matrix of $T$ is the scalar matrix. Now let $k > 1$, and let the result hold for smaller positive integers. Let $U = \text{Im} (T-\lambda_{k}I)$. We find that $U$ is a proper subspace of $V$. Note that $U$ is an invariant subspace of $T$; if we let $u = (T-\lambda_{k}I)(v)$ for some $v \in V$, then
    \begin{equation}
        T(u) = T(T-\lambda_{k}I)(v) = (T-\lambda_{k}I)T(v) \in U.
    \end{equation}
    The minimal polynomial $m_{T|_{U}}(x)$ of $T|_{U}$ divides $m_{T}(x)$, and hence $m_{T|_{U}}(x) = (x-\alpha_{1}) \cdots (x-\alpha_{l})$, where $l \leq k$, and $\alpha_{1},\ldots,\alpha_{l} \in \{\lambda_{1},\ldots,\lambda_{k}\}$. By hypothesis, $T|_{U}$ is triangularizable. So there exists a basis of $U$, say $(u_{1},\ldots,u_{m})$ with respect to which the matrix of $T|_{U}$ is a triangular matrix. So $T|_{U}(u_{k}) \in \text{span}(u_{1},\ldots,u_{k})$. Extend this to a basis $\mc{B} = (u_{1},\ldots,u_{m},v_{m+1},\ldots,v_{n})$. If we rewrite $Tv_{j} = (T-\lambda_{k}I)v_{j} + \lambda_{k}Iv_{j}$, we see that $(T-\lambda_{k}I)v_{j} \in U = \text{span}(u_{1},\ldots,u_{m})$, and $Tv_{j} \in \text{span}(u_{1},\ldots,u_{m},v_{j})$; the matrix of $T$ with respect to $\mc{B}$ is a triangular matrix. 
\end{proof}
\begin{corollary}
    Every operator $T:V \to V$, where $V$ is a complex finite dimensional vector space, is triangularizable.
\end{corollary}

\subsection{Determinant of Partitioned Matrices}

\begin{proposition}
    Let $\Gamma = \begin{pmatrix}
        A & O \\ O & I
    \end{pmatrix}$ or $\Gamma = \begin{pmatrix}
        I & O \\ O & A
    \end{pmatrix}$, where $A$ is a square matrix. Then we necessarily have $\det{\Gamma} = \det{A}$.
\end{proposition}
\begin{proof}
    Let $\Gamma$ be of order $(n+1) \times (n+1)$ and $A$ be of order $n \times n$. By definition,
    \begin{equation}
        \det{\Gamma} = \sum_{\sigma \in S_{n+1}} \epsilon(\sigma) g_{1 \sigma(1)} \cdots g_{(n+1) \sigma(n+1)}.
    \end{equation}
    Note that $g_{(n+1)\sigma(n+1)}$ is 1 if $\sigma(n+1) = n+1$, and $0$ otherwise. Also, $\epsilon(\sigma)$ remains the same when $\sigma$ is considered to be an element of $S_{n}$. Thus,
    \begin{equation}
        \det \Gamma = \sum_{\sigma \in S_{n}} \epsilon(\sigma) g_{1 \sigma(1)} \cdots g_{n \sigma(n)} = \det A.
    \end{equation}
    Iterating this, we get the desired result. A similar proof works for the other type of matrix stated.
\end{proof}

\begin{proposition}
    Let $\Gamma = \begin{pmatrix}
        A & B \\ O & D
    \end{pmatrix}$ where $A$ and $D$ are square matrices. Then we necessarily have $\det \Gamma = \det{A} \cdot \det{D}$.
\end{proposition}
\begin{proof}
    Let the orders be $A_{k \times k}$, $D_{l \times l}$, $B_{k \times l}$ and $O_{l \times k}$. Note that $\Gamma$ can be broken up as
    \begin{equation}
        \Gamma = \begin{pmatrix}
            I_{k} & O_{k \times l} \\
            O_{l \times k} & D_{l \times l}
        \end{pmatrix} \begin{pmatrix}
            I_{k} & B_{k \times l} \\
            O_{l \times k} & I_{l}
        \end{pmatrix} \begin{pmatrix}
            A_{k \times k} & O_{k \times l} \\
            O_{l \times k} & I_{l}
        \end{pmatrix}.
    \end{equation}
    The determinant is multiplicative, so $\det \Gamma = \det D \cdot \det A$ as the determinant of the middle matrix can be shown to be 1.
\end{proof}

\begin{proposition}
    Let $\Gamma = \begin{pmatrix}
        A & B \\ C & D
    \end{pmatrix}$ where $A$ and $D$ are square matrices. If $A$ is invertible, then we necessarily have $\det \Gamma = \det A \cdot \det (D - CA^{-1}B)$.
\end{proposition}
\begin{proof}
    Again, we break down $\Gamma$.
    \begin{equation}
        \Gamma = \begin{pmatrix}
            I & O \\ CA^{-1} & I
        \end{pmatrix} \begin{pmatrix}
            A & O \\ O & D-CA^{-1}B
        \end{pmatrix} \begin{pmatrix}
            I & A^{-1}B \\ O & I
        \end{pmatrix}.
    \end{equation}
    From here, it is clear that $\det \Gamma = \det{A} \cdot \det(D-CA^{-1}B)$.
\end{proof}

\begin{proposition}
    Let $\Gamma = \begin{pmatrix}
        A & B \\ C & D
    \end{pmatrix}$ where $A$ and $D$ are square matrices. If $D$ is invertible, then we necessarily have $\det \Gamma = \det D \cdot \det (A - BD^{-1}C)$.
\end{proposition}
\begin{proof}
    Yet again, we break down $\Gamma$.
    \begin{equation}
        \Gamma = \begin{pmatrix}
            I & BD^{-1} \\ O & I
        \end{pmatrix} \begin{pmatrix}
            A-BD^{-1}C & O \\ O & D
        \end{pmatrix} \begin{pmatrix}
            I & O \\ D^{-1}C & I
        \end{pmatrix}.
    \end{equation}
    From here, it is clear that $\det \Gamma = \det{D} \cdot \det(A-BD^{-1}C)$.
\end{proof}

\section{On the Characteristic and Minimal Polynomials}
\textit{February 6th.}
\begin{theorem}
    Let $A$ be a $m \times n$ matrix and $B$ be a $n \times m$ matrix with $m \leq n$. Then,
    \begin{equation}
        \chi_{BA}(x) = x^{n-m} \chi_{AB}(x).
    \end{equation}
\end{theorem}
\begin{proof}
    Note that there exist non-singular matrices $P$ and $Q$ such that $PAQ = \begin{pmatrix}
        I_{r} & O \\ O & O
    \end{pmatrix}$, where $r$ is the rank of $A$. Partition $Q^{-1}BP^{-1}$ as $\begin{pmatrix}
        C & D \\ E & G
    \end{pmatrix}$ where $C$ is of order $r \times r$, with the other submatrices being of appropriate order. Then,
    \begin{equation}
        PABP^{-1} = PAQQ^{-1}BP^{-1} = \begin{pmatrix}
            I_{r} & O \\ O & O
        \end{pmatrix} \begin{pmatrix}
            C_{r \times r} & D \\ E & G
        \end{pmatrix} = \begin{pmatrix}
            C & D \\ O & O
        \end{pmatrix}.
    \end{equation}
    Similarly,
    \begin{equation}
        Q^{-1}BAQ = Q^{-1}BP^{-1}PAQ = \begin{pmatrix}
            C_{r \times r} & D \\ E & G
        \end{pmatrix} \begin{pmatrix}
            I_{r} & O \\ O & O
        \end{pmatrix} = \begin{pmatrix}
            C & O \\ E & O
        \end{pmatrix}.
    \end{equation}
    Thus,
    \begin{equation}
        \chi_{AB}(x) = \chi_{PABP^{-1}}(x) = \det \begin{pmatrix}
            xI_{r} - C & -D \\ O & xI
        \end{pmatrix} = x^{m-r} \det(xI-C)
    \end{equation}
    and
    \begin{equation}
        \chi_{BA}(x) = \chi_{Q^{-1}BAQ}(x) = \det \begin{pmatrix}
            xI - C & O \\ -E & xI
        \end{pmatrix} = x^{n-r} \det(xI-C)
    \end{equation}
    which tells us that $\chi_{BA}(x) = x^{n-m} \chi_{AB}(x)$.
\end{proof}

\begin{enumerate}
    \item Suppose $T:V \to V$ and $U \subseteq V$.
    \begin{enumerate}
        \item If $U \subseteq \ker T$, then $U$ is $T$-invariant.
        \item If $T(V) \subseteq U$, then $U$ is $T$-invariant.
    \end{enumerate}
    \item If $V_{1},\ldots,V_{m}$ are $T$-invariant subspaces, then $V_{1} + \ldots + V_{m}$ is $T$-invariant.
    \item Let $P:V \to V$ be a linear operator such that $P^{2} = P$;  then the eigenvalues of $P$ are 0 or 1.
    \item Let $T:V \to V$ be an invertible operator. Then $\lambda$ is an eigenvalue of $T$ if and only if $\frac{1}{\lambda}$ is an eigenvalue of $T^{-1}$.
    \item Let $T:V \to V$, with $0 \neq v \in V$. Then $W = \text{span}(v, Tv, T^{2}v, \ldots)$ is $T$-invariant, and is the smallest $T$-invariant subspace of $V$ containing $v$.
    \item Let $T:V \to V$ and $\text{rank}T = k \leq n$, where $n$ is the dimension of $V$. Then $T$ has at most $k+1$ distinct eigenvalues.
    \item 
\end{enumerate}

\chapter{INNER PRODUCT SPACES}
\textit{February 13th.}

\section{An Introduction}
The function $\ip{,} : \R^{n} \times \R^{n} \to \R$ defined as
\begin{equation}
    \ip{X,Y} = x_{1}y_{1} + x_{2}y_{2} + \ldots + x_{n}y_{n}
\end{equation}
is called an inner product. Specifically, this is the dot product on the vector space over the reals. It satisfies the following properties;
\begin{enumerate}
    \item $\ip{X,X} \geq 0$ for all $X \in \R^{n}$.
    \item $\ip{X,X} = 0$ if and only if $X = 0$.
    \item $\ip{X,Y} = \ip{Y,X}$.
    \item $\ip{X_{1}+X_{2},Y} = \ip{X_{1},Y} + \ip{X_{2},Y}$.
    \item $\ip{\alpha X, Y} = \alpha \ip{X,Y}$.
\end{enumerate}
In $\C^{n}$, we have the product
\begin{equation}
    \ip{Z,W} = z_{1}\conj{w_{1}} + z_{2}\conj{w_{2}} + \ldots + z_{n}\conj{w_{n}}.
\end{equation}
This satisfies the properties---
\begin{enumerate}
    \item $\ip{Z,Z} \geq 0$.
    \item $\ip{Z,Z} = 0$ if and only if $Z = 0$.
    \item $\ip{Z,W} = \conj{\ip{W,Z}}$.
    \item $\ip{Z_{1}+Z_{2},W} = \ip{Z_{1},W} + \ip{Z_{2},W}$.
    \item $\ip{\alpha Z, W} = \alpha \ip{Z,W}$.
\end{enumerate}
These properties are, respectively, called the positivity, the definiteness, the conjugate symmetry, the additivity, and the homogeneity of the inner product over the complex vector space. We now define a general inner product.\\

Let the underlying field be either $\R$ or $\C$, and let $V$ be a vector space over this field. An \eax{inner product} on $V$ is simply a function $\ip{,}:V \times V \to \F$ such that it satisfies the following properties for all $v,u,v_{1},v_{2} \in V$ and $\alpha \in \F$---
\begin{enumerate}
    \item $\ip{v,v} \geq 0$,
    \item $\ip{v,v} = 0$ if and only if $v = 0$,
    \item $\ip{v_{1}+v_{2},u} = \ip{v_{1},u} + \ip{v_{2},u}$,
    \item $\ip{\alpha v, u} = \alpha \ip{v,u}$, and
    \item $\ip{v,u} = \conj{\ip{u,v}}$.
\end{enumerate}
A vector space over $\F$, $\F$ being either $\R$ or $\C$, is called an \eax{inner product space} if $V$ is equipped with a valid inner product. As seen earlier, on $\R^{n}$, the usual dot product makes $\R^{n}$ an inner product space. As another example, if $V$ is the space of all real valued continuous function $f:(-1,1) \to \R$, then the inner product on here can be defined as
\begin{equation}
    \ip{f,g} = \int_{-1}^{1} f(x)g(x) dx.
\end{equation}
On $\C^{m \times n}$, we can define the inner product as
\begin{align}
    \ip{A,B} = \tr(B^{\ast}A).
\end{align}
Every inner product $\ip{u,v}$ for any vector space $V$ will look like
\begin{align}
    \ip{u,v} = Y^{\ast} AX,
\end{align}
where $Y$ and $X$ are the coordinate vectors of $v$ and $u$ with respect to some basis $\mc{B}$. This will be proved later.

For an inner product space $V$, the following properties may be derived from the basic properties;
\begin{enumerate}
    \item $\ip{0,v} = 0$ for all $v \in V$.
    \item Fix $v \in V$. Define $f_{v}:V \to \F$ as $u \mapsto \ip{u,v}$. Then $f_{v}$ is a linear mapping from the space $V$ to the space $\F$ for any $v \in V$.
    \item Let $v = \alpha_{1}v_{2} + \alpha_{2}v_{2} + \ldots + \alpha_{k}v_{k}$ and $u = \beta_{1}u_{1} + \beta_{2}u_{2} + \ldots + \beta_{l}u_{l}$ where $u,v,u_{j},v_{i} \in V$ and $\alpha_{i},\beta_{j} \in \F$. Then,
    \begin{align}
        \ip{v,u} = \sum_{i=1}^{k} \sum_{j=1}^{l} \alpha_{i}\conj{\beta_{j}} \ip{v_{i},u_{j}}.
    \end{align}
\end{enumerate}

\section{The Notion of Length and Orthogonality}
Let $(V,\ip{,})$ be an inner product space. We define the \eax{norm} of a vector $v \in V$, denoted by $\norm{v}$, as
\begin{equation}
    \norm{v} = \sqrt{\ip{v,v}}.
\end{equation}
For $V$ being either $\R^{n}$ or $\C^{n}$, we can easily verify that the norm becomes the usual Euclidean length of a vector. Note that $\norm{v} = 0$ if and only if $v$ is the zero vector in $V$. It can also be shown that $\norm{\lambda v} = \abs{\lambda} \norm{v}$ for some $\lambda \in \F$.

\begin{definition}
    We say that two vectors $v,w \in V$ are \eax{orthogonal vectors} if $\ip{v,w} = 0$.
\end{definition}
Note that the zero vector is orthogonal to every vector in the vector space, even itself; in fact, it is the only vector othogonal to itself. We can also make sense of a Pythogrean theorem here. If $u,v \in V$ are orthogonal, then we can show that
\begin{align}
    \norm{u+v}^{2} = \norm{u}^{2} + \norm{v}^{2}.
\end{align}
Given two vectos $u,v \in V$, we can write $u$ as the sum of a scalar multiple of $v$, say $cv$, and a vector $w$ such that $\ip{w,v} = 0$. If we rewrite $u$ as $u = cv + (u-cv)$, and impose that $\ip{u-cv,v} = 0$, then we get $c = \dfrac{\ip{u,v}}{\norm{v}^{2}}$ fulfulling our conditions.

We also have a \eax{Cauchy-Schwarz} inequality. It says that for any $u,v \in V$, then
\begin{align}
    \abs{\ip{u,v}} \leq \norm{u}\norm{v}
\end{align}
and equality holds if and only if one of the vectors is a scalar multile of the other.
\begin{proof}
    If either one of the vectors is the zero vector, both sides are just zero. Hence, assume that neither vector is zero, and note that we can write
    \begin{equation}
        u = \frac{\ip{u,v}}{\norm{v}^{2}}v + w
    \end{equation}
    where $\ip{w,v} = 0$. Thus,
    \begin{align}
        \norm{u}^{2} = \ip{\frac{\ip{u,v}}{\norm{v}^{2}}v + w, \frac{\ip{u,v}}{\norm{v}^{2}}v + w} = \frac{\ip{u,v}\conj{\ip{u,v}}}{\norm{v}^{4}}\ip{v,v} + \ip{w,w} = \frac{\abs{\ip{u,v}}^{2}}{\norm{v}^{2}} + \ip{w,w} \geq \frac{\abs{\ip{u,v}}^{2}}{\norm{v}^{2}}.
    \end{align}
    The inequality follows. The equality is left as an exercise to the reader.
\end{proof}

Let $V$ be an inner product space, and let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of $V$. Let $X = (x_{1},\ldots,x_{n})^{t}$ and $Y = (y_{1},\ldots,y_{n})^{t}$ be the coordinate vectors of $v,w \in V$, respectively, with respect to the basis $\mc{B}$ Then,
\begin{align}
    \ip{v,w} = Y^{\ast}AX \text{ where } A = (a_{ij}) \text{ and } a_{ij} = \ip{v_{j},v_{i}}.
\end{align}
This can be seen since
\begin{align}
    \ip{v,w} = \ip{\sum_{i=1}^{n} x_{i}v_{i}, \sum_{j=1}^{n} y_{j}v_{j}} = \sum_{i=1}^{n} \sum_{j=1}^{n} x_{i} \conj{y_{j}} \ip{v_{i},v_{j}}.
\end{align}
Conversely, let $V$ be a vector space of dimension $n$ and $\mc{B}$ be a basis of $V$. Then defining $\ip{v,w} = Y{^t}AX$, where $A$ is of order $n \times n$ satisfying $A^{\ast} = A$, gives an inner product

\textit{March 4th.}
\begin{theorem}[The \eax{triangle inequaity}]
    For all $v,w \in V$, $\norm{v+w} \leq \norm{v} + \norm{w}$ holds.
\end{theorem}
\begin{proof}
    We square the left side to get
    \begin{align}
        \norm{v+w}^{2} &= \ip{v+w,v+w} = \ip{v,v} + \ip{v,w} + \ip{w,v} + \ip{w,w} \notag \\
        &= \norm{v}^{2} + 2\text{Re}\ip{v,w} + \norm{w}^{2} \leq \norm{v}^{2} + 2\abs{\ip{v,w}} + \norm{w}^{2} \notag \\
        &\leq \norm{v}^{2} + 2\norm{v}\norm{w} + \norm{w}^{2} = (\norm{v}+\norm{w})^{2}.
    \end{align}
\end{proof}

\begin{theorem}[The \eax{parallelogram law}]
    For all $x,y \in V$, $\norm{x+y}^{2} + \norm{x-y}^{2} = 2\norm{x}^{2} + 2\norm{y}^{2}$ holds. 
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\section{Orthogonality and Orthonormality}
Note that if $v$ is orthogonal to $w$ and $z$, then it is orthogonal to both $w+z$ and $\alpha w$ for $v,w,z \in V$ and $\alpha \in \F$. Thus, if $x \in \text{span}\{w,z\}$, then $v$ is also orthogonal to $x$. The entire subspace spanned by $w$ and $z$ is orthogonal to $v$.

\begin{proposition}
    A set of orthogonal vectors, say $(S = \{v_{1},v_{2},\ldots,v_{n}\}$ with $\ip{v_{i},v_{j}} = 0$ for all $i \neq j$, is a linearly independent set, provided that $S$ does not contain the zero vector.
\end{proposition}
\begin{proof}
    Let $\alpha_{1}v_{1} + \alpha_{2}v_{2} + \ldots + \alpha_{n}v_{n} = 0$. For any valid $j$, note that
    \begin{align*}
        0 = \ip{0,v_{j}} = \ip{\sum_{i=1}^{n} \alpha_{i}v_{i}, v_{j}} = \alpha_{j}\ip{v_{j},v_{j}}
    \end{align*}
    which must imply that $\alpha_{j} = 0$ since $v_{j}$ is not the zero vector.
\end{proof}

\begin{definition}
    A set of orthogonal vectors is said to be an \eax{orthonormal set of vectors} if the norm of every vector in the set is unity.
\end{definition}

We note that if $S = \{v_{1},v_{2},\ldots,v_{n}\}$ is an orthogonal set, then $S' = \{\frac{v_{1}}{\norm{v_{1}}},\frac{v_{2}}{\norm{v_{2}}},\ldots,\frac{v_{n}}{\norm{v_{n}}}\}$ is an orthonormal set, provided $v_{i}$ is never the zero vector.

In the vector space $\R^{n}$, an orthonormal basis forms the columns of an invertible matrix $A$ such that $A^{t}A = I_{n}$. We take this as our definition of an orthogonal matrix.

\begin{definition}
    A invertible matrix $A$ is said to be an \eax{orthogonal matrix} if $A^{t} = A^{-1}$, that is, $A^{t}A = AA^{t} = I_{n}$.
\end{definition}

\begin{proposition}
    Let $\mc{B} = \{v_{1},\ldots,v_{n}\}$ be an orthonormal basis of $V$, where $V$ is an inner product space. Then for every $v \in V$, $v = \sum_{i=1}^{n} \ip{v,v_{i}} v_{i}$.
\end{proposition}
\begin{proof}
    We simply have $\ip{v,v_{j}} = \ip{\sum_{i=1}^{n} \alpha_{i}v_{i},v_{j}} = \alpha_{j}\ip{v_{j},v_{j}} = \alpha_{j}$, for some $v = \alpha_{1}v_{1} + \alpha_{2}v_{2} + \ldots + \alpha_{n}v_{n}$ with $\alpha_{i} \in \F$.
\end{proof}

\textit{March 6th.}
Here are some properties;
\begin{itemize}
    \item Two vectors $x$ and $y$ in a real inner product space are orthogonal if and only if $\norm{x+y}^{2} = \norm{x}^{2} + \norm{y}^{2}$.
    \item Two vectors $x$ and $y$ in a complex inner product space are orthogonal if and only if $\norm{\alpha x + \beta y}^{2} = \norm{\alpha x}^{2} + \norm{\beta y}^{2}$ for all pairs $\alpha$ and $\beta$ in $\C$.
    \item If $x$ and $y$ are vectors in a real inner product space, and if $\norm{x} = \norm{y}$, then $x-y$ and $x+y$ are orthogonal.
\end{itemize}

\subsection{The Orthogonal Complement}
\begin{definition}
    In any inner product space, the \eax{distance between two vectors} $x$ and $y$ is defined as
    \begin{align*}
        d(x,y) = \norm{x-y} = \sqrt{\ip{x-y,x-y}}.
    \end{align*}
\end{definition}
\begin{definition}
    In any inner product space, the \eax{angle between two vectors} $x$ and $y$ is defined to be $\theta \in [0,\pi]$ such that
    \begin{align*}
        \cos \theta = \frac{\ip{x,y}}{\norm{x}\norm{y}}.
    \end{align*}
\end{definition}

\begin{definition}
    Given a subspace $W \subseteq V$, the \eax{orthogonal complement} of $W$ is defined as
    \begin{align*}
        W^{\perp} = \{v \in V : \ip{v,w} = 0 \text{ for all } w \in W\}.
    \end{align*}
\end{definition}

\begin{proposition}
    Let $W$ be a subspace of $V$. Then every vector $v \in V$ can be written as $v = w + w^{\perp}$ where $w \in W$ and $w^{\perp} \in W^{\perp}$. Also, $w$ is characterize by the property that it is the nearmost point in $W$ to $V$.
\end{proposition}
\begin{proof}
    Fix $\{w_{1},\ldots,w_{k}\}$ be an orthonormal basis of $W$. Let $w = \sum_{i=1}^{k} \ip{v,w_{i}} w_{i} \in W$. Let $w^{\perp} = v-w$. Then, for some $1 \leq j \leq k$,
    \begin{align}
        \ip{v-w,w_{j}} = \ip{v,w_{j}} - \ip{\sum_{i=1}^{k} \ip{v,w_{i}}w_{i},w_{j}} = \ip{v,w_{j}} - \ip{v,w_{j}}\ip{w_{j},w_{j}} = \ip{v,w_{j}} - \ip{v,w_{j}} = 0.
    \end{align}
    The second part of the proposition is left as an exercise to the reader.
\end{proof}


\begin{proposition}
    Let $V$ be an inner product space, with $W$ as a subspace. The following hold true:
    \begin{itemize}
        \item $W^{\perp}$ is a subspace,
        \item $W \cap W^{\perp} = \{0\}$,
        \item $W + W^{\perp} = V$,
        \item $(W^{\perp})^{\perp} = W$
    \end{itemize}
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{proposition}
    Let $U,W$ be subspaces of $V$. The following hold true:
    \begin{itemize}
        \item If $U \subseteq W$, then $W^{\perp} \subseteq U^{\perp}$,
        \item $(U+W)^{\perp} = U^{\perp} \cap W^{\perp}$,
        \item $(U \cap W)^{\perp} = U^{\perp} + W^{\perp}$. 
    \end{itemize}
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{definition}
    Let $W$ be a subspace of an inner product space $V$, and let $v \in V$ such that $v = w + w^{\perp}$ where $w \in W$ and $w^{\perp} \in W^{\perp}$. Then $w$ is called the \eax{orthogonal projection} of $v$ into $W$ along $W^{\perp}$.
\end{definition}

Take $V$ to be $\R^{n}$ or $\C^{n}$ with the standard inner product(s).

\begin{theorem}
    For any real matrix $A$, $\emph{\nullsp}{A} = (\emph{\rowsp}{A})^{\perp}$. For any complex matrix $A$, $\emph{\nullsp}{A} = (\emph{\colsp}{A^{\ast}})^{\perp}$.
\end{theorem}
\begin{proof}
    Note that it is enough to prove the second part. Let $X \in (\colsp{A^{\ast}})^{\perp}$ for a complex matrix $A$. $X$ is in this space if and only if $\ip{X,A^{\ast}Y} = 0$ for all $Y \in \C^{n}$.
    \begin{align}
        \Leftrightarrow (A^{\ast}Y)^{\ast}X = 0 \Leftrightarrow Y^{\ast}AX = 0 \Leftrightarrow AX = 0 \Leftrightarrow X \in \nullsp{A}.
    \end{align}
\end{proof}

\begin{definition}
    Let the ground field $\F$ be $\C$ or $\R$, and let $W$ be a subspace of $\F^{n}$. The \eax{orthogonal projector} into $W$ is the matrix $P_{n \times n}$ such that for all $v \in \F^{n}$, $Pv$ is the orthogonal projection of $v$ into $W$. Conversely, a matrix $Q_{n \times n}$ is said to be an orthogonal projector if it is the orthogonal projector into some subspace $W$ of $\F^{n}$.
\end{definition}

\begin{proposition}
    In the previous definition, $F^{n}$ can be wrriten as $\emph{\colsp}(Q) \oplus \emph{\colsp}(I-Q)$.
\end{proposition}
\begin{proof}
    It is enough to show that $\colsp(I-Q) = \colsp Q^{\perp}$.
\end{proof}

\begin{theorem}
    For an $n \times n$ matrix $Q$, the following are equivalent;
    \begin{itemize}
        \item $Q$ is an orthogonal projector,
        \item $Q^{\ast}Q = Q$,
        \item $Q^{\ast} Q$ and $Q^{2} = Q$.
    \end{itemize}
\end{theorem}
\begin{proof}
    If we first assume that $Q$ is an orthogonal projector, then $v - Qv$ is orthogonal to every vector in $\colsp{Q}$ and any vector $v \in \F^{n}$. So, for all $u,v \in \F^{n}$,
    \begin{align}
        \ip{Qu,(I-Q)v} = 0 \implies v^{\ast} (I-Q)^{\ast} Qu = 0 \implies (I-Q)^{\ast} Q = 0 \implies Q = Q^{\ast}Q.
    \end{align}
    The converse is also true. So, (i) and (ii) imply each other. Showing that (ii) and (iii) are equivalent is trivial.
\end{proof}

\subsection{Orthogonal and Unitary Matrices}
We have already seen orthogonal matrices.

\begin{definition}
    A complex square matrix is called a \eax{unitary matrix} such that $AA^{\ast} = A^{\ast}A = I_{n}$.
\end{definition}

It can be shown that $2 \times 2$ orthogonal matrices (and unitary matrices) are of the form
\begin{align}
    \begin{pmatrix}
        \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta
    \end{pmatrix} \text{ or }
    \begin{pmatrix}
        \cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
    \end{pmatrix} \text{ for } \theta \in [0,2\pi).
\end{align}
The first kind represents rotation by the angle $\theta$, and the second kind represents reflection along the line making an angle of $\frac{\theta}{2}$.

\textit{March 11th.}\\
Note that the columns, as well as the rows, of a unitary matrix form an orthonormal basis of $\C^{n}$; the columns, as well as the rows, of an orthogonal matrix from an orthonormal basis of $\R^{n}$.

\begin{proposition}
    If $A,B$ are unitary (or orthogonal), then so are $AB,A^{t},\bar{A},A^{-1}$.
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{proposition}
    The determinant of a unitary (or orthogonal) matrix has modulus $1$.
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{theorem}
    Let $A$ be a $n \times n$ matrix. The following are then equivalent:
    \begin{itemize}
        \item $A$ is unitary,
        \item $\ip{AX,AY} = \ip{X,Y}$ for all $X,Y \in \C^{n}$,
        \item $\norm{AX} = \norm{X}$ for all $X \in \C^{n}$,
        \item $\norm{AX} = 1$ wherever $\norm{X} = 1$,
        \item $\norm{AX-AY} = \norm{X-Y}$ for all $X,Y \in \C^{n}$,
        \item $(Ax_{1},\ldots,Ax_{n})$ is an orthonormal basis of $\C^{n}$ whenever $(x_{1},\ldots,x_{n})$ is an orthonormal basis of $\C^{n}$.
    \end{itemize}
\end{theorem}
\begin{proof}
    (i) implies (ii) is easy since $\ip{AX,AY} = (AY)^{\ast}AX = Y^{\ast}A^{\ast}AX = Y^{\ast}X = \ip{X,Y}$. For (ii) implies (iii), we have
    \begin{align}
        \ip{AX,AX} = \ip{X,X} \implies \norm{AX}^{2} = \norm{X}^{2} \implies \norm{AX} = \norm{X}.
    \end{align}
    We show (iii) implies (i); we see that $1 = \norm{e_{i}^{2}} = \norm{Ae_{i}}^{2} = \ip{Ae_{i},Ae_{i}} = e_{i}^{\ast}A^{\ast}Ae_{i} = e_{i}^{\ast}Be_{i} = b_{ii}.$ where we have used $A^{\ast}A = B = (b_{ij})$. So we infer that $B = A^{\ast}A$ has 1's on the diagonal. Let $x = e_{i}+e_{j}$ for $i \neq j$. Then,
    \begin{align}
        2 = \norm{x}^{2} = \norm{Ax}^{2} = (e_{i}+e_{j})^{t}B(e_{i}+e_{j}) = b_{ii} + b_{ij} + b_{ji} + b_{jj} \implies b_{ij}+b_{ji} = 0 \text{ for } i \neq j.
    \end{align}
    If $A$ is real, $B$ is symmetric as $B = A^{t}A$ so it follows that $b_{ij} = 0$ for all $i \neq j$. This shows that $B$ is identity and $A$ is orthogonal. If $A$ is complex, take $x = e_{k}i + e_{j}$ for $k \neq j$. Then,
    \begin{align}
        \norm{Ax}^{2} = x^{\ast}Bx = b_{kk}-b_{kj}i+b_{jk}i+b_{jj}.
    \end{align}
    A similar argument shows that $b_{kj} = b_{jk}$, and the previous equation shows that $b_{kj} = -b_{jk}$. Thus, for $k \neq j$, $b_{kj} = 0$.

    (iii) implies (iv) is trivial. For (iv) implies (iii), simply consider $\frac{X}{\norm{X}}$ for some non-zero $X \in \C^{n}$. For (iii) implies (v), we consider $X-Y \in \C^{n}$. For (v) implies (iii), we consider $Y = 0$.

    (ii) implies (vi) can be done by considering $\ip{Ax_{i},Ax_{j}} = 0$ for $i \neq j$ for a orthonormal basis $(x_{1},\ldots,x_{n})$. For (vi) implies (iv), let $x_{1}$ be a vector such that $\norm{x_{1}} = 1$. Extend $x_{1}$ to an orthonormal basis, say $(x_{1},x_{2},\ldots,x_{n})$. (vi) says that $(Ax_{1},Ax_{2},\ldots,Ax_{n})$ is also an orthonormal basis, so $\norm{Ax_{1}} = 1$ for any $x_{1}$.
\end{proof}

It is of note to keep in mind that the set of complex unitary matrices and the set of complex orthogonal matrices are different sets. However, the set of real unitary matrices and the set of real orthogonal matrices are the same set.

\begin{proposition}
    Let $U$ be an $n \times n$ unitary matrix. Then $\abs{\lambda} = 1$ for all eigenvalues $\lambda$ of $U$.
\end{proposition}
\begin{proof}
    If $v$ is an associated eigenvector, we simply have
    \begin{align}
        \abs{\lambda} = \abs{\lambda} \norm{\frac{v}{\norm{v}}} = \norm{\frac{\lambda v}{\norm{v}}} = \frac{1}{\norm{v}} \norm{Uv} = \frac{1}{\norm{v}}\norm{v} = 1.
    \end{align}
\end{proof}

\begin{theorem}[\eax{Schur's decomposition}]
    Let $\lambda_{1},\ldots,\lambda_{n}$ be the eigenvalues of $A \in M_{n}(\C)$. Then there exists a unitary matrix $U \in M_{n}(\C)$ such that $U^{\ast}AU$ is an upper triangular matrix, with the eigenvalues on the diagonal.
\end{theorem}
\begin{proof}
    Let us assume that the result holds for any $k \times k$ matrix with $k < n$. Let $x_{1}$ be a unit eigenvector of $A$ corresponding to $\lambda_{1}$. Extend $x_{1}$ to an orthonormal basis of $\C^{n}$, say, $S = (x_{1},x_{2},\ldots,x_{n})$. Treating $x_{1},\ldots,x_{n}$ as columns of a matrix $S$, $S$ is unitary. We then have
    \begin{align}
        AS = (Ax_{1},Ax_{2},\ldots,Ax_{n}) = (\lambda_{1}x_{1},Ax_{2},\ldots,Ax_{n}) = S(u, S^{-1}Ax_{2},\ldots,S^{-1}Ax_{n})
    \end{align}
    where $u = \begin{pmatrix}
        \lambda_{1} & 0 & \cdots & 0
    \end{pmatrix}^{t}$. This shows that $Su = \lambda_{1}x_{1} + 0x_{2} + \ldots + 0x_{n}$. This implies that
    \begin{align}
        S^{\ast}AS = (u,S^{-1}Ax_{2},S^{-1}Ax_{3},\ldots,S^{-1}Ax_{n}) = \begin{pmatrix}
            \lambda_{1} & v \\ O & B_{(n-1) \times (n-1)}
        \end{pmatrix}
    \end{align}
    where $v$ is a $n-1$ row vector. Apply induction hypothesis on the $(n-1) \times (n-1)$ matrix $B$. Let $T$ be a unitary matrix such that $T^{\ast}BT$ is an upper triangular matrix with $\mu_{1},\ldots,\mu_{n-1}$ on the diagonal. If we let $U = S_{n \times n} \begin{pmatrix}
        1 & O \\ O & T
    \end{pmatrix}$ then $U$ is a unitary matrix as it is a product of two unitary matrices. We then have
    \begin{align}
        U^{\ast}AU = \begin{pmatrix}
            1 & O \\ O & T^{\ast}
        \end{pmatrix} S^{\ast} AS \begin{pmatrix}
            1 & O \\ O & T
        \end{pmatrix} = \begin{pmatrix}
            1 & O \\ O & T^{\ast}
        \end{pmatrix} \begin{pmatrix}
            \lambda_{1} & v \\ O & B
        \end{pmatrix} \begin{pmatrix}
            1 & O \\ O & T
        \end{pmatrix} = \begin{pmatrix}
            \lambda_{1} & \cdots \\ O & T^{\ast}BT
        \end{pmatrix}
    \end{align}
\end{proof}

\begin{definition}
    An $n \times n$ matrix $A \in M_{n}(\C)$ is termed a \eax{normal matrix} if $AA^{\ast} = A^{\ast}A$.
\end{definition}

\textit{March 13th.}

\begin{theorem}[\eax{Spectral decomposition}]
    Let $A \in M_{n}(\C)$ with eigenvalues $\lambda_{1},\ldots,\lambda_{n}$. Then $A$ is normal if and only if $A$ is unitarily diagonalizable, that is, there exists a unitary matrix $U$ such that $U^{\ast}AU = \emph{\text{diag}}(\lambda_{1},\ldots,\lambda_{n})$. In particular, $A$ is Hermitian if and only if the $\lambda_{i}$'s are real.
\end{theorem}
\begin{proof}
    Let $A$ be normal. We then have
    \begin{align}
        (U^{\ast}AU)^{\ast} U^{\ast}AU = U^{\ast}AUU^{\ast}AU = U^{\ast}A^{\ast}AU \text{ and } (U^{\ast}AU)(U^{\ast}AU)^{\ast} = U^{\ast}AA^{\ast}U.
    \end{align}
    If $A$ is normal then $U^{\ast}AU$ is also normal, where $U$ is unitary. But for an upper triangular matrix to be normal, it must be a diagonal matrix. The converse is easy since $A = UDU^{\ast}$ for some diagonal matrix $D$ and $U$ unitary.

    If $A$ is normal, then by spectral decomposition, there exists a unitary matrix $U$ such that $U^{\ast}AU$ is diagonal, say $D$. As $U$ is invertible, $A = UDU^{\ast} = UD^{\ast}U^{\ast} = A^{\ast}$ implies that $D = D^{\ast}$, or $\lambda_{i} = \overline{\lambda}_{i}$. This is only possible if these $\lambda_{i}$'s are real. Conversely, let $A$ have all real eigenvalues and let $A$ be a normal matrix. There then exists a unitary matrix $U$ such that $U^{\ast}AU = D$, a diagonal matrix. $A = UDU^{\ast} = UD^{\ast}U^{\ast} = A^{\ast}$ holds since $D$ is a real matrix.
\end{proof}

\begin{definition}
    Let $A \in M_{n}(\C)$. $A$ is said to be a \eax{positive definite matrix} if $X^{\ast}AX > 0$ for all non-zero $X \in \C^{n}$. $A$ is said to be a \eax{positive semidefinite matrix} if $X^{\ast}AX \geq 0$ for all $X \in \C^{n}$.
\end{definition}

\begin{remark}
    Let $A$ be normal. $A$ is positive semidefinite if and only if the $\lambda_{i}$'s are non-negative, where the $\lambda_{i}$'s are the eigenvalues.
\end{remark}
\begin{proof}
    Let all the $\lambda_{i}$'s be non-negative. There exists a unitary $U$ such that $U^{\ast}AU = \text{diag}(\lambda_{1},\ldots,\lambda_{n}) = D$. The columns of $U$ form an orthonormal basis of $\C^{n}$ consisting of eigenvectors of $A$. Let $v_{1},\ldots,v_{n}$ be the eigenvectors. Choosing $X$ as $X = c_{1}v_{1} + \ldots + c_{n}v_{n}$, we have
    \begin{align}
        X^{\ast}AX = X^{\ast}(c_{1}\lambda_{1}v_{1} + \ldots + c_{n}\lambda_{n}v_{n}) = \abs{c_{1}}^{2}\lambda_{1}\ip{v_{1},v_{1}} + \ldots + \abs{c_{n}}^{2}\lambda_{n}\ip{v_{n},v_{n}} \geq 0
    \end{align}
    as the $\lambda_{i}$'s are non-negative. Conversely, let $A$ be normal and positive semidefinite. So, $X^{\ast}AX \geq 0$ for all $X \in \C^{n}$. Let $X$ be the column vector with $s^{\text{th}}$ component 1 and $t^{\text{th}}$ component $c \in \C$ and zeroes elsewhere. We then have
    \begin{align}
        0 \leq X^{\ast}AX = a_{ss}+ca_{st} + \overline{c}a_{ts}+\abs{c}^{2}a_{tt}.
    \end{align}
    If we take $c$ to be 0, we get that $a_{ss} \geq 0$ for all $s$. For $c = 1$, we have $a_{ss} + a_{st} + a_{ts} + a_{tt} \geq 0$. $c=i$ gives us $a_{ss} + ia_{st} - ia_{ts} + a_{tt} \geq 0$. Letting $a_{st} = x_{1}+iy_{1}$ and $a_{ts} = x_{2}+iy_{2}$, we get $x_{1} = x_{2}$ and $y_{1} = -y_{2}$. This implies that $a_{st} = \overline{a}_{ts}$. Thus, $A$ is Hermitian; the eigenvalues are then real. If we let $\lambda$ be an eigenvalue with $X$ being a corresponding eigenvector, we have
    \begin{align}
        0 \leq X^{\ast}AX = \lambda X^{\ast}X = \lambda \norm{X}^{2} \implies \lambda \geq 0.
    \end{align}
\end{proof}

\begin{corollary}
    The eigenvalues of a real symmetric matrix are real.
\end{corollary}
\begin{corollary}
    The eigenvalues of a skew-Hermitian matrix $(A = -A^{\ast})$ matrix (of a real skew-symmetric matrix) are purely imaginary.
\end{corollary}

For $A \in M_{n}(\C)$, $\lambda$ is an eigenvalue of $A$ if and only if $i\lambda$ is an eigenvalue of $iA$. If $A$ is skew-Hermitian, then $iA$ is a Hermitian matrix. Thus, if $iA$ has all real eigenvalues, $A$ has all purely imaginary eigenvalues. In particular, $I+A$ and $I-A$ are non-singular.

\begin{example}
    Let $A \in M_{n}(\C)$. Show that $x^{\ast}Ax = 0$ for all $x \in \C^{n}$ if and only if $A = 0$, and $x^{T}Ax = 0$ for all $x \in \R^{n}$ if and only if $A^{T} = -A$.
\end{example}

\begin{example}
    Let $A \in M_{n}(\C)$. Show that if $\lambda$ is an eigenvalue of $A$, then $\lambda^{k}$ is an eigenvalue of $A^{k}$. Also that $\alpha \in \C$ is an eigenvalue of $f(A)$ if and only if $\alpha = f(\lambda)$ for some eigenvalue $\lambda$ of $A$ and polynomial $f$.
\end{example}

\begin{example}
    Let $A \in M_{n}(\C)$. $A$ is Hermitian if and only if $\ip{AX,Y} = \ip{X,AY}$ for all $X,Y \in \C^{n}$. $A$ is Hermitian if and only if $\ip{AX,X} = \ip{X,AX}$ for all $X \in \C^{n}$.
\end{example}


\textit{March 18th.}
\begin{theorem}
    The following are equivalent for $A \in M_{n}(\C)$.
    \begin{itemize}
        \item $A$ is Hermitian.
        \item $X^{\ast}AX \in \R$ for all $X \in C^{n}$.
        \item $A^{2} = A^{\ast}A$.
        \item $\text{tr}(A^{2}) = \text{tr}(A^{\ast}A)$.
    \end{itemize}
\end{theorem}
\begin{proof}
    (i) implies (ii) is trivial. For (ii) implies (i), $X^{\ast}AX = (X^{\ast}AX)^{\ast}$ shows that $X^{\ast}(A-A^{\ast})X = 0$ for all $X \in \C^{n}$. This forces $A-A^{\ast} = 0$. (i) implies (iii) is also trivial; (iii) implies (i) can be shown via Schur's decomposition. There exists a unitary $U$ such that $U^{\ast}AU$ is upper triangular, say $T$, with diagonal entries $\lambda_{1},\ldots,\lambda_{n}$ which are the eigenvalues of $A$. We have
    \begin{align}
        UT^{2}U^{\ast} = A^{2} = A^{\ast}A = UT^{\ast}U^{\ast}UTU^{\ast} = UT^{\ast}TU^{\ast} \implies T^{2} = T^{\ast}T.
    \end{align}
    Comparing the diagonal entries, we get $\lambda_{j}^{2} = \abs{\lambda_{j}}^{2} + \sum_{i < j} \abs{t_{ij}}^{2}$ which shows that the $\lambda_{i}$'s are real, which again shows that $t_{ij} = 0$ for all $i < j$. Thus, $T$ is diagonal, and $A^{\ast} = A$. Again, (i) implies (iv) is trivial, and (iv) implies (i) is left as an exercise.
\end{proof}

\begin{theorem}
    Let $A \in M_{n}(\C)$ have all eigenvalues of modulus 1. Then $A$ is unitary if, for all $X \in C^{n}$, $\norm{AX} \leq \norm{X}$.
\end{theorem}
\begin{proof}
    Let $A = U^{\ast}DU$ be a Schur decomposition, where $U$ is unitary and $D$ is upper triangular. Let $\lambda_{i}$'s on the diagonal be the eigenvalues and the upper triangular entries be $d_{ij}$. Here, $\abs{\lambda_{i}} = 1$ for all $1 \leq i \leq n$ and $d_{ij} \in \C$. We have
    \begin{align}
        \norm{Ax}^{2} = (Ax)^{\ast}Ax = x^{\ast}A^{\ast}Ax = x^{\ast}U^{\ast}D^{\ast}DUx = (Ux)^{\ast}(D^{\ast}D)Ux.
    \end{align}
    We let this $Ux = e_{2}$ so that $\norm{AU^{\ast}e_{2}} \leq \norm{U^{\ast}e_{2}} = \norm{e_{2}} = 1$ which must mean that $d_{12} = 0$ as $\abs{\lambda_{2}} = 1$. Similarly, we can show that all non-diagonal entries are zero and $D$ is diagonal and Hermitian. We then have
    \begin{align}
        AA^{\ast} = U^{\ast}DUU^{\ast}D^{\ast}U = I_{n}.
    \end{align}
    $A$ is Hermitian.
\end{proof}

\begin{theorem}
    Let $A \in M_{n}(\C)$. $A$ is positive semidefinite if and only if there exists a unitary $U$ such that $A = U^{\ast} \emph{\text{diag}}(\lambda_{1},\ldots,\lambda_{n})U$ where the $\lambda_{i}$'s are non-negative.
\end{theorem}
\begin{proof}
    The first part is left as an exercise. For the converse, if $u_{1},\ldots,u_{n}$ denote the columns of $U$, these column vectors form an orthogonal set of basis vectors in $\C^{n}$. It can be shown that these $u_{i}$'s are also the eigenvectors of $A$. Letting $X = \alpha_{1}u_{1} + \ldots + \alpha_{n}u_{n}$, we have
    \begin{align}
        AX &= \alpha_{1}Au_{1} + \ldots + \alpha_{n}Au_{n} = \alpha_{1}\lambda_{1}u_{1} + \ldots + \alpha_{n}\lambda_{n}u_{n} \notag \\
        \implies X^{\ast}AX &= \lambda_{1}\abs{\alpha_{1}}^{2} \norm{u_{1}}^{2} + \ldots + \lambda_{n}\abs{\alpha_{n}}^{2}\norm{u_{n}}^{2} = \lambda_{1}\abs{\alpha_{1}}^{2} + \ldots + \lambda_{n}\abs{\alpha_{n}}^{2} \geq 0.
    \end{align}
    Similarly, we can show that $A$ is positive definite if and only if the $\lambda_{i}$'s are positive.
\end{proof}

\subsection{Singular Value Decomposition}
\begin{definition}
    The \eax{singular values of a matrix} $A$ are the positive square roots of the non-zero eigenvalues of $A^{\ast}A$. We note that $A^{\ast}A$ is Hermitian, so it has real eigenvalues; $A^{\ast}A$ is also positive semidefinite, so the eigenvalues are also non-negative.
\end{definition}

\begin{theorem}
    Let $A$ be a $m \times n$ matrix, with singular values $\sigma_{1},\sigma_{2},\ldots,\sigma_{n}$. Then there exist unitary $U \in M_{m}(\C)$ and $V \in M_{n}(\C)$ such that $A = U_{m \times m} \begin{pmatrix}
        D_{r \times r} & O \\ O & O
    \end{pmatrix} V_{n \times n}$ where $D = \emph{\text{diag}}(\sigma_{1},\sigma_{2},\ldots,\sigma_{r})$.
\end{theorem}
\begin{proof}
    If $A$ is $1 \times 1$, say $A = c$, then $A^{\ast}A = \abs{c}^{2}$. The only singular value of $A$ is $\abs{c}$. Then $A = \abs{c}e^{i\theta}$ for some real $\theta$.

    Let $A$ be a non-zero column matrix, $A = \begin{pmatrix}
        a_{1} & a_{2} & \ldots & a_{n}
    \end{pmatrix}^{t}$. Here, $A^{\ast}A = \abs{a_{1}}^{2} + \abs{a_{2}}^{2} + \ldots + \abs{a_{n}}^{2} = \norm{A}^{2}$. The only singular value is, thus, $\norm{A}$. Let $U$ be a unitary matrix with first column as the unit vector $\begin{pmatrix}
        \frac{a_{1}}{\sigma_{1}} & \cdots & \frac{a_{n}}{\sigma_{1}}
    \end{pmatrix}^{t}$ and let $V$ be unity. We then have $A = UD$, where $D = \begin{pmatrix}
        \sigma_{1} & \cdots & 0
    \end{pmatrix}^{t}$. Similarly, we can show this to be true for a row matrix $A$.

    Assume that $n >1, m>1$, and $A \neq 0$. Let $v_{1}$ be an eigenvector of $A^{\ast}A$ associated to $\sigma_{1}^{2}$, that is, $(A^{\ast}A)v_{1} = \sigma_{1}^{2}v_{1}$ with $v_{1}^{\ast}v_{1} = 1$. Let $u_{1} = \sigma_{1}^{-1}Av_{1}$. Note that $\ip{u_{1},u_{1}} = 1$. Let $P$ and $Q$ be unitary matrices with $v_{1}$ and $u_{1}$ as the first columns. As $A^{\ast}u_{1} = \sigma_{1}v_{1}$ and $(Av_{1})^{\ast} = \sigma_{1}u_{1}^{\ast}$, we get $P_{m \times m}^{\ast} A_{m \times n}^{\ast} Q_{n \times n} = \begin{pmatrix}
        \sigma_{1} & O \\ O & B_{(n-1) \times (n-1)}
    \end{pmatrix}$. We induct on $B$ to complete the proof. We have $A = Q \begin{pmatrix}
        \sigma_{1} & O \\ O & B^{\ast}
    \end{pmatrix} P^{\ast} \implies A^{\ast}A = P \begin{pmatrix}
        \sigma_{1}^{2} & O \\ O & BB^{\ast}
    \end{pmatrix} P^{\ast}$. This shows that the remaining singular values of $A$ are the same as the singular values of $B$.
\end{proof}


\chapter{QUADRATIC FORMS}


\section{Introduction}
\begin{definition}
    A \eax{quadratic form} in $n$ variables $x_{1},x_{2},\ldots,x_{n}$ is an expression of the form
    \begin{align*}
        q(x_{1},x_{2},\ldots,x_{n}) = \sum_{i=1}^{n} \alpha_{i}x_{i}^{2} + \sum_{1 \leq i < j \leq n} \beta_{ij}x_{i}x_{j}
    \end{align*}
    where $\alpha_{i},\beta_{ij} \in \R$. Everything discussed here may be interpreted in $\C$, but we stick to $\R$ for the duration of this chapter.
\end{definition}
For one variable $x_{1}$, the quadratic form is simply $q(x_{1}) = \alpha_{1}x_{1}^{2}$. For two variables, we have $q(x_{1},x_{2}) = \alpha_{1}x_{1}^{2} + \alpha_{2}x_{2}^{2} + \beta_{12}x_{1}x_{2}$. An $n$-quadratic form gives rise to a matrix $A = (\alpha_{ij})$ where the entries are given as
\begin{align}
    a_{ij} = \begin{cases}
        \alpha_{i} &\text{ if } i = j,\\
        \frac{1}{2} \beta_{ij} &\text{ if } i < j,\\
        \frac{1}{2} \beta_{ji} &\text{ if } i > j.
    \end{cases}
\end{align}
We note that $A$ is symmetric matrix. Conversely, if $A$ is a symmetric matrix, we can extract a quadratic form $q$ by defining
\begin{align}
    q(x_{1},x_{2},\ldots,x_{n}) = X^{t}AX \text{ where } X = \begin{pmatrix}
        x_{1} \\ x_{2} \\ \vdots \\ x_{n}
    \end{pmatrix}.
\end{align}
Here, the coefficients are then given as $\alpha_{i} = a_{ii}$ and $\beta_{ij} = 2a_{ij}$ for $i < j$.\\

\textit{March 25th.}
\begin{example}
    Suppose $q(x_{1},x_{2}) = 5x_{1}^{2}-10x_{1}x_{2}+x_{2}^{2}$. The associated matrix $A$ is then simply given as $A = \begin{pmatrix}
        5 & -5 \\ -5 & 1
    \end{pmatrix}$. It can be verified that $q(x_{1},x_{2}) = X^{t}AX$ where $X = \begin{pmatrix}
        x_{1} & x_{2}
    \end{pmatrix}^{t}$.
\end{example}
There is a one-to-one correspondence between the set of quadratic forms and the set of symmetric matrices. Let $X = PY$, where $P$ is invertible, that is, the new basis comprises of the columns of $P$. Then,
\begin{align}
    q(X^{t}) = X^{t}AX = (PY)^{t}A(PY) = Y^{t}(P^{t}AP)Y.
\end{align}
With respect to the new coordinate system, $q(Y^{t}) = Y^{t}BY$ where $B=P^{t}AP$.

\begin{definition}
    Two matrices $A$ and $B$ are termed \eax{congruent matrices} if there exists an invertible $P$ such that $B = P^{t}AP$.
\end{definition}

Two matrix representations of a quadratic form are congruent, and any two symmetric congruent matrices represent the same quadratic form. Congruent matrices have the same rank. Thus, a rank can be assigned to a quadratic form; it is simply the rank of an associated matrix.\\

A quadratic form $q$ satisfying $q(X^{t}) > 0$ for all $X \neq 0$ is termed a \eax{positive definite quadratic form}. Similarly, a form $q$ satisfying $q(X^{t}) < 0$ for all $X \neq 0$ is termed a \eax{negative definite quadratic form}. Changing the inequalities to also include an `equals to' leads to the definitions \eax{positive semidefinite quadratic form} and \eax{negative semidefinite quadratic form}. An \eax{indefinite quadratic form} is simply a form which can take negative, positive, and zero values.

We note that $q_{1}(x,y) = x^{2}+y^{2}$ is positive definite. The form $q_{2}(x,y) = -x^{2}-y^{2}$ is negative definite. Similarly, the form $q_{3}(x,y) = x^{2}-2xy+y^{2}$ is positive semidefinite and the form $q_{4}(x,y) = -x^{2}+2xy-y^{2}$ is negative semidefinite. Finally, the form $q_{5}(x,y) = x^{2}-y^{2}$ is indefinite.
With respect to a change of basis, the definiteness of a quadratic form is conserved.

\begin{definition}
    A quadratic form $q$ in $n$ variables is said to be in the diagonal form if $q(X^{t}) = \alpha_{1}x_{1}^{2} + \ldots + \alpha_{n}x_{n}^{2}$.
\end{definition}

\begin{theorem}
    Any quadratic form $X^{t}AX$ can be reduced to a diagonal form by a change of basis.
\end{theorem}
\begin{proof}
    As $A$ is symmetric, by the spectral theorem, there exists an orthogonal matrix $U$ such that $U^{-1}AU$ is diagonal. As $U^{-1} = U^{t}$, since $U$ is orthogonal, $U^{-1}AU$ represents the same form with respect to the basis consisting of columns of $U$.
\end{proof}

Notice that the diagonal form consists of the eigenvalues of $A$ on the diagonal. We discuss how to find this diagonal form explicitly; if $A = 0$, there is nothing to do. So, we assume that $A$ is non-zero. Further suppose that there exists $i$ such that $a_{ii} \neq 0$ and $a_{ij} = 0$ for all $i \neq j$. Interchange $x_{1}$ and $x_{i}$ making $a_{11} \neq 0$. This is inflicted by a non-singular linear transformation.

If $A$ is non-zero with $a_{11} = a_{22} = \ldots = a_{nn} = 0$ and $a_{ij} \neq 0$ for some $i \neq j$, make the change of variables $x_{j} = y_{i} + y_{j}$ and $x_{k} = y_{k}$ for $k \neq j$. This converts the form $X^{t}AX$ to $Y^{t}BY$ where $b_{ii} = 2a_{ij} \neq 0$. Let $a_{11} \neq 0$. Thus,
\begin{align}
    X^{t}AX &= a_{11}x_{1}^{2} + 2a_{12}x_{1}x_{2} + 2a_{13}x_{1}x_{3} + \ldots + 2a_{1n}x_{1}x_{n} + \sum_{i,j=2}^{n} a_{ij}x_{i}x_{j} \notag \\
    &= a_{11} \left( x_{1} + \frac{a_{12}}{a_{11}}x_{2} + \ldots + \frac{a_{1n}}{a_{11}} x_{n} \right)^{2} + \sum_{i,j=2}^{n} \left( a_{ij} - \frac{a_{ii}a_{ij}}{a_{11}} \right)x_{i}x_{j}.
\end{align}
Apply a change of variables as $z_{1} = x_{1} + \frac{a_{12}}{a_{11}}x_{2} + \ldots + \frac{a_{1n}}{a_{11}}x_{n}$ and $z_{k} = x_{k}$ for $k \neq 1$. The change of variable matrix looks like $\begin{pmatrix}
    1 & \frac{a_{12}}{a_{11}} & \cdots & \frac{a_{1n}}{a_{11}} \\
    0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 1
\end{pmatrix}$. The quadratic form now looks like $a_{11}z_{1}^{2} + q'(z_{2},\ldots,z_{n})$ where $q'$ is a quadratic form in $(n-1)$ variables. We repeat the same procedure and $X^{t}AX$ can be reduced to a diagonal form.

\begin{definition}
    The \eax{rank of a quadratic form} is the rank of the associated matrix. It is well-defined since any two congruent matrices have the same rank. This is equivalent to the dimension of the column space of the matrix.
\end{definition}

\begin{definition}
    The \eax{index of a quadratic form} $q$ is the number of positive entries in the diagonal of the diagonal form of $q$. This is equivalent to the number of positive eigenvalues of the matrix.
\end{definition}

\begin{definition}
    The \eax{signature of a quadratic form} is the difference between the number of positive entries and the number of negative entries in the diagonal of the diagonal form of $q$. This is equivalent to the difference between the number of positive eigenvalues and the number of negative eigenvalues of the matrix.
\end{definition}


\textit{March 27th.}

\begin{theorem}[\eax{Sylvester's law of inertia}]
    Two $n \times n$ symmetric matrices $B$ and $C$ are congruent, that is, they represent the same quadratic form in $\R^{n}$ if and only if the diagonal representations of $B$ and $C$ have the same rank, same index, and same signature.
\end{theorem}

\begin{proof}
    Let $C = Q^{t}BQ$ for $Q$ invertible. Without the loss of generality, let the positive diagonal entries in $B$ be $b_{11},\ldots,b_{kk}$, and those of $C$ are $c_{11},\ldots,c_{mm}$ and $k > m$. Consider $Q_{i1}x_{1} + \cdots + Q_{in}x_{n} = 0$ for all $k+1 \leq i \leq n$. This is a system of $n-k$ equations in $n$ unknowns. So, there exists a non-zero $X$ such that $X$ is a solution for the above system of equations, with $x_{i} = 0$ for $1 \leq i \leq m$. As $x_{1},\ldots,x_{m} = 0$, we have $X^{t}CX = \sum_{i=m+1}^{n}x_{i}^{2}c_{ii} \leq 0$. $QX \neq 0$ since $Q$ is invertible and $X$ is non-zero. Also, the last $n-k$ components of $QX$ are zeroes. So,
    \begin{align}
        X^{t}CX = X^{t}Q^{t}BQX = (QX)^{t}B(QX) > 0.
    \end{align}
    This is a contradiction to the fact that $k > m$. So, $k \leq m$. Reversing the roles of $B$ and $C$, $m \leq k$ forces $m = k$. For negatives, we simply work with $-B$ and $-C$.
\end{proof}


\begin{theorem}
    Let $x^{t}Ax$ be a quadratic form in $n$ variables with rank $r$, signature $s$, and index $p$. Then
    \begin{itemize}
        \item $A$ is positive definite if and only if $p = n \text{ }(= s = r)$.
        \item $A$ is positive semidefinite if and only if $p = r \text{ } (= s)$.
        \item $A$ is negative definite if and only if $s = -n \text{ } (= -r)$.
        \item $A$ is negative semidefinite if and only if $p = 0 \text{ } (r = -s)$.
        \item $A$ is indefinite if and only if $p \geq 1$ and $\abs{s} < r$.
    \end{itemize}
\end{theorem}

\section{Definiteness}
\textit{April 1st.}
\begin{definition}
    A real symmetric matrix $A$ is said to be positive definite (positive semidefinite) if $X^{t}AX > 0$ $(X^{t}AX \geq 0)$ for all $X \neq 0$.
\end{definition}

\begin{theorem}
    A symmetric matrix $A$ is positive definite (positive semidefinite) if each eigenvalue of $A$ is positive (non-negative). 
\end{theorem}

\begin{theorem}
    A (real) matrix $A$ is positive semidefinite if and only if there exists a real matrix $B$ such that $A = B^{t}B$.
\end{theorem}
\begin{proof}
    If $A = B^{t}B$, then $X^{t}AX = X^{t}B^{t}BX = (BX)^{t}BX \geq 0$. Conversely, let $A$ be positive semidefinite. As $A$ is symmetric, there exists an invertible $P$ such that $P^{t}AP = \begin{pmatrix}
        I_{r} & O \\ O & O
    \end{pmatrix}$ where $r$ is the rank of $A$. Letting $B = \text{diag}(I_{r},O)P^{-1}$, we get $A = B^{t}B$.
\end{proof}
Note that in the above proof, $a_{ij} = \ip{B_{i},B_{j}}$ where $B_{i}$ is the $i^{\text{th}}$ column of $B$.

\begin{theorem}
    $A$ is positive definite if and only if $A=B^{t}B$ for some real non-singular matrix $B$.
\end{theorem}

\begin{corollary}
    The inverse of a positive definite matrix is positive definite.
\end{corollary}

\begin{definition}
    A \eax{principal submatrix} of a square matrix $A$ is the submatrix obtained from $A$ by considering the same rows and columns. A \eax{leading principal submatrix} of $A$ is the principal submatrix obtained by taking the first $k$ rows and $k$ columns.
\end{definition}

\begin{theorem}
    Let $A$ be a real square matrix. If $A$ is positive definite, then every principal submatrix of $A$ has a positive determinant. Conversely, if the determinants of all the leading principal submatrices of a real symmetric matrix are positive, then $A$ is positive definite.
\end{theorem}
\begin{proof}
    Let $A$ be positive definite. Let $A_{k}$ be a $k \times k$ principal submatrix of $A$. Then there exists an invertible $P$ such that $P^{t}AP$ has $A_{k}$ on the top left. Note that $P^{t}$ is a sequence of elementary matrices of type II applied to $A$ to bring the rows of $A_{k}$ to the first $k$ rows. Also, $X^{t}A_{k}X = Y^{t}AY$ where $Y = P\begin{pmatrix}
        X & O
    \end{pmatrix}^{t}$. Since $Y^{t}AY > 0$ for all non-zero $Y$, we have $X^{t}A_{k}X > 0$ for all non-zero $X$. So, $A_{k}$ is positive definite for all $k$ and hence $\det A_{k} > 0$.

    Conversely, if $A$ is of order $1 \times 1$, that is, $A = \begin{pmatrix}
        a
    \end{pmatrix}$, then for all non-zero $x \in \R$, $x^{t}ax = x^{2}a > 0$. Hence, we assume the order $n$ of $A$ is greater than 1. Assume that the result holds true for all matrices of order $n-1$. Suppose $A$ is not positive definite. As $\det A > 0$, there exist at least two eigenvalues $\lambda_{1}$ and $\lambda_{2}$ which are negative. As $A$ is symmetric, spectral theorem tells us that there exists two orthogonal eigenvectors corresponding to $\lambda_{1}$ and $\lambda_{2}$; call them $X$ and $Y$ respectively. Consider a linear combination of $X$ and $Y$, say $\theta = aX + bY$ for $a,b \in \R$ such that the last coordinate of $\theta$ is zero, that is, $\theta \in R^{n-1}$. If $A_{n-1}$ is the $(n-1)$ leading principal submatrix of $A$, then
    \begin{align}
        \theta^{t}A_{n-1}\theta = (aX+bY)^{t}A_{n-1}(aX+bY) = a^{2}\lambda_{1} \norm{X}^{2} + b^{2}\lambda_{2}\norm{Y}^{2} < 0.
    \end{align}
    So the leading principal submatrix $A_{n-1}$ is not positive definite, but this is a contradiction.
\end{proof}

\begin{theorem}
    A real symmetric matrix $A$ is positive semidefinite if and only if all principal submatrices of $A$ have determinant non-negative.
\end{theorem}
\begin{proof}
    The first part is similar to the case for positive definiteness. Conversely, let all principal submatrices have determinant non-negative. Again, $n=1$ is trivial case. Assume that for matrices of order $n-1$, $\det A \geq 0$. Let $A(i_{1},\ldots,i_{k}|j_{1},\ldots,j_{k})$ denote the submatrix of the $i_{1}^{\text{th}},\ldots,i_{k}^{\text{th}}$ rows and $j_{1}^{\text{th}},\ldots,j_{k}^{\text{th}}$ columns. If all diagonal entries of $A$ are zeroes, then $\det A(i,j|i,j) = -a_{ij}^{2} \geq 0 \implies a_{ij} = 0$ for all $i,j$, so $A = 0$. Hence, $A$ is positive semidefinite. So, let $A$ have at least one positive diagonal entry, say the $i^{\text{th}}$ one. Applying $R_{1i}$ and $C_{1i}$, to bring $a_{ii}$ to the top-leftmost place. This new matrix is $P^{t}AP$ for some invertible $P$. Assume the result holds for all matrices of order $n-1$. Let $A_{k}$ denote a $k^{\text{th}}$ order principal submatrix of $A$. Make the $(i,1)^{\text{th}}$ entry of $A$ zero by performing $R_{i1}(-a_{i1}/a_{11})$, and the $(1,i)^{\text{th}}$ entry zero by performing $C_{1i}(-a_{1i}/a_{ii})$ for $u = 2,\ldots,n$. Let $B$ be the resultant matrix and let $C$ be the matrix obtained from $B$ by deleting the first row and first column. As the operations applied on $A$ to obtain $B$ are elementary matrices of type I, the determinant of these operations is 1. So, $\det A = \det B = a_{11} \det C$.

    Apply similar operators on $A_{k}$ to obtain $B_{k}$ and $C_{k}$. Then
    \begin{align}
        \det A(1,i_{1},\ldots,i_{k}|1,i_{1},\ldots,i_{k}) &= \det B(1,i_{1},\ldots,i_{k}|1,i_{1},\ldots,i_{k}) = a_{11} \notag \\
        &= a_{11} \det C(i_{1}-1,\ldots,i_{k}-1|i_{1}-1,\ldots,i_{k}-1) \geq 0
    \end{align}
    for all $2 \leq i_{1} \leq \ldots \leq i_{k} \leq n$. Hence, all principal submatrices of $C$ have non-negative determinants; by hypothesis, $C$ is positive semidefinite. $B = \text{diag}(a_{11},C)$ is positive semidefinite. As $A$ is congruent to $B$, $A$ is also positive semidefinite.
\end{proof}

\begin{theorem}
    Let $A_{n \times n}$ be a positive semidefinite matrix. Fix $1 \leq i \leq n$. Then $\det A \leq a_{ii} A_{ii}$, where $A_{ii}$ is the determinant of the matrix obtained by deleting the $i^{\text{th}}$ row and column of $A$. If $A$ is also positive definite, then $\det A = a_{ii}A_{ii}$ if and only if $a_{ij} = 0$ for all $j \neq i$.
\end{theorem}

\begin{proof}
    Note that it is enough to prove for $i = n$. Let $\det A = 0$. Then $A_{nn}$ is non-negative and $a_{nn}$ is also non-negative. So, $0 = \det A \leq a_{nn}A_{nn}$. We now let $\det A \neq 0$. Then $A$ is positive definite. Let $A = \begin{pmatrix}
        D & U \\ U^{t} & a_{nn}
    \end{pmatrix}$, where $D$ is positive definite (being a principal submatrix) and $D^{-1}$ is also positive definite. So, $U^{t}D^{-1}U \geq 0$. But
    \begin{align}
        \det A = \det D \cdot \det (a_{nn}-U^{t}D^{-1}U) = \det D \cdot (a_{nn}-U^{t}D^{-1}U) = A_{nn}(a_{nn}-U^{t}D^{-1}U) \leq a_{nn}A_{nn}.
    \end{align}
    Note that equality holds if and only if $U^{t}D^{-1}U = 0$. But this occurs if and only if $U = 0$.
\end{proof}


\textit{April 3rd.}
\begin{theorem}
    If $A$ is positive semidefinite, then $\det A \leq a_{11}a_{22} \cdots a_{nn}$. If $A$ is positive definite, and the equality holds, then $A$ is diagonal.
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{theorem}[\eax{Hadamard inequality}]
    For a real $n \times n$ matrix $B$,
    \begin{align*}
        (\det B)^{2} \leq \prod_{j=1}^{n} \norm{B_{j}}^{2}.
    \end{align*}
\end{theorem}
\begin{proof}
    Simply consider $A = B^{t}B$. The proof is left as an exercise to the reader.
\end{proof}

Suppose $A_{n \times n}$ is symmetric. Consider $X^{t}AX$ for all $X \in \R^{n}$. If $A$ has at least one positive eigenvalue, say $\lambda$, and if $v$ is an eigenvector corresponding to $\lambda$, then $v^{t}Av = \lambda \norm{v}^{2}$ can be made as large as possible from the choice of $v$. Thus, the set $\{X^{t}AX : X \in \R^{n}\}$ is not bounded above. If there exist no positive eigenvalues of $A$, then is $A$ is negative semidefinite, and $X^{t}AX \leq 0$ for all $X \in \R^{n}$. We can say more about the set of all possible values of $X^{t}AX$.

\begin{theorem}
    For any quadratic form $X^{t}AX$,
    \begin{align*}
        \max_{\norm{X} = 1} X^{t}AX = \max_{Y \neq 0} \frac{Y^{t}AY}{Y^{t}Y} = \lambda_{1}, \text{ and }
        \min_{\norm{X} = 1} X^{t}AX = \min_{Y \neq 0} \frac{Y^{t}AY}{Y^{t}Y} = \lambda_{n}
    \end{align*}
    where $\lambda_{1}$ and $\lambda_{n}$ are the largest and smallest eigenvalues of $A$. Moreover, $\frac{Y^{t}AY}{Y^{t}Y} = \lambda_{1}$ if and only if $Y$ is an eigenvector of $A$ corresponding to $\lambda_{1}$. A similar result holds true for $\lambda_{n}$.
\end{theorem}
\begin{proof}
    Let $\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{n}$ be the eigenvalues of $A$. There exists a unitary $P$ such that $P^{t}AP = D = \text{diag}(\lambda_{1},\lambda_{2},\ldots,\lambda_{n})$. If $U_{1},\ldots,U_{n}$ are the columns of $P$, then
    \begin{align}
        A = \lambda_{1}U_{1}U_{1}^{t} + \lambda_{2}U_{2}U_{2}^{t} + \cdots + \lambda_{n}U_{n}U_{n}^{t}.
    \end{align}
    As the set $(U_{1},\ldots,U_{n})$ forms an orthonormal basis of $\R^{n}$, any $Y$ can be written as $Y = \alpha_{1}U_{1} + \cdots + \alpha_{n}U_{n}$ which gives
    \begin{align}
        Y^{t}AY &= \lambda_{1}\alpha_{1}^{2} + \cdots + \lambda_{n}\alpha_{n}^{2} \notag \\
        \implies \frac{Y^{t}AY}{Y^{t}Y} &= \frac{\lambda_{1}\alpha_{1}^{2} + \cdots + \lambda_{n}\alpha_{n}^{2}}{\alpha_{1}^{2} + \cdots + \alpha_{n}^{2}} \leq \lambda_{1}.
    \end{align}
    Let $k = \max \{j : \lambda_{1} = \lambda_{j}\}$. If $k = n$, then $A = \lambda_{1}(U_{1}U_{1}^{t} + \cdots + U_{n}U_{n}^{t}) = \lambda_{1}P^{t}P = \lambda_{1}I$. Thus, the inequality becomes an equality for all $Y$, and $Y$ is an eigenvector corresponding to $\lambda_{1}$. If $k < n$, then
    \begin{align}
        \frac{Y^{t}AY}{Y^{t}Y} = \lambda_{1} &\implies \frac{\lambda_{1}\alpha_{1}^{2} + \cdots \lambda_{n}{\alpha_{n}^{2}}}{\alpha_{1}^{2}+ \cdots + \alpha_{n}^{2}} = \lambda_{1} \notag \\
        \implies \alpha_{k+1}^{2}(\lambda_{k+1}-\lambda_{1}) + \cdots + \alpha_{n}^{2}(\lambda_{n}-\lambda_{1}) = 0 &\implies \alpha_{k+1} = \ldots = \alpha_{n} = 0.
    \end{align}
    Thus, $Y = \alpha_{1}U_{1} + \cdots + \alpha_{k}U_{k} \implies AY = \lambda_{1}Y$. The converse is easy to see. The proof for the minimum is left as an exercise to the reader.
\end{proof}

\section{Simultaneous Diagonalization}
\textit{April 4th.}

Let $X^{t}AX$ and $X^{t}BX$ be two quadratic forms in $n$ variables. They are said to be \eax{simultaneously diagonalizable quadratic forms} if there exists an invertible $P$ such that both $P^{t}AP$ and $P^{t}BP$ are diagonal.

\begin{theorem}
    Le $X^{t}AX$ and $X^{t}BX$ be two quadratic forms in $n$ variables. If at least one of them is positive definite, or negative definite, then they are both simultaneously diagonalizable.
\end{theorem}
\begin{proof}
    Let $B$ be positive definite. So, there exists an invertible $N$ such that $N^{t}N$, that is, $(N^{-1})^{t}BN^{-1} = I$. Let $M = N^{-1}$, so that $M^{t}BM = I$. We also note that $M^{t}AM$ is symmetric. Hence, there exists an orthonormal matrix $P$ such that $P^{t}(M^{t}AM)P$ is diagonal, or $(MP)^{t}AMP$ is diagonal. It can be verified that the matrix $(MP)^{t}BMP$ also evaluates to the identity, a diagonal matrix.
\end{proof}
If $B$ is positive definite, and $A$ is any symmetric matrix, then there exists a non-singular matrix $S$ such that by the transformation $X = SY$, $X^{t}AX$ is diagonal and $X^{t}BX$ is the identity. Let $S^{t}AS = \text{diag}(\alpha_{1},\ldots,\alpha_{n}) = D$ and $S^{t}BS = I$. Note that this is equivalent to saying $S^{t}BS = I$ and $S^{-1}(B^{-1}A)S = D$; we do this to see that $\alpha_{1},\ldots,\alpha_{n}$ are really just the eigenvalues of $B^{-1}A$.

Let $X_{1},\ldots,X_{n}$ be the columns of $S$. Then the above is, again, equiavlent to saying that $X_{i}^{t}BX_{j} = \delta_{ij}$ and $(B^{-1}A)X_{i} = \alpha_{i}X_{i}$ for $1 \leq i \leq n$, or $AX_{i} = \alpha_{i}BX_{i}$. Also note that the $\alpha_{i}$'s are roots of the polynomial equation $\det (xI - B^{-1}A) = 0$. Since we can rewrite $\det (xI-B^{-1}A)$ as $\det B^{-1} \cdot \det(xB-A)$, we find that the roots of $\det (xI-B^{-1}A) = 0$ are exactly the roots of $\det (xB-A) = 0$.

\begin{theorem}
    Let $(A-\alpha B)X = 0$ and $(A-\beta B)Y = 0$, where $A,B$ are symmetric, and $\alpha \neq \beta$. Then, $X^{t}BY = 0$.
\end{theorem}
\begin{proof}
    The proof is simple since $\alpha X^{t}BY = \alpha Y^{t}BX = Y^{t}AX = X^{t}AY = \beta X^{t}BY \implies X^{t}BY = 0$ for $\alpha \neq \beta$.
\end{proof}

To find the matrix $S$ discussed above, we find the roots of $\det (xB-A) = 0$. Let $\beta_{1},\ldots,\beta_{k}$ be the distinct roots of $\det (xB-A) = 0$. Consider the inner product $\ip{X,Y} = Y^{t}BX$. Let $\beta_{i}$ have an algebraic multiplicity $n_{i}$ in this equation. We can find $n_{i}$ orthornormal vecotrs $X_{i_{1}},X_{i_{2}},\ldots,X_{i_{n_{i}}}$ corresponding to $\beta_{i}$ such that $(\beta_{i}B-A)X_{i_{j}} = 0$ for all $1 \leq j \leq n_{i}$. $S = (X_{11},\ldots,X_{1n_{1}},X_{21},\ldots,X_{2n_{2}},\ldots,X_{k1},\ldots,X_{kn_{k}})$, while using the above theorem, gives an orthonormal matrix such that $S^{t}BS = I$ and $S^{-1}AS = D$.\\

\textit{April 8th.}\\
\begin{theorem}
    Let $A$ and $B$ be $n \times n$ real symmetric matrices such that $A$ is positive semidefinite and $\emph{\colsp}{B} \subseteq \emph{\colsp{A}}$. Then there exists a non-singular $P$ such that $P^{t}AP$ and $P^{t}BP$ are diagonal.
\end{theorem}
\begin{proof}
    As $A$ is positive semidefinite, there exists a invertible $P$ such that $A = P^{t} \begin{pmatrix}
        I_{r} & O \\ O & O
    \end{pmatrix}P$. Let $C = \begin{pmatrix}
        I_{r} & O \\ O & O
    \end{pmatrix}P$. Note that $C^{t}C = A$. We then have $\colsp{B} \subseteq \colsp{A} = \colsp{(C^{t}C)} = \colsp{C^{t}}$. The last equality follows as $C$ is of rank $r$, so if $C^{t}X \in \colsp{C^{t}}$, then $X = CY \implies C^{t}X \in \colsp(C^{t}C)$. It can be shown that since $B$ is symmetric and $\colsp{B} \subseteq \colsp{A}$, then $B = ADA^{t}$ for some symmetric $D$. In our case, since $\colsp{B} \subseteq \colsp(C^{t})$, we have $B = C^{t}DC$ for some real symmetric matrix $D$. There also exists an orthogonal $S$ such that $D = S^{t}ES$, where $E$ is a diagonal matrix. So, $A = C^{t}S^{t}SC$ and $B = C^{t}S^{t}ESC$. Since $SC$ is an $r \times n$ matrix with rank $r$, there exists an invertible $Q$ such that $SCQ = \begin{pmatrix}
        I_{r} & O
    \end{pmatrix}$. Thus, we finally have
    \begin{align}
        Q^{t}AQ = Q^{t}C^{t}S^{t}SCQ = \begin{pmatrix}
            I_{r} & O \\ O & O
        \end{pmatrix} \text{ and } Q^{t}BQ = Q^{t}C^{t}S^{t}ESCQ = \begin{pmatrix}
            I_{r} \\ O
        \end{pmatrix} E \begin{pmatrix}
            I_{r} & O
        \end{pmatrix} = \begin{pmatrix}
            E & O \\ O & O
        \end{pmatrix}.
    \end{align}
\end{proof}

\begin{theorem}
    If $X^{t}AX$ and $X^{t}BX$ are both positive semidefinite in $n$-variables, or both negative definite, then they can be simultaneously diagonalized.
\end{theorem}
\begin{proof}
    Assume that both $A$ and $B$ are positive semidefinite. Then $X^{t}(A+B)X = X^{t}AX + X^{t}BX \geq 0$ tells us that $A+B$ is also positive semidefinite. It can be shown that $\colsp{B} \subseteq \colsp{(A+B)}$. So, by the previous theorem, there exists a non-singular $P$ such that $P^{t}(A+B)P$ and $P^{t}BP$ are diagonal. This forces $P^{t}AP$ to also be diagonal.
\end{proof}


\begin{theorem}
    If $A$ and $B$ are both positive semidefinite of order $n$, then $\det(A+B) \geq \det{A} + \det{B}$. If $n \geq 2$, and $A$ and $B$ are positive definite, then the strict inequality holds.
\end{theorem}
\begin{proof}
    As $A$ and $B$ are both positive semidefinite, there exists a invertible $P$ such that $P^{t}AP = \text{diag}(\alpha_{1},\ldots,\alpha_{n})$ and $P^{t}BP = \text{diag}(\beta_{1},\ldots,\beta_{n})$, and $P^{t}(A+B)P = \text{diag}(\alpha_{1}+\beta_{1},\ldots,\alpha_{n}+\beta_{n})$. As the $\alpha_{i}$'s and the $\beta_{i}$'s are non-negative,
    \begin{align}
        \det(A+B) = \frac{1}{(\det P)^{2}}\prod_{i=1}^{n}(\alpha_{i}+\beta_{i}) \geq \frac{1}{(\det P)^{2}} \prod_{i=1}^{n}\alpha_{i} + \frac{1}{(\det P)^{2}}\prod_{i=1}^{n}\beta_{i} = \det A + \det B.
    \end{align}
    If both $A$ and $B$ are positive definite then the $\alpha_{i}$'s and the $\beta_{i}$'s are positive, and the inequality above turns strict.
\end{proof}

\begin{theorem}
    If $A$ and $B$ are positive definite matrices of the same order $n$ and if $A-B$ is positive semidefinite, then $B^{-1}-A^{-1}$ is positive semidefinite.
\end{theorem}
\begin{proof}
    There exists an invertible $P$ such that $P^{t}AP$ and $P^{t}BP$ are diagonal. Let $C = P^{t}AP$ and $D = B^{t}BP$. As $A-B$ is positive semidefinite, then $P^{T}(A-B)P = C-D$ is also positive definite. We then consider $e_{i}^{t}(C-D)e_{i} = c_{ii}-d_{ii} \geq 0 \implies c_{ii} \geq d_{ii}$ for all $1 \leq i \leq n$. As $C$ and $D$ are positive definites, $c_{ii} \neq d_{ii}$ for all $1 \leq i \leq n$. So, $\frac{1}{d_{ii}} \geq \frac{1}{c_{ii}}$. Thus, $D^{-1}-C^{-1} = \text{diag}(d_{11}^{-1}-c_{11}^{-1},\ldots,d_{nn}^{-1}-c_{nn}^{-1})$ is positive semidefinite, and so is $B^{-1}-A^{-1} = P(D^{-1}-C^{-1})P$.
\end{proof}

\begin{lemma}
    Let $X \in \C^{n}$ be non-zero, and let $A \in M_{n}(\C)$. Then there exists en eigenvector $Y$ of $A$ belonging to $\emph{\text{span}} \{X,AX,A^{2}X,\ldots\}$.
\end{lemma}
\begin{proof}
    Let $k$ be the smallest positive integer such that $X,AX,\ldots,A^{k}X$ are linearly dependent. Let $\sum_{i=0}^{k}c_{i}A^{i}X = 0$ where $c_{k} \neq 0$. Also let $\beta_{1},\ldots,\beta_{k}$ be the roots of the polynomial $g(t) = \sum_{i=1}^{k} c_{i}t^{i} \in \C[t]$, with $g(t) = c_{k}(t-\beta_{1})\cdots(t-\beta_{k})$. Then $\sum_{i=0}^{k}c_{i}A^{i} = g(A) = c_{k}\prod_{i=0}^{k}(A-\beta_{i}I)$. Now let $Y = \prod_{i=2}^{k}(A-\beta_{i}I)X$. By the minimality of $k$, $Y$ is non-zero, and $(A-\beta_{1}I)Y=0$.
\end{proof}

\begin{theorem}
    The quadratic forms in $n$-variables $X^{t}A_{1}X, X^{t}A_{2}X,\ldots,X^{t}A_{p}X$ are simultaneously diagonalizable by an orthogonal transformation, that is, there exists a $Q \in O_{n}(\R)$ such that the matrices $Q^{t}A_{1}Q,Q^{t}A_{2}Q,\ldots,Q^{t}A_{p}Q$ are diagonal matrices if and only if the matrices $A_{2},A_{2},\ldots,A_{p}$ pairwise commute.
\end{theorem}
\begin{proof}
    If the quadratic forms are orthogonally simultaneously diagonalizable by a $P \in O_{n}(\R)$, then each of $P^{t}A_{1}P,P^{t}A_{2}P,\ldots,P^{t}A_{p}P$ are diagonal. But diagonal matrices commute, indicating that
    \begin{align}
        P^{t}A_{i}A_{j}P = (P^{t}A_{i}P)(P^{t}A_{j}P) = (P^{t}A_{j}P)(P^{t}A_{i}P) = P^{t}A_{j}A_{i}P \implies A_{i}A_{j} = A_{j}A_{i}.
    \end{align}
    For the converse, we induct on $n$. The statement is trivial for $n=1$. So, for $n > 1$, we assume that the result holds for all symmetric matrices of order $n-1$ or lower. Assume that $A_{1},A_{2},\ldots,A_{p}$ are pairwise commuting real symmetric matrices of order $n$. Let $X_{1}$ be a real eigenvector of $A_{1}$ corresponding to $\lambda_{1}$, say. Then $A_{2}$ has an eigenvector $X_{2}$ of the form $\sum_{i=0}^{k} \alpha_{i}A_{2}^{i}X_{1}$. It follows that $A_{1}X_{2} = \lambda_{1}X_{2}$, so $X_{2}$ is an eigenvector of $A_{1}$ as well. Let $X_{3}$ be a real eigenvector of $A_{3}$ in the span of the set $\{X_{2},A_{3}X_{2},A_{3}^{2}X_{2},\ldots\}$. Preceeding in the same way, we get a common eigenvector $X$ for all $A_{i}$'s, $1 \leq i \leq p$. Let $X$ have norm 1. Let $Q$ be the orthogonal matrix with $X$ as the first column; this gives $Q^{t}A_{i}Q = \begin{pmatrix}
        \lambda_{i} & O \\ O & B_{i}
    \end{pmatrix}$ for $1 \leq i \leq p$. $X$ is an eigenvector of $A_{i}$ corresponding to the eigenvalue $\lambda_{i}$ and the $B_{i}$'s are $(n-1) \times (n-1)$ real matrices. As the $A_{i}$'s commute, $A_{i}A_{j} = A_{j}A_{i}$ will imply that $B_{i}B_{j} = B_{j}B_{i}$. By using the induction hypothesis, there exists an orthogonal $S$ such that $S^{t}B_{i}S$ is diagonal for all $1 \leq i \leq p$. Let $P = Q \begin{pmatrix}
        1 & O \\ O & S
    \end{pmatrix}$. Then, $P$ is orthogonal, and it can be verified that $P^{t}A_{i}P = \begin{pmatrix}
        \lambda_{i} & O \\ O & S^{t}B_{i}S
    \end{pmatrix}$.
\end{proof}

\textit{April 10th.}\\
We state an alternative proof of the converse in the last theorem.
\begin{proof}[Alternative proof of the converse]
    We induct on $p$ instead of $n$. Assume that the result holds for $p-1$, and assume that $A_{i}$'s commute pairwise for $1 \leq i \leq p$. There exists an orthogonal $Q$ such that $D_{k} = Q^{t}A_{k}Q$ is diagonal for $1 \leq k \leq p-1$. Now partition the set $\{1,\ldots,n\}$ by the following rule; $i$ and $j$ belong to the same block if and only if $(D_{k})_{ii} = (D_{k})_{jj}$ for all $1 \leq k \leq p-1$. We assume that the blocks of the partition consists of the first $n_{1}$ elements, next $n_{2}$ elements, $\ldots$, and last $n_{s}$ elements, possibly by replacing $Q$ by $Q$ times a permutation matrix, wich will enable a shuffling of the rows of $D_{k}$'s. We shall call this matrix also $Q$. Now let $C = Q^{t}A_{p}Q$. Since $A_{p}A_{k} = A_{k}A_{p}$ for all $1 \leq k \leq p-1$, we have $D_{k}C = CD_{k}$. Equating the $(i,j)^{\text{th}}$ elements, we get $(D_{k})_{ii}c_{ij} = c_{ij}(D_{k})_{jj}$ for all $1 \leq k \leq p-1$. If $i$ and $j$ lie in different blocks, then $(D_{k})_{ii} \neq (D_{k})_{jj}$ for some $k$, which forces $c_{ij} = 0$. Thus, $C = \text{diag}(C_{1},C_{2},\ldots,C_{s})$ where $C_{t}$ is a matrix of order $n_{t}$, with $1 \leq t \leq s$. Since $C$ is symmetric, each $C_{t}$ is symmetric and there exists an orthogonal $U_{t}$ of order $n_{t}$ such that $U_{t}^{t}C_{t}U_{t}$ is diagonal. Finally, take $U = \text{diag}(U_{1},U_{2},\ldots,U_{s})$ and let $P = QU$. It can be verified that $PP^{t} = I$ and $P^{t}A_{p}P$ is diagonal.

    For $1 \leq k \leq p-1$, let first $n_{1}$ diagonal entries of $D_{k}$ be $\delta_{1}$, $\ldots$, next $n_{s}$ diagonal entries of $D_{k}$ be $\delta_{s}$. Then,
    \begin{align}
        P^{t}A_{k}P = U^{t}Q^{t}A_{k}QU = U^{t}D_{k}U = \text{diag}(\delta_{1}I_{n_{1}},\ldots,\delta_{s}I_{n_{s}}).
    \end{align}
    Thus, the $P$ is desirable.
\end{proof}

Let $P \in M_{n}(\R)$. We call $P$ an \eax{orthogonal projector} if there exists a subspace $S \subseteq \R^{n}$ such that $PX$ is the projection of $X$ into $S$ along $S^{\perp}$. Here, $S = \colsp P$. Some equivalent conditions are that $P^{\ast}P = P$, and that $P^{2}=P$ and $P^{\ast}=P$.


\begin{theorem}[\eax{Cramer's rule}]
    Let $A$ be a non-singular $n \times n$ matrix. Then the solution of $AX = b$ is given by $X = (x_{j})$ with $x_{j} = \frac{\det A_{j}}{\det A}$, where $A_{j}$ is the $n \times n$ matrix obtained from $A$ by replacing the $j^{\text{th}}$ column by the column vector $b$.
\end{theorem}

\begin{proof}
    Recall the classical adjoint, where $A(\adj A) = (\adj A) A = (\det A) I_{n}$ with $(\adj A)_{ij} = (-1)^{i+j} \det M_{ji}$. Here, $M_{ij}$ is the $(n-1) \times (n-1)$ matrix obtained from $A$ by deleting the $i^{\text{th}}$ row and $j^{\text{th}}$ column. Since $A$ is non-singular, we have $A^{-1} = \frac{\det A} (\adj A)$. The $j^{\text{th}}$ row(s) of $\adj A$ look like
    \begin{align}
        \begin{pmatrix}
            (-1)^{1+j} \det M_{ij} & (-1)^{2+j}\det M_{2j} & \cdots & (-1)^{n+j} \det M_{nj}.
        \end{pmatrix}
    \end{align}
    Thus,
    \begin{align}
        x_{j} = \frac{(j^{\text{th}} \text{ row of } \adj A)b}{\det A} = \frac{\det A_{j}}{\det A},
    \end{align}
    where $A_{j}$ is the matrix obtained from $A$ by replacing the $j^{\text{th}}$ column by $b$.
\end{proof}


\printindex

\end{document}