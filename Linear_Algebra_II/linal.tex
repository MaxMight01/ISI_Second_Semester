
\documentclass[15pt,a4paper]{book}

\usepackage{amsmath, amsthm, amssymb} 
\usepackage{graphicx} % For including graphics
\usepackage{hyperref} % For clickable links
\usepackage{bookmark} % Better control over bookmarks
\usepackage{geometry} % Customize page layout
\usepackage{xcolor} % Colors for text and graphics
\usepackage{enumitem} % Customizable lists
\usepackage{fancyhdr} % Header and footer
\usepackage{titlesec} % Custom section/chapter titles
\usepackage[toc,page]{appendix} % For the appendix
\usepackage{longtable} % For tables spanning multiple pages
\usepackage{mathrsfs} % For script fonts in math mode
\usepackage{tocloft} % Custom table of contents
\usepackage{datetime2} % For dates
\usepackage{caption} % For better control over captions
\usepackage{float} % Fine control over figure/table placement
\usepackage{imakeidx} % For index
\usepackage{afterpage} % For blank page

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\cftchapfont}{\normalfont} % Remove bold for chapter names
\renewcommand{\cftchappagefont}{\normalfont} % Remove bold for chapter page numbers
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\eax}[1]{\emph{#1}\index{#1}} % Macro for emphasis and index
\newcommand{\abs}[1]{\left| #1 \right|} % Absolute value
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tr}{\text{tr}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}

% Custom Notation List Environment
\newlist{notationlist}{description}{1}
\setlist[notationlist]{font=\bfseries,labelsep=1em}

% Geometry Settings
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
}

% Hyperref Colors
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    citecolor=red
}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}

% Custom Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Chapter name on top left
\fancyhead[R]{\rightmark}  % section name on top right
\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Making index
\makeindex[intoc]

% Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries \centering}
  {\chaptername\ \thechapter}{20pt}{\Huge \centering}

\titlespacing*{\chapter}{0pt}{20pt}{100pt}

\begin{document}

\pagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vspace*{\fill}
    % Title in all caps
    {\Huge \textbf{\MakeUppercase{Linear Algebra II}}\par}

    \vspace{0.5cm} % Adjust vertical spacing between title and subtitle
    % Subtitle in normal text, slightly enlarged
    {\Large Anita Naolekar, notes by Ramdas Singh\par}

    \vspace{0.5cm} % Additional spacing before the author
    % Author information
    {\large Second Semester\par}
    \vspace*{\fill}
    \end{center}
\end{titlepage}

\clearpage

\pagenumbering{roman}

\chapter*{List of Symbols}
\begin{notationlist}
    \item
\end{notationlist}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
\pagenumbering{arabic}
\pagestyle{fancy}


%%-------------------------------------------------------------------------------------------------


\chapter{PERMUTATION GROUPS}


\textit{January 3rd.}

Let $S_{n}$ denote the set of all bijections (permutations) on the set $\{1,2,\ldots,n\}$. If $\sigma, \tau \in S_{n}$, let us define $\sigma \tau$ to be the bijection defined as
\begin{equation}
    (\sigma \tau)(i) = \sigma(\tau(i)) \forall 1 \leq i \leq n.
\end{equation}

This gives us a binary operation on $S_{n}$ which is associative, and $S_{n}$ will then contain the identity permutation 1 such that $\sigma 1 = 1 \sigma = \sigma$ for all $\sigma \in S_{n}$. For every such $\sigma$, we can also find a $\sigma^{-1} \in S_{n}$ such that $\sigma \sigma^{-1} = \sigma^{-1}\sigma = 1$. The set $S_{n}$ equipped with this binary operation, thus, forms a group. In this case, we call $S_{n}$ as the \eax{symmetric group} of degree $n$. We now define a cycle in regards to permutations.

\begin{definition}
    A \eax{cycle} is a a string of positive integers, say $(i_{1},i_{2},\ldots,i_{k})$, which represents the permutation $\sigma \in S_{n}$ (with $k\leq n$) such that $\sigma(i_{j})=i_{j+1}$ for all $1 \leq j \leq k-1$, and $\sigma(i_{k}) = i_{1}$, and fixes all other integers. 
\end{definition}

We also note that $S_{3}$ is the smallest Abelian group possible, upto isomorphism. $S_{3}$ is one of the only two groups of order 6, and can be written as
\begin{equation}
    S_{3} = \{1, \sigma=(1,2,3), \sigma^{2}=(1,3,2), \tau=(1,2),\sigma\tau=(1,3),\tau\sigma=(2,3)\}.
\end{equation}
Some other observations arise. We find that $\sigma^{3} = \tau^{2} = 1$, and that $\tau\sigma = \sigma^{2}\tau$. We notice another fact via this $\sigma$;

\begin{remark}
    A \eax{k-cycle} $\sigma=(i_{1},i_{2},\ldots,i_{k})$ is of order $k$, that is, $\sigma^{k}=1$.
\end{remark}

\begin{definition}
    Two cycles in $S_{n}$ are called disjoint if they have no intger in common.
\end{definition}
We note that if $\sigma$ and $\tau$ are two disjoint cycles in $S_{n}$ then $\sigma$ and $\tau$ commute, that is, $\sigma \tau = \tau \sigma$.

\begin{proposition}
    Every $\sigma$ in $S_{n}$ can be written uniquely as a product of disjoint cycles.
\end{proposition}
Every cycle can be written as a product of 2-cycles. 2-cycles are called \eax{transpositions}. This can easily be seen as
\begin{equation}
    (a_{1},a_{2},\ldots,a_{n})=(a_{1},a_{n})(a_{1},a_{n-1})\cdots(a_{1},a_{3})(a_{1},a_{2}).
\end{equation}

\section{Even and Odd Permutations}
Let $x_{1},x_{2},\ldots,x_{n}$ be indeterminates, and let
\begin{equation}
    \Delta = \prod_{1 \leq i < j \leq n} (x_{i}-x_{j}).
\end{equation}
Let $\sigma \in S_{n}$, and define 
\begin{equation}
    \sigma(\Delta) = \prod_{1 \leq i < j \leq n} (x_{\sigma(i)}-x_{\sigma(j)}).
\end{equation}
We find that $\sigma(\Delta) = \pm \Delta$. Based on this, we classify permutations as odd or even.

\begin{definition}
    A permutation $\sigma$ is said to be an \eax{even permutation} if $\sigma(\Delta) = \Delta$, and is said to be an \eax{odd permutation} if $\sigma(\Delta) = -\Delta$. The sign of a permutation $\sigma$, denoted by $\epsilon(\sigma)$, is $+1$ if $\sigma$ is even, and is $-1$ if $\sigma$ is odd. So, $\sigma(\Delta) = \epsilon(\sigma)\Delta$.
\end{definition}

\begin{proposition}
    The map $\epsilon: S_{n} \to \{-1,+1\}$, where $\epsilon(\sigma)$ is the sign of $\sigma$, is a homomorphism, that is, $\epsilon(\sigma \tau) = \epsilon(\sigma)\epsilon(\tau)$ for all $\sigma, \tau \in S_{n}$.
\end{proposition}
\begin{proof}
    Start with $\tau(\Delta)$;
    \begin{equation}
        \tau(\Delta) = \prod_{1 \leq i < j \leq n} (x_{\tau(i)}-x_{\tau(j)}).
    \end{equation}
    Let there be $k$ factors of this polynomial where $\tau(i)>\tau(j)$ with $i<j$. We find that $\tau(\Delta) = (-1)^{k}\Delta$, and so, $\epsilon(\tau) = (-1)^{k}$. Now, $\sigma \tau(\Delta)$ has exactly $k$ factors of the form $x_{\sigma(j)}-x_{\sigma(i)}$, with $j > i$. Bringing out a factor $(-1)^{k}$, we find that $\sigma \tau (\Delta)$ has all factors of the form $x_{\sigma(i)}-x_{\sigma(j)}$, with $j > i$. Thus,
    \begin{equation}
        \epsilon(\sigma \tau)\Delta = \sigma \tau (\Delta) = (-1)^{k} \prod_{1 \leq i < j \leq n} (x_{\sigma(i)}-x_{\sigma(j)}) = (-1)^{k} \sigma(\Delta) = (-1)^{k} \epsilon(\sigma) \Delta = \epsilon(\tau) \epsilon(\sigma) \Delta.
    \end{equation}
    Cancelling out the $\Delta$, we find $\epsilon(\sigma \tau) = \epsilon(\sigma) \epsilon(\tau)$.
\end{proof}
$\epsilon$ is a homomorphism to an Abelian group, so $\epsilon(\sigma \tau) = \epsilon(\sigma) \epsilon(\tau) = \epsilon(\tau) \epsilon(\sigma)$.

\begin{proposition}
    If $\lambda = (i,j)$ is a transposition, then $\epsilon(\lambda) = -1$.
\end{proposition}
\begin{proof}
    If $\lambda = (1,2) \in S_{n}$, it is easy to show that
    \begin{equation}
        \lambda(\Delta) = (x_{1}-x_{2}) \cdots (x_{1}-x_{n}) (x_{2}-x_{3}) \cdots (x_{2}-x_{n}) \cdots = (-1)(\Delta).
    \end{equation}
    Now, if $\sigma = (i,j)$, with $(i,j) \neq (1,2)$, then $(i,j) = \lambda (1,2) \lambda$ where $\lambda$ interchanges $1$ and $i$, and interchanges $2$ and $j$. Using that fact that $\epsilon$ is a homomorphism, $\epsilon(\sigma) = -1$.
\end{proof}

A cycle $\sigma$ of length $k$ is an even permutation if and only if $k$ is odd. This is because it can be decomposed into $k-1$ transpositions, and we would then have $\epsilon(\sigma) = (-1)^{k-1} = 1$ (using the fact that $\epsilon$ is a homomorphism). Some more corollaries of the previous proposition include the fact that $\epsilon$ is a surjective map, and that $\epsilon(\sigma^{-1}) = \epsilon(\sigma)$.

If, for $\sigma \in S_{n}$, $\sigma$ can be decomposed as $\sigma_{1}\sigma_{2} \cdots \sigma_{k}$, where $\sigma_{i}$ is a $m_{i}$-cycle, then $\epsilon(\sigma_{i}) = (-1)^{m_{i}-1}$, and $\epsilon(\sigma) = (-1)^{(\sum m_{i}) - k}$.

\begin{proposition}
    $\sigma$ is an odd permutation if and only if the number of cycles of even length in its cycle decomposition is odd.
\end{proposition}

\section{The Determinant}

\begin{definition}
    If $A = (a_{ij})$ is a square matrix of order $n$, then the \eax{determinant} of $A$ is defined as
    \begin{equation}
        \det{A} = \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma(n)}.
    \end{equation}
\end{definition}
Using this definition of the determinant of a square matrix, one may derive the usual determinant properties with ease.\\ \\
\textit{January 7th.}

\begin{remark}
    The following properties may be inferred:
    \begin{itemize}
        \item If $A$ contains a row of zeroes, or a column of zeroes, then $\det{A} = 0$.
        \item $\det{I_{n}} = 1$.
        \item The determinant of a diagonal matrix is the product of the diagonal elements. This is because if $\sigma \in S_{N}$ is not the identity permutation, then there exists at least one element in the corresponding term where $i \neq \sigma(i)$, and $a_{i \sigma(i)}$ makes the term zero. For the identity transformation, it contains only those elements of the form $a_{ii}$.
    \end{itemize}
\end{remark}

Other non-trivial properties may also be shown with ease.

\begin{corollary}
    If $A$ is an upper triangular matrix, then $\det{A}$ is the product of the diagonal entries.
\end{corollary}
\begin{proof}
    If $a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \neq 0$, then $a_{n \sigma(n)} \neq 0$, that is, $\sigma(n) = n$, as $a_{ni} = 0 \; \forall \; i < n$. Again, $\sigma_{(n-1) \sigma(n-1)} \neq 0$ leads us to conclude that $\sigma(n-1) = n-1$ as $\sigma$ is a bijection and has to lead to a non-zero element. By similar logic, $\sigma(i) = i$ for all valid $i$. So, $\sigma$ is the identity permutation.
\end{proof}
\begin{corollary}
    If $A$ is a lower triangular matrix, then $\det{A}$ is the product of the diagonal entries.
\end{corollary}
\begin{proof}
    The proof of this is similar to the previous proof if we consider that the determinant of the tranpose of a matrix is equal to the determinant of said matrix.
\end{proof}

\begin{theorem}
    The determinant of a matrix is equal to the determinant of its transpose, that is, $\det{A} = \det{A^{t}}$ for a square matrix $A$.
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}


\begin{proposition}
    Let $B$ be obtained from $A$ by multiplying a row (or column) of $A$ by a non-zero scalar, $\alpha$. Then, $\det{B} = \alpha \det{A}$.
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{proposition}
    If $B$ is obtained from $A$ by interchanging any two rows (or columns) of $A$, then $\det{B} = -\det{A}$.
\end{proposition}
\begin{proof}
    Let $B$ be obtained from $A$ by interchanging the rows $k$ and $l$, with $k < l$. We then have
    \begin{align}
        \det{B} &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) b_{1 \sigma(1)} b_{2 \sigma(2)} \cdots b_{n \sigma(n)} \notag \\
        &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{(k-1) \sigma(k-1)} a_{l \sigma(k)} \sigma_{(k+1) \sigma(k+1)} \cdots a_{k \sigma(l)} \cdots a_{n \sigma(n)}.
    \end{align}
    As $\sigma$ runs through all elements in $S_{n}$, $\tau = \sigma(k, l)$ also runs through all $S_{n}$. Hence, via $\epsilon(\tau) = -\epsilon(\sigma)$, the equation now looks like
    \begin{align}
        \det{B} &= -\sum_{\tau \in S_{n}} \epsilon(\tau) a_{1 \tau(1)} \cdots a_{l \tau(l)} \cdots a_{k \tau(k)} \cdots a_{n \tau(n)} = -\det{A}.
    \end{align}
\end{proof}

\begin{proposition}
    If two rows (or columns) of $A$ are equal, then $\det{A} = 0$.
\end{proposition}
\begin{proof}
    Suppose that the rows $k$ and $l$ of $A$ are equal. Interchanging will alter the determinant by $-1$, so $\det{A} = -\det{A} \implies 2 \det{A} = 0 \implies \det{A} = 0$ if $2 \neq 0$ in the field $F$ from where the elements of $A$ arrive.

    If $2 = 0$ in $F$, that is, $F$ is of characteristic $2$, we pair the $\sigma$ term in the expression of $\det {A}$ with the term $\tau$ where $\tau = \sigma (k, l)$. The terms corresponding to $\sigma$ and $\tau$ in the expressions are the same, differing in only the sign. Hence, $\det{A} = 0$.
\end{proof}

\begin{theorem}
    For a fixed $k$, let the row $k$ of $A$ be the sum of the two row vectors $X^{t}$ and $Y^{t}$, that is, $a_{kj} = x_{j} + y_{j}$ for all $1 \leq j \leq n$. Then $\det{A} = \det{B} + \det{C}$ where $B$ is obtained from $A$ by replacing the row $k$ of $A$ by the row vector $X^{t}$, and $C$ is obtained from $A$ by replacing the row $k$ of $A$ by the row vector $Y^{t}$.
\end{theorem}
\begin{proof}
    We utilize the fact that $a_{kj} = x_{j} + y_{j}$. We have
    \begin{align}
        \det{A} &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{k \sigma(k)} \cdots a_{n \sigma(n)} \notag \\
        &= \left( \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots x_{\sigma(k)} \cdots a_{n \sigma(n)} \right) + \left( \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots y_{\sigma(k)} \cdots a_{n \sigma(n)} \right) \notag \\
        &= \det{B} + \det{C}. \notag
    \end{align}
\end{proof}

\begin{proposition}
    If a scalar multiple of a row (or column) is added to a row (or column) of a matrix, the determinant remains unchanged.
\end{proposition}
\begin{proof}
    The proof follows immediately from the previously proved properties.
\end{proof}
\textit{January 10th.}

\begin{definition}
    For $a_{ij} \in A$, the \eax{cofactor} of $a_{ij}$ is $A_{ij}=(-1)^{i+j} \det M_{ij}$, where $M_{ij}$ is the $(n-1) \times (n-1)$ matrix obtained from $A$ by deleting the $i^{\text{th}}$ row and $j^{\text{th}}$ column of $A$.
\end{definition}
\begin{lemma}
    Fix $k,j$. If $a_{kl} = 0$ for all $l \neq j$, then $\det{A} = a_{kj}A_{kj}$.
\end{lemma}
\begin{proof}
    Take $A$ to be a $n \times n$ matrix. We deal in cases.
    \begin{itemize}
        \item Case I: $k=j=n$. In the expansion of the determinant,
        \begin{equation}
            \det{A} = \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)}, \notag
        \end{equation}
        only those $\sigma$'s survive where $\sigma(n) = n$. These $\sigma$'s can be thought of as permutations of $S_{n-1}$ instead. The sign of $\sigma \in S_{n}$ and $\sigma \in S_{n-1}$ is the same as $n$ is fixed. Thus, we get
        \begin{equation}
            a_{nn} \sum_{\sigma \in S_{n-1}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{(n-1) \sigma(n-1)} = a_{nn} \det M_{nn} = (-1)^{n+n} a_{nn} A_{nn} = a_{nn} A_{nn}.
        \end{equation}
        \item Case II: $(k,j) \neq (n,n)$. We construct a matrix $B$ by interchanging $n-k$ rows and $n-j$ columns to bring $a_{ij}$ to the position $(n,n)$. Thus, we have $\det B = (-1)^{n-k+n-j} \det A = (-1)^{k+j} \det A$. But $B = a_{kj} \det M_{kj}$, so
        \begin{equation}
            \det A = (-1)^{k+j} a_{kj} \det M_{kj} = a_{kj} A_{kj}.
        \end{equation}
    \end{itemize}
\end{proof}
\begin{theorem}
    Let $A$ be a $n \times n$ matrix, and let $1 \leq k \leq n$. Then, $\det {A} = \sum \limits_{j=1}^{n} a_{kj} A_{kj}$, expansion by the $k^{\text{th}}$ row.
\end{theorem}
\begin{proof}
    Write out the $k^{\text{th}}$ row of $A$ as $x_{1}^{t}+\ldots+x_{n}^{t}$, where $x_{i} = (0,\ldots,0,a_{ki},0,\ldots,0)^{t}$, and all the other rows remaining are the same. Writing the matrix $A$ as the sum of $n$ matrices where each matrix is the same as $A$ but with a row that looks like $x_{i}^{t}$, we can easily show that $\det{A} = \sum_{j=1}^{n} a_{kj} A_{kj}$.
\end{proof}
\begin{example}
    Let $n \geq 1$, and let $A_{n} = \begin{pmatrix}
        a_{1}^{n-1} & a_{1}^{n-2} & \ldots & a_{1} & 1 \\
        a_{2}^{n-1} & a_{2}^{n-2} & \ldots & a_{2} & 1 \\
        \ldots & \ldots & \ldots & \ldots & \ldots \\
        a_{n}^{n-1} & a_{n}^{n-2} & \ldots & a_{n} & 1
    \end{pmatrix}$. Then, $\det {A_{n}} = \prod \limits_{1 \leq i \leq j \leq n} (a_{i}-a_{j})$.
\end{example}
\begin{proof}
    If $a_{i}=a_{j}$ for some $i \neq j$, then $\det{A_{n}} = 0$ as two rows are then identical. Hence, assume that the $a_{i}$'s are distinct. Now construct
    \begin{equation}
        B_{n} = 
        \begin{pmatrix}
            x_{1}^{n-1} & x_{1}^{n-2} & \ldots & x_{1} & 1 \\
            a_{2}^{n-1} & a_{2}^{n-2} & \ldots & a_{2} & 1 \\
            \ldots & \ldots & \ldots & \ldots & \ldots \\
            a_{n}^{n-1} & a_{n}^{n-2} & \ldots & a_{n} & 1
        \end{pmatrix}.
    \end{equation}
    Notice that $\det B_{n} \in F[x]$, where $F$ is the field, and $x$ is an indeterminate. $\det {B}$ is also of degree $(n-1)$; let us call this polynomial $f(x)$. Each of $a_{2},\ldots,a_{n}$ are roots of $f(x)$, so $f(x)$ must be of the form $f(x) = C(x-a_{2}) \ldots (x-a_{n})$. Equating coefficients of $x^{n-1}$, we get
    \begin{equation}
        C = \prod_{2 \leq i < j \leq n} (a_{i}-a_{j}) = \det \begin{pmatrix}
            a_{2}^{n-2} & \ldots & a_{2} & 1 \\
            \ldots & \ldots & \ldots & \ldots \\
            a_{n}^{n-2} & \ldots & a_{n} & a_{1}
        \end{pmatrix}.
    \end{equation}
    Thus, we must have
    \begin{align}
        f(x) &= \left( \prod_{2 \leq i < j \leq n} (a_{i}-a_{j}) \right) (x-a_{2}) \cdots (x-a_{n}) \\
        \implies \det A_{n} = f(1) &= \prod_{1 \leq i < j \leq n} (a_{i}-a_{j}).
    \end{align}
\end{proof}
\begin{example}
    Show that there exists a unique polynomial of degree $n$ that takes arbitrary prescribed values at the $(n+1)$ points $x_{0},x_{1},\ldots,x_{n}$.
\end{example}

\chapter{EIGENVECTORS AND EIGENVALUES}

\section{Linear Transformers and an Introduction}
Let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of vector space $V$ and $\mc{C} = (w_{1},\ldots,w_{n})$ be a basis of a vector space $W$. As these are bases, given a $v \in V$, there exists a unique $X \in F^{n}$ such that $v = \mc{B}X$, called the \eax{coordinate vector} of $v$ with respect to the basis $\mc{B}$. We note that since the mapping from a $v \in V$ to a $X \in F^{n}$ is linear in nature and is bijection, the vector spaces $V$ and $F^{n}$ are isomorphic to each other. Similarly, a mapping that takes $w \in W$ to $Y \in F^{m}$ shows that $W$ and $F^{m}$ are isomorphic to each other. 

Now suppose that there exists a linear map that takes $v \mapsto Tv$ with $v \in V$ and $Tv \in W$. This transformer $T$ is with respect to the bases $\mc{B}$ and $\mc{C}$ of $V$ and $W$, respectively. We construct the $m \times n$ matrix $A$ so that the $j^{\text{th}}$ column of $A$ is the coordinate vector of $Tv_{j}$ with respect to the basis $\mc{C}$. We will then have $T(\mc{B}) = \mc{C}A$. For any vector $v \in V$, we have
\begin{align}
    v &= \mc{B}X = v_{1}x_{1}+ \ldots v_{n}x_{n} \notag \\
    \implies T(v) &= T(v_{1})x_{1} + \ldots T(v_{n})x_{n} = (T(v_{1}),\ldots,T(v_{n})) \begin{pmatrix}
        x_{1} \\ x_{2} \\ \ldots \\ x_{n}
    \end{pmatrix} = T(\mc{B})X = (\mc{C}A) X \\
    &= (w_{1},\ldots,w_{m}) AX;
\end{align}
the coordinate vector of $Tv$ with respect to the basis $AX$. In fact, if we denote the isomorphism from $V$ to $F^{n}$ by $\phi_{\mc{B}}$ and the isomorphism from $W$ to $F^{m}$ by $\phi_{\mc{C}}$, we get $\phi_{\mc{C}} \circ T = (\text{mult. by $A$}) \circ \phi_{\mc{B}}$.

The next theorem will be divided into two parts.
\begin{theorem}
    \begin{enumerate}
        \item The vector space form. Let $T:V \to W$ be a linear mapping between finite dimensional vector spaces $V$ and $W$, of dimensions $n$ and $m$ respectively. There are bases $\mc{B}$ and $\mc{C}$ of $V$ and $W$ respectively such that the matrix of $T$ with respect to the bases $\mc{B}$ and $\mc{C}$ looks like $\begin{pmatrix}
            I_{r} & O_{r \times (n-r)} \\ O_{(m-r) \times r} & O_{(m-r) \times (n-r)}
        \end{pmatrix}_{m \times n}$.
        \item The matrix form. If $A$ is a $m \times n$ matrix, then there exists an invertible matrix $Q_{m \times m}$ and an invertible matrix $P_{n \times n}$ such that $Q^{-1}AP$ is of the form $\begin{pmatrix}
            I_{r} & 0 \\ 0 & 0
        \end{pmatrix}$, where $r$ is the rank of $A$.
        \item In fact, both these forms of the theorem are equivalent.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Let $(u_{1},\ldots,u_{n-r})$ be a basis of $\ker{T}$. We can extend this to a basis $\mc{B}$ by appending independent vectors that do not belong to the kernel of $T$, that is, $(v_{1},\ldots,v_{r},u_{1},\ldots,u_{n-r})$. Let $(Tv_{1},\ldots,Tv_{r})$ be a basis of $\text{Im}{T}$. We can extend this to a basis of $W$, say $\mc{C} = (w_{1},\ldots,w_{r},w_{r+1},\ldots,w_{m})$, where $w_{i} = Tv_{i}$ for $1 \leq i \leq r$. These bases are the desired ones.
        \item $P$ is a sequence of column operations, multipled to form a matrix, and $Q^{-1}$ is a sequence of row operations, multiplied to form a matrix, that get the matrix $A$ into the desired form. These are our desired $P$ and $Q$.
        \item Suppose the vector space form holds. Let $A$ be a $m \times n$ matrix over $F$, with $A:F^{n} \to F^{m}$ defined as $X \mapsto AX$. There then exists a basis $\mc{B}$ of $F^{n}$ and a basis $\mc{C}$ of $F^{m}$ such that the linear map $A$ with respect to ther bases $\mc{B}$ and $\mc{C}$ has the desired matrix. We then have $\mc{B} =I_{n}P_{n \times n}$ and $\mc{C} = I_{m}Q_{m \times m}$, with both $P$ and $Q$ invertible. We claim that the matrix of the linear mapping $A$ with respect to the bases $\mc{B}$ and $\mc{C}$ is $Q^{-1}AP$.
    \end{enumerate}
\end{proof}

\textit{January 16th.}
\begin{proposition}
    \begin{enumerate}
        \item Let $T:V \to W$ be a linear map, and $A$ the matrix of $T$ with respect to the bases $\mc{C}$ and $\mc{C}$ of $V$ and $W$ respectively. Let $\mc{B}'$ and $\mc{C}'$ be new bases of $V$ and $W$ respectively, and let the change of basis matrices be given by $\mc{B}' = \mc{B}P$ and $\mc{C}' = \mc{C}Q$. Then the matrix of $T$ with respect to $\mc{B}'$ and $\mc{C}'$ is $Q^{-1}AP$.
        \item If $A' = Q_{1}^{-1}AP_{1}$, where $P_{1}$ and $Q_{1}$ are $n \times n$ and $m \times m$ invertible matrices, respectively, then $A'$ is the matrix of $T$ with respect to the bases $\mc{B}P_{1}$ and $\mc{C}Q_{1}$.
    \end{enumerate}
    
\end{proposition}
\begin{proof}
    Let the coordinate vector of $v$ with respect to the basis $\mc{B}'$ be $X'$. We claim that the coordinate vector of $Tv$ with respect to the basis $\mc{C}'$ is $Y'$, where $Y' = (Q^{-1}AP)X'$. We assume that $\mc{B}' = \mc{B}P_{n \times n}$, $\mc{C}' = \mc{C}Q_{m \times m}$, and $T(\mc{B}) = \mc{C}A_{m \times n}$. If $v = \mc{B}X$, then $T(v) = \mc{C}(AX)$. If we let $v = \mc{B}'\mc{X}' = v_{1}'x_{1}' + \ldots + v_{n}'x_{n}'$, then
    \begin{equation}
        T(v) = \mc{C}'Y' = (\mc{C}Q)' = \mc{C}(QY') = \mc{C}(APX') \implies QY' = APX' \implies Y' = (Q^{-1}AP)X'
    \end{equation}
    To prove the second part, we will show that the first part implies it. Let $A_{m \times n}$ be a matrix. Let $T_{A}$ be the linear map from $\R^{n} \to \R^{m}$ given by multiplication by $A$, that is $T_{A} : \R^{n} \to \R^{m}$ given by $X \mapsto AX$. By the first part, there exist bases $P_{n \times n}$ and $Q_{m \times m}$, both invertible, such that with respect $P$ and $Q$, the matrix of $T_{A}$ looks like $\begin{pmatrix}
        I & O \\ O & O
    \end{pmatrix}$, that is, $Q^{-1}AP = \begin{pmatrix}
        I & O \\ O & O
    \end{pmatrix}$.
\end{proof}

\subsection{Linear Operators}
Let $T:V_{\mc{B}} \to V_{\mc{B}}$. Let $A$ be the matrix of $T$ with respect to the basis $\mc{B}$. The other matrices of $T$ with respect to new bases are $P^{-1}AP$, where $P_{n \times n}$ is invertible. Also, the fact that $T$ is bijective, one-one, or onto are all equivalent for a finite dimensional vector space $V$.

\subsection{Eigenvectors and Eigenvalues}
\begin{definition}
    A non-zero vector $v \in V$ is said to be an \eax{eigenvector} of $T$ if $T(v) = \lambda v$ for some $\lambda \in \F$. If $A$ is a $n \times n$ matrix, a non-zero column vector $X$ is said to be an eigenvector of $A$ if $AX = \lambda X$ for some $\lambda \in \F$. $\lambda$, in both these cases, is called the \eax{eigenvalue} of $v$ and $X$ respectively.
\end{definition}
Usually, we always disregard the zero vector being an eigenvector. If $v$ is an eigenvector of $T:V \to V$, and $v = \mc{B}X$ with respect to some basis $\mc{B}$ of $V$, then $X$ is an eigenvector of the matrix of $T$ with respect to the basis $\mc{B}$. In fact,
\begin{equation}
    \mc{B}(AX) = (\mc{B}A)X = T(\mc{B})X = T(\mc{B}X) = Tv = \lambda v = \lambda \mc{B} X = \mc{B} (\lambda X) \implies AX = \lambda X.
\end{equation}
The converse is also true; if $X$ is an eigenvector of $A_{n \times n}$, then $X$ is also an eigenvector of $T_{A} : \R^{n} \to \R^{n}$.

\begin{proposition}
    0 is an eigenvalue of $A_{n \times n}$ ($T:V \to V)$ if and only if $A$ ($T$) is non-invertible (not an isomorphism).
\end{proposition}
Suppose $v$ is an eigenvector of $T: V \to V$ with eigenvalue $\lambda$. Let $W$ be the subspace spanned by $v$. Then every vector $w \in W$ is an eigenvector of $T$ with eigenvalue $\lambda$. The proof of this is left as an exercise.

\begin{definition}
    Two matrices $A_{n \times n}'$ and $A_{n \times n}$ are called \eax{similar matrices} if there exists an invertible matrix $P_{n \times n}$ such that $P^{-1}AP = A'$.
\end{definition}

Again let $T: V \to V$ be a linear operator, and let $\mc{B} = (v_{1},\ldots,v_{n})$. Suppose, with respect to the basis $\mc{B}$, the matrix of $T$ is $\begin{pmatrix}
    \lambda_{1} & \ldots & \ldots & \ldots \\ 0 & \ldots & \ldots & \ldots \\ \ldots & \ldots & \ldots & \ldots \\ 0 & \ldots & \ldots & \ldots    
\end{pmatrix}$. Then $v_{1}$ is an eigenvector with eigenvalue $\lambda_{1}$.
\section{Finding Eigenvalues and Eigenvectors}
\textit{January 21st.}

Let $T:V \to V$ and let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of $V$. Then the matrix of $T$ with respect to the basis $\mc{B}$ is a diagonal matrix if and only if each of the basis elements is an eigenvector. An equivalent statement for matrices is that an $n \times n$ matrix $A$ is similar to a diagonal matrix if and only if $\F^{n}$ admits a basis consisting of eigenvectors of $A$. The proof of this is left as an exercise to the reader.

We can now discuss the computation. For a linear operator $T: V \to V$, $\lambda$ is an eigenvalue of $T$ if and only if there exists a non-zero vector $v$ such that $Tv = \lambda v$. This can be rearranged to give
\begin{equation}
    (\lambda I_{v} -T)v = 0.
\end{equation}
We can now consider $\lambda I_{v} - T: V \to V$ to be a linear operator which maps $v \mapsto \lambda v - Tv$. If eigenvalues exist, this operator is a singular operator, that is, it contains a non-trivial kernel. The matrix of the operator $\lambda I_{v} - T$ comes out to be $\lambda I_{n} - A$, where $A$ is the matrix of $T$ with respect to the basis $\mc{B}$. This matrix is now singular, so we must have
\begin{equation}
    \det(\lambda I_{n} - A) = 0.
\end{equation}
The equation $\det(\lambda I_{n} - A)$ is called the \eax{characteristic polynomial} of $A$, and also $T$(?). The roots of this polynomial in $\lambda$ which lie in $\F$ are the eigenvalues of $A$, and $T$ as well.

We would now like to show that similar matrices have the same eigenvalues, that is,
\begin{equation}
    \det(\lambda I_{n} - P^{-1}AP) = \det(\lambda I_{n} - A).
\end{equation}
This is simple to see as $\det(\lambda I_{n} - P^{-1}AP) = \det(P^{-1}(\lambda I_{n} - A)P) = \det P^{-1} \cdot \det(\lambda I_{n} - A) \cdot \det P = \det(\lambda I_{n} - A)$. The found out eigenvalues from this equation can then be put back and solved for $v$ to get the corresponding eigenvectors.

\begin{proposition}
    Let $\lambda_{1},\ldots,\lambda_{r}$ be distinct eigenvalues of $T:V \to V$ and let $v_{1},\ldots,v_{r}$ be the corresponding eigenvectors of $T$. Then $(v_{1},\ldots,v_{r})$ is a linearly independent set in $V$.
\end{proposition}
\begin{proof}
    We claim that this is true for $r = 1,2$. Using a form of induciton, we will assume the result for $r-1$. Begin with
    \begin{align}
        \alpha_{1}v_{1} + \ldots + \alpha_{r}v_{r} &= 0 \notag \\
        \implies \alpha_{1}Tv_{1} + \ldots + \alpha_{r}Tv_{r} &= 0 \notag \\
        \implies \alpha_{1}\lambda_{1}v_{1} + \ldots + \alpha_{r}\lambda_{r}v_{r} &= 0.
    \end{align}
    Multiplying the first equation by $\lambda_{1}$ and subtracting it from the current equation, we have
    \begin{align}
        (\alpha_{2}\lambda_{2}-\alpha_{2}\lambda_{1})v_{2} + (\alpha_{3}\lambda_{3}-\alpha_{3}\lambda_{1})v_{3} + \ldots + (\alpha_{r}\lambda_{r}-\alpha_{r}\lambda_{1})v_{r} &= 0 \notag \\
        \implies \alpha_{2}(\lambda_{2}-\lambda_{1}) + \alpha-{3}(\lambda_{3}-\lambda_{1})v_{3} + \ldots + \alpha_{r}(\lambda_{r}-\lambda_{1})v_{r} &= 0.
    \end{align}
    By hypothesis, $\alpha_{j}(\lambda_{j}-\lambda_{1}) = 0$. As the eigenvalues are distinct, we must have $\alpha_{j} = 0$ for $j = 2,3,\ldots,r$. We are left with $\alpha_{1}v_{1} = 0$, which gives us $\alpha_{1} = 0$.
\end{proof}
When the $n$ eigenvalues found of $A$ are distinct, the corresponding eigenvectors $v_{1},\ldots,v_{n}$ are linearly independent in $\F^{n}$, and hence $\mc{B} = (v_{1},\ldots,v_{n})$ is a basis of $\F^{n}$. The matrix $P^{-1}AP$ is the matrix of the linear operator $T_{A} : \F^{n} \to \F^{n}$ with respect to the basis $\mc{B}$, with the column of $P$ being the eigenvectors $v_{1},\ldots,v_{n}$. As $\mc{B}$ consists of only eigenvectors, $P^{-1}AP$ is a diagonal matrix with the diagonal entries being the $n$ eigenvalues.

We now define the determinant and trace for a linear operator. For such an operator $T$, $\text{tr}T = \text{tr}A$ where $A$ is a matrix of $T$ with respect to some abitrary basis. Note that since $\tr(P^{-1}AP) = \tr(APP^{-1}) = \tr{A}$, the choice of basis is not important. Similarly, we define $\det{T} = \det{A}$.

We can now have a closer look at the characteristic equation. To find the constant term of $\det(xI-A)$, we simply plug in $x=0$ to give us $\det(-A) = (-1)^{n} \det A$. The coefficient of $x^{n-1}$ in $\det(xI-A)$ is $-\tr{A}$ as the coefficients of $x^{n-1}$ come solely from the expansion of $(x-a_{11})(x-a_{22})\cdots(x-a_{nn})$. Clearly, we can conclude that the sum of the eigenvalues is $\tr{A}$ and the product of the eigenvalues is $\det{A}$.

\subsection{Eigenspace}
\textit{January 23rd.}

For ease, let us denote $\chi_{T}(x)$ to mean $\det(xI-A)$. The \eax{eigenspace} for a given eigenvalue $\lambda$ is defined as
\begin{equation}
    E_{\lambda} = \{v \in V : Tv = \lambda v\}.
\end{equation}
This is a subspace of the vector space $V$. The \eax{geometric multiplicity} of $\lambda$ is defined as the dimension of $E_{\lambda}$. This geometric multiplicity of $\lambda$ is always less than or equal to its algebraic multiplicity in $\chi_{T}(x)$. For recall, the \eax{algebraic multiplicity} of $\lambda$ is the highest power of $(x-\lambda)$ that divides $\chi_{T}(x)$.

\begin{theorem}
    Let $\lambda$ be an eigenvalue of $T:V \to V$. Then the geometric multiplicity of $\lambda$ is always less than or equal to its algebraic multiplicity.
\end{theorem}
\begin{proof}
    Let $k$ me the geometric multiplicity of $\lambda$. Let $(v_{1},\ldots,v_{k})$ be an ordered basis of $E_{\lambda}$. Extend this to a basis $\mc{B} = (v_{1},\ldots,v_{k},u_{1},\ldots,u_{n-k})$ of $V$. The matrix of $T$ with respect to the basis $\mc{B}$ is of the form $A = \begin{pmatrix}
        \lambda I_{k} & B \\ O & D
    \end{pmatrix}$. Thus, the characteristic polynomial looks like
    \begin{equation}
        \chi_{T}(x) = \det(xI_{n}-A) = \det \begin{pmatrix}
            (x-\lambda)I_{k} & -B \\ O & xI_{n-k}-D
        \end{pmatrix}
        = (x-\lambda)^{k} \cdot \det(XI_{n-k}-D).
    \end{equation}
    This shows that $(x-\lambda)^{k}$ divides $\chi_{T}(x)$, so we must have an algebraic multiplicity greater than or equal to this $k$.
\end{proof}

\section{Diagonalizability}
We first define what this means for a linear mapping from $V$ to $V$.
\begin{definition}
    A linear operator $T: V \to V$ is said to be a \eax{diagonizable linear operator} if there exists a basis of $V$ consisting of eigenvectors of $T$. This means that the matrix of $T$ with respect to this basis if a digaonal matrix and the matrix of $T$ with respect to any other basis is similar to this diagonal matrix.
\end{definition}
A similar definition works for matrices.
\begin{definition}
    An $n \times n$ matrix $A$ over $\F$ is said to be a \eax{diagonizable matrix} if $A$ is similar to a diagonal matrix. Equivalently, $\F^{n}$ then admits a basis consisting of eigenvectors of $A$, thinking of $T_{A}:\F^{n} \to \F^{n}$ as a linear operator.
\end{definition}

Now let us suppose that $T$ is diagonizable. Let $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$ be the distinct eigenvalues of $T$. There then exists an ordered basis consisting of eigenvectors of $T$ and with respect to this basis, the matrix of $T$ is a diagonal matrix with diagonal entries consisting solely of $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$.

If $\lambda_{i}$ is of algebraic multiplicity $d_{i}$, then the matrix of $T$ looks like $\begin{pmatrix}
    \lambda_{1}I_{d_{1}} & & & \\
     & \lambda_{2}I_{d_{2}} & & \\
     & & \ldots & \\
     & & & \lambda_{k}I_{d_{k}}
\end{pmatrix}$. Thus, the characteristic polynomial then looks like $(x-\lambda_{1})^{d_{1}} (x-\lambda_{2})^{d_{2}} \cdots (x-\lambda_{k})^{d_{k}}$.

The geometric multiplicity of $\lambda_{i}$ is the dimension of $E_{\lambda_{i}}$, that is, the nullity of the operator $(\lambda_{i}I_{n}-A)$. But here, $\ker(\lambda_{i}I_{n}-A) = d_{i}$, which is just the algebraic multiplicity of $\lambda_{i}$. Hence, if $T$ is diagonizable, then each eigenvalue of it has the same algebraic multiplicity and geometric multiplicity.

\begin{proposition}
    If $E_{\lambda_{1}},\ldots,E_{\lambda_{k}}$ are the eigenspaces corresponding to the distinct eigenvalues, say, $\lambda_{1},\ldots,\lambda_{k}$ of $T$, then $E = E_{\lambda_{1}} + \cdots + E_{\lambda_{k}}$ is a direct sum.
\end{proposition}
\begin{proof}
    It is enough to show that $E_{\lambda_{1}},\ldots,E_{\lambda_{k}}$ are independent. Let $v_{1}+v_{2}+\ldots+v_{k} = 0$, where $v_{i} \in E_{\lambda_{i}}$. As $v_{1},v_{2},\ldots,v_{k}$ come from distinct eigenspaces, they are linearly independent, and our equation must imply that $v_{1} = \ldots = v_{k} = 0$.
\end{proof}
\begin{proposition}
    If $T$ is a diagonizable operator, and if $\lambda_{1},\ldots,\lambda_{k}$ are the distinct eigenvalues of $T$, then
    \begin{equation}
        V = E_{\lambda_{1}} \oplus \ldots \oplus E_{\lambda_{k}}.
    \end{equation}
\end{proposition}
\begin{proof}
    As $T$ is diagonizable, the algebraic and geometric multiplicities are equal for all the eigenvalues $\lambda_{i}$. Denote $\dim E_{\lambda_{i}} = d_{i}$. As $\chi_{T}(x)$ completely factors into linear factors, due to $T$ being diagonizable, we have $n = d_{1} + \ldots + d_{k}$. Also, $E_{\lambda_{1}} + \ldots + E_{\lambda_{k}}$ is a direct sum, that is,
    \begin{equation}
        \dim(E_{\lambda_{1}}+\ldots+E_{\lambda_{k}}) = \dim E_{\lambda_{1}} + \ldots + \dim E_{\lambda_{k}} = n.
    \end{equation}
    This direct sum is a subspace of $V$ and has the dimension as $V$. This mut mean that the direct sum is exactly $V$.
\end{proof}

\begin{theorem}
    Let $T$ be a linear operator on a finite dimensional vector space $V$, and let $\lambda_{1},\ldots,\lambda_{k}$ be the distinct eigenvalues of $T$. Also let $E_{\lambda_{i}}$ be the eigenspace of $\lambda_{i}$. Then, the following are equivalent.
    \begin{itemize}
        \item $T$ is diagonizable,
        \item $\chi_{T}(x) = (x-\lambda_{1})^{d_{1}} \cdots (x-\lambda_{k})^{d_{k}}$ and $\dim E_{\lambda_{i}} = d_{i}$,
        \item $V = E_{\lambda_{1}} \oplus \ldots \oplus E_{\lambda_{k}}$.
    \end{itemize}
\end{theorem}

\section{Polynomials}
\textit{January 28th.}

Let $\F[x]$ denote the set of all polynomials with coefficients coming from the field $\F$. With respect to the additions, it is an Abelian group. The multiplication here is associative, commutative, and distributive; there also exists a multiplicative identity. This makes $\F[x]$ into a commutative ring. Note that $\F[x]$ is also an infinite dimensional vector space over $\F$, since scalar multiplication is also defined. Together, these combine to form an algebra over the field.

\begin{definition}
    Let $d \in \F[x]$ with $d \neq 0$. For $f \in \F[x]$, we say that $d$ divides $f$ if there exists a $q \in \F[x]$ such that $f = dq$ in $\F[x]$.
\end{definition}
\begin{corollary}
    For $f \in \F[x]$, $f(c) = 0$ if and only if $x-c$ divides $f(x)$.
\end{corollary}
\begin{corollary}
    A polynomial $f \in \F[x]$ of degree $n$ has at most $n$ roots in $\F$.
\end{corollary}
\begin{proof}
    The proof is by induction. Note that this is true for $n = 0, 1$. If $\alpha$ is a root, then $f(x) = (x-\alpha) \cdot q(x)$. As $q(x)$ is of degree $n-1$, and all roots of $q(x)$ are root of $f(x)$, this follows by hypothesis.
\end{proof}

\begin{definition}
    An \eax{ideal} of $\F[x]$ is a subspace of $\F[x]$, say $I$, such that if $f \in I$ and $g \in F[x]$, then $fg \in I$.
\end{definition}
\begin{example}
    Let $f \in \F[x]$. Define $I_{f} = \langle f\rangle = \{fg : g \in \F[x]\}$. Note that $I_{f}$ is called a \eax{principal ideal}, that is, it is an ideal generated by a single element.
\end{example}
\begin{theorem}
    $\F[x]$ is a principal ideal domain, that is, every ideal in $\F[x]$ is a principal ideal.
\end{theorem}
\begin{proof}
    Let $d$ be a polynomial of least degree in the ideal $I$, where $I$ is a non-zero ideal. Let, without loss of generatlity, $d$ be monic (if not, simply multiply it by a sutitable scalar).

    Let $f \in I$. Then there exists $q,r \in \F[x]$ such that $f = dq + r$ and either $r = 0$ or $\deg r < \deg d$. Note that since $f,d \in I$, $dq \in I$, so $f-dq \in I \implies r \in I$. As $d$ was of minimal degree in $I$, we must have $r = 0$. Thus, $f = dq$ and, thus, $I = \langle d \rangle$.
\end{proof}
If $I$ is an ideal of $\F[x]$, then there exists a unique polynomial $d \in I$ such that $I = \langle d \rangle$.

\subsection{Interaction with Linear Operators}
Let $f \in \F[x]$, and let $T:V \to V$ be a linear mapping. If
\begin{equation*}
    f(x) = a_{0} + a_{1}x + \ldots + a_{k}x^{k}
\end{equation*}
with $a_{k} \neq 0$, we define
\begin{equation*}
    f(T) = a_{0}I_{n} + a_{1}T + \ldots + a_{k}T^{k}.
\end{equation*}
Note that $f(T)$ is also a linear mapping from $V$ to $V$. Let $I$ be the set of all $f \in \F[x]$ such that $f(T)$ is the zero operator. All such polynomials are called \eax{annihilator}s. $I$ satisfies the properties of a vector space; it is a subspace of the space of all polynomials. $I$ is also an ideal of $\F[x]$.

\begin{definition}
    The \eax{minimal polynomial} of the linear operator $T:V \to V$ is the generator of the ideal of annihilators.
\end{definition}
Denote the minimal polynomial by $m_{T}(x)$. So, $m_{T}(x)$ is
\begin{enumerate}
    \item monic,
    \item of least degree among all annihilators of $T$.
\end{enumerate}
If $A$ is a $n \times n$ matrix, the minimal polynomial of $A$ is defined as the unique monic polynomial $m_{A}(x)$ of least degree such that $m_{A}(A) = O_{n \times n}$. It can be verified that if $A$ is the matrix of a linear operator $T:V \to V$ and if $f \in \F[x]$, then the matrix of the operator $f(T) : V \to V$ is $f(A)$ with respect to the same basis. IT follows that the minimal polynomial of $T$ is same as the minimal polynomial of a matrix of $T$.

Note that $T$ belongs to $\hom (V, V)$, which is of dimension $n^{2}$. Thus, $I, T, T^{2}, \ldots, T^{n^{2}}$ is a linearly dependent set and there exist scalars $a_{0},a_{2},\ldots,a_{n^{2}}$ such that
\begin{equation}
    a_{0}I + a_{1} T = a_{2}T^{2} + \ldots + a_{n^{2}} T^{n} = O.
\end{equation}
So, an annihilator of $T$ is
\begin{equation*}
    f(x) = a_{0} + a_{1}x + a_{2}x^{2} + \ldots + a_{n^{2}} x^{n^{2}}
\end{equation*}
and we must have $\deg m_{T}(x) \leq n^{2}$.

\begin{theorem}
    Let $T: V \to V$ with $n$ the dimension of the space $V$. The characteristic polynomial of $T$ and the minimal polynomial of $T$ have the same roots, except (possibly) for the multiplicities.
\end{theorem}
\begin{proof}
    We claim that $m_{T}(c) = 0$ if and only if $c$ is an eigenvalue. Let $m_{T}(c) = 0$. Thus, $m_{T}(c) = (x-c) \cdot q(x)$, with $q \in \F[x]$ and $\deg q < \deg m$. Also, $q(T)$ is \textit{not} the zero operator. So, there exists a $u \in V$ (non-zero vector) such that $q(T)(u) = v \neq 0$. Then,
    \begin{align}
        0 = m(T)(u) = (T-cI) \cdot q(T)(u) = (T-cI)v
    \end{align}
    which shows that $v$ is an eigenvector of $T$ with eigenvalue $c$. So all roots of $m_{T}(x)$ are roots of the characteristic polynomial.

    Conversely, let $c$ be an eigenvalue of $T$. Say, $Tv = cv$ for some $v \neq 0$. Thus, $m_{T}(T)(v) = m(c)(v)$. But $m_{T}(T) = 0$ must mean that $0 = m(c)(v)$, and $m(c) = 0$. So every root of the characteristic polynomial is a root fo the minimal polynomial.
\end{proof}

\begin{appendices}

\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries}
  {\chaptername\ \thechapter}{20pt}{\Huge}

\titlespacing*{\chapter}{0pt}{20pt}{40pt}

\chapter{Appendix}
Extra content goes here.

\printindex

\end{appendices}

\end{document}