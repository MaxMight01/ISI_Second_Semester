
\documentclass[15pt,a4paper]{book}

\usepackage{amsmath, amsthm, amssymb} 
\usepackage{graphicx} % For including graphics
\usepackage{hyperref} % For clickable links
\usepackage{bookmark} % Better control over bookmarks
\usepackage{geometry} % Customize page layout
\usepackage{xcolor} % Colors for text and graphics
\usepackage{enumitem} % Customizable lists
\usepackage{fancyhdr} % Header and footer
\usepackage{titlesec} % Custom section/chapter titles
\usepackage[toc,page]{appendix} % For the appendix
\usepackage{longtable} % For tables spanning multiple pages
\usepackage{mathrsfs} % For script fonts in math mode
\usepackage{tocloft} % Custom table of contents
\usepackage{datetime2} % For dates
\usepackage{caption} % For better control over captions
\usepackage{float} % Fine control over figure/table placement
\usepackage{imakeidx} % For index
\usepackage{afterpage} % For blank page

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\cftchapfont}{\normalfont} % Remove bold for chapter names
\renewcommand{\cftchappagefont}{\normalfont} % Remove bold for chapter page numbers
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\eax}[1]{\emph{#1}\index{#1}} % Macro for emphasis and index
\newcommand{\abs}[1]{\left| #1 \right|} % Absolute value
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tr}{\text{tr}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\adj}{\text{adj}}
\newcommand{\ip}[1]{\langle #1 \rangle}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Custom Notation List Environment
\newlist{notationlist}{description}{1}
\setlist[notationlist]{font=\bfseries,labelsep=1em}

% Geometry Settings
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
}

% Hyperref Colors
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    citecolor=red
}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}

% Custom Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Chapter name on top left
\fancyhead[R]{\rightmark}  % section name on top right
\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Making index
\makeindex[intoc]

% Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries \centering}
  {\chaptername\ \thechapter}{20pt}{\Huge \centering}

\titlespacing*{\chapter}{0pt}{20pt}{100pt}

\begin{document}

\pagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vspace*{\fill}
    % Title in all caps
    {\Huge \textbf{\MakeUppercase{Linear Algebra II}}\par}

    \vspace{0.5cm} % Adjust vertical spacing between title and subtitle
    % Subtitle in normal text, slightly enlarged
    {\Large Anita Naolekar, notes by Ramdas Singh\par}

    \vspace{0.5cm} % Additional spacing before the author
    % Author information
    {\large Second Semester\par}
    \vspace*{\fill}
    \end{center}
\end{titlepage}

\clearpage

\pagenumbering{roman}

\chapter*{List of Symbols}
\begin{notationlist}
    \item
\end{notationlist}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
\pagenumbering{arabic}
\pagestyle{fancy}


%%-------------------------------------------------------------------------------------------------


\chapter{PERMUTATION GROUPS}


\textit{January 3rd.}

Let $S_{n}$ denote the set of all bijections (permutations) on the set $\{1,2,\ldots,n\}$. If $\sigma, \tau \in S_{n}$, let us define $\sigma \tau$ to be the bijection defined as
\begin{equation}
    (\sigma \tau)(i) = \sigma(\tau(i)) \forall 1 \leq i \leq n.
\end{equation}

This gives us a binary operation on $S_{n}$ which is associative, and $S_{n}$ will then contain the identity permutation 1 such that $\sigma 1 = 1 \sigma = \sigma$ for all $\sigma \in S_{n}$. For every such $\sigma$, we can also find a $\sigma^{-1} \in S_{n}$ such that $\sigma \sigma^{-1} = \sigma^{-1}\sigma = 1$. The set $S_{n}$ equipped with this binary operation, thus, forms a group. In this case, we call $S_{n}$ as the \eax{symmetric group} of degree $n$. We now define a cycle in regards to permutations.

\begin{definition}
    A \eax{cycle} is a a string of positive integers, say $(i_{1},i_{2},\ldots,i_{k})$, which represents the permutation $\sigma \in S_{n}$ (with $k\leq n$) such that $\sigma(i_{j})=i_{j+1}$ for all $1 \leq j \leq k-1$, and $\sigma(i_{k}) = i_{1}$, and fixes all other integers. 
\end{definition}

We also note that $S_{3}$ is the smallest Abelian group possible, upto isomorphism. $S_{3}$ is one of the only two groups of order 6, and can be written as
\begin{equation}
    S_{3} = \{1, \sigma=(1,2,3), \sigma^{2}=(1,3,2), \tau=(1,2),\sigma\tau=(1,3),\tau\sigma=(2,3)\}.
\end{equation}
Some other observations arise. We find that $\sigma^{3} = \tau^{2} = 1$, and that $\tau\sigma = \sigma^{2}\tau$. We notice another fact via this $\sigma$;

\begin{remark}
    A \eax{k-cycle} $\sigma=(i_{1},i_{2},\ldots,i_{k})$ is of order $k$, that is, $\sigma^{k}=1$.
\end{remark}

\begin{definition}
    Two cycles in $S_{n}$ are called disjoint if they have no intger in common.
\end{definition}
We note that if $\sigma$ and $\tau$ are two disjoint cycles in $S_{n}$ then $\sigma$ and $\tau$ commute, that is, $\sigma \tau = \tau \sigma$.

\begin{proposition}
    Every $\sigma$ in $S_{n}$ can be written uniquely as a product of disjoint cycles.
\end{proposition}
Every cycle can be written as a product of 2-cycles. 2-cycles are called \eax{transpositions}. This can easily be seen as
\begin{equation}
    (a_{1},a_{2},\ldots,a_{n})=(a_{1},a_{n})(a_{1},a_{n-1})\cdots(a_{1},a_{3})(a_{1},a_{2}).
\end{equation}

\section{Even and Odd Permutations}
Let $x_{1},x_{2},\ldots,x_{n}$ be indeterminates, and let
\begin{equation}
    \Delta = \prod_{1 \leq i < j \leq n} (x_{i}-x_{j}).
\end{equation}
Let $\sigma \in S_{n}$, and define 
\begin{equation}
    \sigma(\Delta) = \prod_{1 \leq i < j \leq n} (x_{\sigma(i)}-x_{\sigma(j)}).
\end{equation}
We find that $\sigma(\Delta) = \pm \Delta$. Based on this, we classify permutations as odd or even.

\begin{definition}
    A permutation $\sigma$ is said to be an \eax{even permutation} if $\sigma(\Delta) = \Delta$, and is said to be an \eax{odd permutation} if $\sigma(\Delta) = -\Delta$. The sign of a permutation $\sigma$, denoted by $\epsilon(\sigma)$, is $+1$ if $\sigma$ is even, and is $-1$ if $\sigma$ is odd. So, $\sigma(\Delta) = \epsilon(\sigma)\Delta$.
\end{definition}

\begin{proposition}
    The map $\epsilon: S_{n} \to \{-1,+1\}$, where $\epsilon(\sigma)$ is the sign of $\sigma$, is a homomorphism, that is, $\epsilon(\sigma \tau) = \epsilon(\sigma)\epsilon(\tau)$ for all $\sigma, \tau \in S_{n}$.
\end{proposition}
\begin{proof}
    Start with $\tau(\Delta)$;
    \begin{equation}
        \tau(\Delta) = \prod_{1 \leq i < j \leq n} (x_{\tau(i)}-x_{\tau(j)}).
    \end{equation}
    Let there be $k$ factors of this polynomial where $\tau(i)>\tau(j)$ with $i<j$. We find that $\tau(\Delta) = (-1)^{k}\Delta$, and so, $\epsilon(\tau) = (-1)^{k}$. Now, $\sigma \tau(\Delta)$ has exactly $k$ factors of the form $x_{\sigma(j)}-x_{\sigma(i)}$, with $j > i$. Bringing out a factor $(-1)^{k}$, we find that $\sigma \tau (\Delta)$ has all factors of the form $x_{\sigma(i)}-x_{\sigma(j)}$, with $j > i$. Thus,
    \begin{equation}
        \epsilon(\sigma \tau)\Delta = \sigma \tau (\Delta) = (-1)^{k} \prod_{1 \leq i < j \leq n} (x_{\sigma(i)}-x_{\sigma(j)}) = (-1)^{k} \sigma(\Delta) = (-1)^{k} \epsilon(\sigma) \Delta = \epsilon(\tau) \epsilon(\sigma) \Delta.
    \end{equation}
    Cancelling out the $\Delta$, we find $\epsilon(\sigma \tau) = \epsilon(\sigma) \epsilon(\tau)$.
\end{proof}
$\epsilon$ is a homomorphism to an Abelian group, so $\epsilon(\sigma \tau) = \epsilon(\sigma) \epsilon(\tau) = \epsilon(\tau) \epsilon(\sigma)$.

\begin{proposition}
    If $\lambda = (i,j)$ is a transposition, then $\epsilon(\lambda) = -1$.
\end{proposition}
\begin{proof}
    If $\lambda = (1,2) \in S_{n}$, it is easy to show that
    \begin{equation}
        \lambda(\Delta) = (x_{1}-x_{2}) \cdots (x_{1}-x_{n}) (x_{2}-x_{3}) \cdots (x_{2}-x_{n}) \cdots = (-1)(\Delta).
    \end{equation}
    Now, if $\sigma = (i,j)$, with $(i,j) \neq (1,2)$, then $(i,j) = \lambda (1,2) \lambda$ where $\lambda$ interchanges $1$ and $i$, and interchanges $2$ and $j$. Using that fact that $\epsilon$ is a homomorphism, $\epsilon(\sigma) = -1$.
\end{proof}

A cycle $\sigma$ of length $k$ is an even permutation if and only if $k$ is odd. This is because it can be decomposed into $k-1$ transpositions, and we would then have $\epsilon(\sigma) = (-1)^{k-1} = 1$ (using the fact that $\epsilon$ is a homomorphism). Some more corollaries of the previous proposition include the fact that $\epsilon$ is a surjective map, and that $\epsilon(\sigma^{-1}) = \epsilon(\sigma)$.

If, for $\sigma \in S_{n}$, $\sigma$ can be decomposed as $\sigma_{1}\sigma_{2} \cdots \sigma_{k}$, where $\sigma_{i}$ is a $m_{i}$-cycle, then $\epsilon(\sigma_{i}) = (-1)^{m_{i}-1}$, and $\epsilon(\sigma) = (-1)^{(\sum m_{i}) - k}$.

\begin{proposition}
    $\sigma$ is an odd permutation if and only if the number of cycles of even length in its cycle decomposition is odd.
\end{proposition}

\section{The Determinant}

\begin{definition}
    If $A = (a_{ij})$ is a square matrix of order $n$, then the \eax{determinant} of $A$ is defined as
    \begin{equation}
        \det{A} = \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma(n)}.
    \end{equation}
\end{definition}
Using this definition of the determinant of a square matrix, one may derive the usual determinant properties with ease.\\ \\
\textit{January 7th.}

\begin{remark}
    The following properties may be inferred:
    \begin{itemize}
        \item If $A$ contains a row of zeroes, or a column of zeroes, then $\det{A} = 0$.
        \item $\det{I_{n}} = 1$.
        \item The determinant of a diagonal matrix is the product of the diagonal elements. This is because if $\sigma \in S_{N}$ is not the identity permutation, then there exists at least one element in the corresponding term where $i \neq \sigma(i)$, and $a_{i \sigma(i)}$ makes the term zero. For the identity transformation, it contains only those elements of the form $a_{ii}$.
    \end{itemize}
\end{remark}

Other non-trivial properties may also be shown with ease.

\begin{corollary}
    If $A$ is an upper triangular matrix, then $\det{A}$ is the product of the diagonal entries.
\end{corollary}
\begin{proof}
    If $a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \neq 0$, then $a_{n \sigma(n)} \neq 0$, that is, $\sigma(n) = n$, as $a_{ni} = 0 \; \forall \; i < n$. Again, $\sigma_{(n-1) \sigma(n-1)} \neq 0$ leads us to conclude that $\sigma(n-1) = n-1$ as $\sigma$ is a bijection and has to lead to a non-zero element. By similar logic, $\sigma(i) = i$ for all valid $i$. So, $\sigma$ is the identity permutation.
\end{proof}
\begin{corollary}
    If $A$ is a lower triangular matrix, then $\det{A}$ is the product of the diagonal entries.
\end{corollary}
\begin{proof}
    The proof of this is similar to the previous proof if we consider that the determinant of the tranpose of a matrix is equal to the determinant of said matrix.
\end{proof}

\begin{theorem}
    The determinant of a matrix is equal to the determinant of its transpose, that is, $\det{A} = \det{A^{t}}$ for a square matrix $A$.
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}


\begin{proposition}
    Let $B$ be obtained from $A$ by multiplying a row (or column) of $A$ by a non-zero scalar, $\alpha$. Then, $\det{B} = \alpha \det{A}$.
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{proposition}
    If $B$ is obtained from $A$ by interchanging any two rows (or columns) of $A$, then $\det{B} = -\det{A}$.
\end{proposition}
\begin{proof}
    Let $B$ be obtained from $A$ by interchanging the rows $k$ and $l$, with $k < l$. We then have
    \begin{align}
        \det{B} &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) b_{1 \sigma(1)} b_{2 \sigma(2)} \cdots b_{n \sigma(n)} \notag \\
        &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{(k-1) \sigma(k-1)} a_{l \sigma(k)} \sigma_{(k+1) \sigma(k+1)} \cdots a_{k \sigma(l)} \cdots a_{n \sigma(n)}.
    \end{align}
    As $\sigma$ runs through all elements in $S_{n}$, $\tau = \sigma(k, l)$ also runs through all $S_{n}$. Hence, via $\epsilon(\tau) = -\epsilon(\sigma)$, the equation now looks like
    \begin{align}
        \det{B} &= -\sum_{\tau \in S_{n}} \epsilon(\tau) a_{1 \tau(1)} \cdots a_{l \tau(l)} \cdots a_{k \tau(k)} \cdots a_{n \tau(n)} = -\det{A}.
    \end{align}
\end{proof}

\begin{proposition}
    If two rows (or columns) of $A$ are equal, then $\det{A} = 0$.
\end{proposition}
\begin{proof}
    Suppose that the rows $k$ and $l$ of $A$ are equal. Interchanging will alter the determinant by $-1$, so $\det{A} = -\det{A} \implies 2 \det{A} = 0 \implies \det{A} = 0$ if $2 \neq 0$ in the field $F$ from where the elements of $A$ arrive.

    If $2 = 0$ in $F$, that is, $F$ is of characteristic $2$, we pair the $\sigma$ term in the expression of $\det {A}$ with the term $\tau$ where $\tau = \sigma (k, l)$. The terms corresponding to $\sigma$ and $\tau$ in the expressions are the same, differing in only the sign. Hence, $\det{A} = 0$.
\end{proof}

\begin{theorem}
    For a fixed $k$, let the row $k$ of $A$ be the sum of the two row vectors $X^{t}$ and $Y^{t}$, that is, $a_{kj} = x_{j} + y_{j}$ for all $1 \leq j \leq n$. Then $\det{A} = \det{B} + \det{C}$ where $B$ is obtained from $A$ by replacing the row $k$ of $A$ by the row vector $X^{t}$, and $C$ is obtained from $A$ by replacing the row $k$ of $A$ by the row vector $Y^{t}$.
\end{theorem}
\begin{proof}
    We utilize the fact that $a_{kj} = x_{j} + y_{j}$. We have
    \begin{align}
        \det{A} &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{k \sigma(k)} \cdots a_{n \sigma(n)} \notag \\
        &= \left( \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots x_{\sigma(k)} \cdots a_{n \sigma(n)} \right) + \left( \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots y_{\sigma(k)} \cdots a_{n \sigma(n)} \right) \notag \\
        &= \det{B} + \det{C}. \notag
    \end{align}
\end{proof}

\begin{proposition}
    If a scalar multiple of a row (or column) is added to a row (or column) of a matrix, the determinant remains unchanged.
\end{proposition}
\begin{proof}
    The proof follows immediately from the previously proved properties.
\end{proof}
\textit{January 10th.}

\begin{definition}
    For $a_{ij} \in A$, the \eax{cofactor} of $a_{ij}$ is $A_{ij}=(-1)^{i+j} \det M_{ij}$, where $M_{ij}$ is the $(n-1) \times (n-1)$ matrix obtained from $A$ by deleting the $i^{\text{th}}$ row and $j^{\text{th}}$ column of $A$.
\end{definition}
\begin{lemma}
    Fix $k,j$. If $a_{kl} = 0$ for all $l \neq j$, then $\det{A} = a_{kj}A_{kj}$.
\end{lemma}
\begin{proof}
    Take $A$ to be a $n \times n$ matrix. We deal in cases.
    \begin{itemize}
        \item Case I: $k=j=n$. In the expansion of the determinant,
        \begin{equation}
            \det{A} = \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)}, \notag
        \end{equation}
        only those $\sigma$'s survive where $\sigma(n) = n$. These $\sigma$'s can be thought of as permutations of $S_{n-1}$ instead. The sign of $\sigma \in S_{n}$ and $\sigma \in S_{n-1}$ is the same as $n$ is fixed. Thus, we get
        \begin{equation}
            a_{nn} \sum_{\sigma \in S_{n-1}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{(n-1) \sigma(n-1)} = a_{nn} \det M_{nn} = (-1)^{n+n} a_{nn} A_{nn} = a_{nn} A_{nn}.
        \end{equation}
        \item Case II: $(k,j) \neq (n,n)$. We construct a matrix $B$ by interchanging $n-k$ rows and $n-j$ columns to bring $a_{ij}$ to the position $(n,n)$. Thus, we have $\det B = (-1)^{n-k+n-j} \det A = (-1)^{k+j} \det A$. But $B = a_{kj} \det M_{kj}$, so
        \begin{equation}
            \det A = (-1)^{k+j} a_{kj} \det M_{kj} = a_{kj} A_{kj}.
        \end{equation}
    \end{itemize}
\end{proof}
\begin{theorem}
    Let $A$ be a $n \times n$ matrix, and let $1 \leq k \leq n$. Then, $\det {A} = \sum \limits_{j=1}^{n} a_{kj} A_{kj}$, expansion by the $k^{\text{th}}$ row.
\end{theorem}
\begin{proof}
    Write out the $k^{\text{th}}$ row of $A$ as $x_{1}^{t}+\ldots+x_{n}^{t}$, where $x_{i} = (0,\ldots,0,a_{ki},0,\ldots,0)^{t}$, and all the other rows remaining are the same. Writing the matrix $A$ as the sum of $n$ matrices where each matrix is the same as $A$ but with a row that looks like $x_{i}^{t}$, we can easily show that $\det{A} = \sum_{j=1}^{n} a_{kj} A_{kj}$.
\end{proof}
\begin{example}
    Let $n \geq 1$, and let $A_{n} = \begin{pmatrix}
        a_{1}^{n-1} & a_{1}^{n-2} & \ldots & a_{1} & 1 \\
        a_{2}^{n-1} & a_{2}^{n-2} & \ldots & a_{2} & 1 \\
        \ldots & \ldots & \ldots & \ldots & \ldots \\
        a_{n}^{n-1} & a_{n}^{n-2} & \ldots & a_{n} & 1
    \end{pmatrix}$. Then, $\det {A_{n}} = \prod \limits_{1 \leq i \leq j \leq n} (a_{i}-a_{j})$.
\end{example}
\begin{proof}
    If $a_{i}=a_{j}$ for some $i \neq j$, then $\det{A_{n}} = 0$ as two rows are then identical. Hence, assume that the $a_{i}$'s are distinct. Now construct
    \begin{equation}
        B_{n} = 
        \begin{pmatrix}
            x_{1}^{n-1} & x_{1}^{n-2} & \ldots & x_{1} & 1 \\
            a_{2}^{n-1} & a_{2}^{n-2} & \ldots & a_{2} & 1 \\
            \ldots & \ldots & \ldots & \ldots & \ldots \\
            a_{n}^{n-1} & a_{n}^{n-2} & \ldots & a_{n} & 1
        \end{pmatrix}.
    \end{equation}
    Notice that $\det B_{n} \in F[x]$, where $F$ is the field, and $x$ is an indeterminate. $\det {B}$ is also of degree $(n-1)$; let us call this polynomial $f(x)$. Each of $a_{2},\ldots,a_{n}$ are roots of $f(x)$, so $f(x)$ must be of the form $f(x) = C(x-a_{2}) \ldots (x-a_{n})$. Equating coefficients of $x^{n-1}$, we get
    \begin{equation}
        C = \prod_{2 \leq i < j \leq n} (a_{i}-a_{j}) = \det \begin{pmatrix}
            a_{2}^{n-2} & \ldots & a_{2} & 1 \\
            \ldots & \ldots & \ldots & \ldots \\
            a_{n}^{n-2} & \ldots & a_{n} & a_{1}
        \end{pmatrix}.
    \end{equation}
    Thus, we must have
    \begin{align}
        f(x) &= \left( \prod_{2 \leq i < j \leq n} (a_{i}-a_{j}) \right) (x-a_{2}) \cdots (x-a_{n}) \\
        \implies \det A_{n} = f(1) &= \prod_{1 \leq i < j \leq n} (a_{i}-a_{j}).
    \end{align}
\end{proof}
\begin{example}
    Show that there exists a unique polynomial of degree $n$ that takes arbitrary prescribed values at the $(n+1)$ points $x_{0},x_{1},\ldots,x_{n}$.
\end{example}

\chapter{EIGENVECTORS AND EIGENVALUES}

\section{Linear Transformers and an Introduction}
Let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of vector space $V$ and $\mc{C} = (w_{1},\ldots,w_{n})$ be a basis of a vector space $W$. As these are bases, given a $v \in V$, there exists a unique $X \in F^{n}$ such that $v = \mc{B}X$, called the \eax{coordinate vector} of $v$ with respect to the basis $\mc{B}$. We note that since the mapping from a $v \in V$ to a $X \in F^{n}$ is linear in nature and is bijection, the vector spaces $V$ and $F^{n}$ are isomorphic to each other. Similarly, a mapping that takes $w \in W$ to $Y \in F^{m}$ shows that $W$ and $F^{m}$ are isomorphic to each other. 

Now suppose that there exists a linear map that takes $v \mapsto Tv$ with $v \in V$ and $Tv \in W$. This transformer $T$ is with respect to the bases $\mc{B}$ and $\mc{C}$ of $V$ and $W$, respectively. We construct the $m \times n$ matrix $A$ so that the $j^{\text{th}}$ column of $A$ is the coordinate vector of $Tv_{j}$ with respect to the basis $\mc{C}$. We will then have $T(\mc{B}) = \mc{C}A$. For any vector $v \in V$, we have
\begin{align}
    v &= \mc{B}X = v_{1}x_{1}+ \ldots v_{n}x_{n} \notag \\
    \implies T(v) &= T(v_{1})x_{1} + \ldots T(v_{n})x_{n} = (T(v_{1}),\ldots,T(v_{n})) \begin{pmatrix}
        x_{1} \\ x_{2} \\ \ldots \\ x_{n}
    \end{pmatrix} = T(\mc{B})X = (\mc{C}A) X \\
    &= (w_{1},\ldots,w_{m}) AX;
\end{align}
the coordinate vector of $Tv$ with respect to the basis $AX$. In fact, if we denote the isomorphism from $V$ to $F^{n}$ by $\phi_{\mc{B}}$ and the isomorphism from $W$ to $F^{m}$ by $\phi_{\mc{C}}$, we get $\phi_{\mc{C}} \circ T = (\text{mult. by $A$}) \circ \phi_{\mc{B}}$.

The next theorem will be divided into two parts.
\begin{theorem}
    \begin{enumerate}
        \item The vector space form. Let $T:V \to W$ be a linear mapping between finite dimensional vector spaces $V$ and $W$, of dimensions $n$ and $m$ respectively. There are bases $\mc{B}$ and $\mc{C}$ of $V$ and $W$ respectively such that the matrix of $T$ with respect to the bases $\mc{B}$ and $\mc{C}$ looks like $\begin{pmatrix}
            I_{r} & O_{r \times (n-r)} \\ O_{(m-r) \times r} & O_{(m-r) \times (n-r)}
        \end{pmatrix}_{m \times n}$.
        \item The matrix form. If $A$ is a $m \times n$ matrix, then there exists an invertible matrix $Q_{m \times m}$ and an invertible matrix $P_{n \times n}$ such that $Q^{-1}AP$ is of the form $\begin{pmatrix}
            I_{r} & 0 \\ 0 & 0
        \end{pmatrix}$, where $r$ is the rank of $A$.
        \item In fact, both these forms of the theorem are equivalent.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Let $(u_{1},\ldots,u_{n-r})$ be a basis of $\ker{T}$. We can extend this to a basis $\mc{B}$ by appending independent vectors that do not belong to the kernel of $T$, that is, $(v_{1},\ldots,v_{r},u_{1},\ldots,u_{n-r})$. Let $(Tv_{1},\ldots,Tv_{r})$ be a basis of $\text{Im}{T}$. We can extend this to a basis of $W$, say $\mc{C} = (w_{1},\ldots,w_{r},w_{r+1},\ldots,w_{m})$, where $w_{i} = Tv_{i}$ for $1 \leq i \leq r$. These bases are the desired ones.
        \item $P$ is a sequence of column operations, multipled to form a matrix, and $Q^{-1}$ is a sequence of row operations, multiplied to form a matrix, that get the matrix $A$ into the desired form. These are our desired $P$ and $Q$.
        \item Suppose the vector space form holds. Let $A$ be a $m \times n$ matrix over $F$, with $A:F^{n} \to F^{m}$ defined as $X \mapsto AX$. There then exists a basis $\mc{B}$ of $F^{n}$ and a basis $\mc{C}$ of $F^{m}$ such that the linear map $A$ with respect to ther bases $\mc{B}$ and $\mc{C}$ has the desired matrix. We then have $\mc{B} =I_{n}P_{n \times n}$ and $\mc{C} = I_{m}Q_{m \times m}$, with both $P$ and $Q$ invertible. We claim that the matrix of the linear mapping $A$ with respect to the bases $\mc{B}$ and $\mc{C}$ is $Q^{-1}AP$.
    \end{enumerate}
\end{proof}

\textit{January 16th.}
\begin{proposition}
    \begin{enumerate}
        \item Let $T:V \to W$ be a linear map, and $A$ the matrix of $T$ with respect to the bases $\mc{C}$ and $\mc{C}$ of $V$ and $W$ respectively. Let $\mc{B}'$ and $\mc{C}'$ be new bases of $V$ and $W$ respectively, and let the change of basis matrices be given by $\mc{B}' = \mc{B}P$ and $\mc{C}' = \mc{C}Q$. Then the matrix of $T$ with respect to $\mc{B}'$ and $\mc{C}'$ is $Q^{-1}AP$.
        \item If $A' = Q_{1}^{-1}AP_{1}$, where $P_{1}$ and $Q_{1}$ are $n \times n$ and $m \times m$ invertible matrices, respectively, then $A'$ is the matrix of $T$ with respect to the bases $\mc{B}P_{1}$ and $\mc{C}Q_{1}$.
    \end{enumerate}
    
\end{proposition}
\begin{proof}
    Let the coordinate vector of $v$ with respect to the basis $\mc{B}'$ be $X'$. We claim that the coordinate vector of $Tv$ with respect to the basis $\mc{C}'$ is $Y'$, where $Y' = (Q^{-1}AP)X'$. We assume that $\mc{B}' = \mc{B}P_{n \times n}$, $\mc{C}' = \mc{C}Q_{m \times m}$, and $T(\mc{B}) = \mc{C}A_{m \times n}$. If $v = \mc{B}X$, then $T(v) = \mc{C}(AX)$. If we let $v = \mc{B}'\mc{X}' = v_{1}'x_{1}' + \ldots + v_{n}'x_{n}'$, then
    \begin{equation}
        T(v) = \mc{C}'Y' = (\mc{C}Q)' = \mc{C}(QY') = \mc{C}(APX') \implies QY' = APX' \implies Y' = (Q^{-1}AP)X'
    \end{equation}
    To prove the second part, we will show that the first part implies it. Let $A_{m \times n}$ be a matrix. Let $T_{A}$ be the linear map from $\R^{n} \to \R^{m}$ given by multiplication by $A$, that is $T_{A} : \R^{n} \to \R^{m}$ given by $X \mapsto AX$. By the first part, there exist bases $P_{n \times n}$ and $Q_{m \times m}$, both invertible, such that with respect $P$ and $Q$, the matrix of $T_{A}$ looks like $\begin{pmatrix}
        I & O \\ O & O
    \end{pmatrix}$, that is, $Q^{-1}AP = \begin{pmatrix}
        I & O \\ O & O
    \end{pmatrix}$.
\end{proof}

\subsection{Linear Operators}
Let $T:V_{\mc{B}} \to V_{\mc{B}}$. Let $A$ be the matrix of $T$ with respect to the basis $\mc{B}$. The other matrices of $T$ with respect to new bases are $P^{-1}AP$, where $P_{n \times n}$ is invertible. Also, the fact that $T$ is bijective, one-one, or onto are all equivalent for a finite dimensional vector space $V$.

\subsection{Eigenvectors and Eigenvalues}
\begin{definition}
    A non-zero vector $v \in V$ is said to be an \eax{eigenvector} of $T$ if $T(v) = \lambda v$ for some $\lambda \in \F$. If $A$ is a $n \times n$ matrix, a non-zero column vector $X$ is said to be an eigenvector of $A$ if $AX = \lambda X$ for some $\lambda \in \F$. $\lambda$, in both these cases, is called the \eax{eigenvalue} of $v$ and $X$ respectively.
\end{definition}
Usually, we always disregard the zero vector being an eigenvector. If $v$ is an eigenvector of $T:V \to V$, and $v = \mc{B}X$ with respect to some basis $\mc{B}$ of $V$, then $X$ is an eigenvector of the matrix of $T$ with respect to the basis $\mc{B}$. In fact,
\begin{equation}
    \mc{B}(AX) = (\mc{B}A)X = T(\mc{B})X = T(\mc{B}X) = Tv = \lambda v = \lambda \mc{B} X = \mc{B} (\lambda X) \implies AX = \lambda X.
\end{equation}
The converse is also true; if $X$ is an eigenvector of $A_{n \times n}$, then $X$ is also an eigenvector of $T_{A} : \R^{n} \to \R^{n}$.

\begin{proposition}
    0 is an eigenvalue of $A_{n \times n}$ ($T:V \to V)$ if and only if $A$ ($T$) is non-invertible (not an isomorphism).
\end{proposition}
Suppose $v$ is an eigenvector of $T: V \to V$ with eigenvalue $\lambda$. Let $W$ be the subspace spanned by $v$. Then every vector $w \in W$ is an eigenvector of $T$ with eigenvalue $\lambda$. The proof of this is left as an exercise.

\begin{definition}
    Two matrices $A_{n \times n}'$ and $A_{n \times n}$ are called \eax{similar matrices} if there exists an invertible matrix $P_{n \times n}$ such that $P^{-1}AP = A'$.
\end{definition}

Again let $T: V \to V$ be a linear operator, and let $\mc{B} = (v_{1},\ldots,v_{n})$. Suppose, with respect to the basis $\mc{B}$, the matrix of $T$ is $\begin{pmatrix}
    \lambda_{1} & \ldots & \ldots & \ldots \\ 0 & \ldots & \ldots & \ldots \\ \ldots & \ldots & \ldots & \ldots \\ 0 & \ldots & \ldots & \ldots    
\end{pmatrix}$. Then $v_{1}$ is an eigenvector with eigenvalue $\lambda_{1}$.
\section{Finding Eigenvalues and Eigenvectors}
\textit{January 21st.}

Let $T:V \to V$ and let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of $V$. Then the matrix of $T$ with respect to the basis $\mc{B}$ is a diagonal matrix if and only if each of the basis elements is an eigenvector. An equivalent statement for matrices is that an $n \times n$ matrix $A$ is similar to a diagonal matrix if and only if $\F^{n}$ admits a basis consisting of eigenvectors of $A$. The proof of this is left as an exercise to the reader.

We can now discuss the computation. For a linear operator $T: V \to V$, $\lambda$ is an eigenvalue of $T$ if and only if there exists a non-zero vector $v$ such that $Tv = \lambda v$. This can be rearranged to give
\begin{equation}
    (\lambda I_{v} -T)v = 0.
\end{equation}
We can now consider $\lambda I_{v} - T: V \to V$ to be a linear operator which maps $v \mapsto \lambda v - Tv$. If eigenvalues exist, this operator is a singular operator, that is, it contains a non-trivial kernel. The matrix of the operator $\lambda I_{v} - T$ comes out to be $\lambda I_{n} - A$, where $A$ is the matrix of $T$ with respect to the basis $\mc{B}$. This matrix is now singular, so we must have
\begin{equation}
    \det(\lambda I_{n} - A) = 0.
\end{equation}
The equation $\det(\lambda I_{n} - A)$ is called the \eax{characteristic polynomial} of $A$, and also $T$(?). The roots of this polynomial in $\lambda$ which lie in $\F$ are the eigenvalues of $A$, and $T$ as well.

We would now like to show that similar matrices have the same eigenvalues, that is,
\begin{equation}
    \det(\lambda I_{n} - P^{-1}AP) = \det(\lambda I_{n} - A).
\end{equation}
This is simple to see as $\det(\lambda I_{n} - P^{-1}AP) = \det(P^{-1}(\lambda I_{n} - A)P) = \det P^{-1} \cdot \det(\lambda I_{n} - A) \cdot \det P = \det(\lambda I_{n} - A)$. The found out eigenvalues from this equation can then be put back and solved for $v$ to get the corresponding eigenvectors.

\begin{proposition}
    Let $\lambda_{1},\ldots,\lambda_{r}$ be distinct eigenvalues of $T:V \to V$ and let $v_{1},\ldots,v_{r}$ be the corresponding eigenvectors of $T$. Then $(v_{1},\ldots,v_{r})$ is a linearly independent set in $V$.
\end{proposition}
\begin{proof}
    We claim that this is true for $r = 1,2$. Using a form of induciton, we will assume the result for $r-1$. Begin with
    \begin{align}
        \alpha_{1}v_{1} + \ldots + \alpha_{r}v_{r} &= 0 \notag \\
        \implies \alpha_{1}Tv_{1} + \ldots + \alpha_{r}Tv_{r} &= 0 \notag \\
        \implies \alpha_{1}\lambda_{1}v_{1} + \ldots + \alpha_{r}\lambda_{r}v_{r} &= 0.
    \end{align}
    Multiplying the first equation by $\lambda_{1}$ and subtracting it from the current equation, we have
    \begin{align}
        (\alpha_{2}\lambda_{2}-\alpha_{2}\lambda_{1})v_{2} + (\alpha_{3}\lambda_{3}-\alpha_{3}\lambda_{1})v_{3} + \ldots + (\alpha_{r}\lambda_{r}-\alpha_{r}\lambda_{1})v_{r} &= 0 \notag \\
        \implies \alpha_{2}(\lambda_{2}-\lambda_{1}) + \alpha-{3}(\lambda_{3}-\lambda_{1})v_{3} + \ldots + \alpha_{r}(\lambda_{r}-\lambda_{1})v_{r} &= 0.
    \end{align}
    By hypothesis, $\alpha_{j}(\lambda_{j}-\lambda_{1}) = 0$. As the eigenvalues are distinct, we must have $\alpha_{j} = 0$ for $j = 2,3,\ldots,r$. We are left with $\alpha_{1}v_{1} = 0$, which gives us $\alpha_{1} = 0$.
\end{proof}
When the $n$ eigenvalues found of $A$ are distinct, the corresponding eigenvectors $v_{1},\ldots,v_{n}$ are linearly independent in $\F^{n}$, and hence $\mc{B} = (v_{1},\ldots,v_{n})$ is a basis of $\F^{n}$. The matrix $P^{-1}AP$ is the matrix of the linear operator $T_{A} : \F^{n} \to \F^{n}$ with respect to the basis $\mc{B}$, with the column of $P$ being the eigenvectors $v_{1},\ldots,v_{n}$. As $\mc{B}$ consists of only eigenvectors, $P^{-1}AP$ is a diagonal matrix with the diagonal entries being the $n$ eigenvalues.

We now define the determinant and trace for a linear operator. For such an operator $T$, $\text{tr}T = \text{tr}A$ where $A$ is a matrix of $T$ with respect to some abitrary basis. Note that since $\tr(P^{-1}AP) = \tr(APP^{-1}) = \tr{A}$, the choice of basis is not important. Similarly, we define $\det{T} = \det{A}$.

We can now have a closer look at the characteristic equation. To find the constant term of $\det(xI-A)$, we simply plug in $x=0$ to give us $\det(-A) = (-1)^{n} \det A$. The coefficient of $x^{n-1}$ in $\det(xI-A)$ is $-\tr{A}$ as the coefficients of $x^{n-1}$ come solely from the expansion of $(x-a_{11})(x-a_{22})\cdots(x-a_{nn})$. Clearly, we can conclude that the sum of the eigenvalues is $\tr{A}$ and the product of the eigenvalues is $\det{A}$.

\subsection{Eigenspace}
\textit{January 23rd.}

For ease, let us denote $\chi_{T}(x)$ to mean $\det(xI-A)$. The \eax{eigenspace} for a given eigenvalue $\lambda$ is defined as
\begin{equation}
    E_{\lambda} = \{v \in V : Tv = \lambda v\}.
\end{equation}
This is a subspace of the vector space $V$. The \eax{geometric multiplicity} of $\lambda$ is defined as the dimension of $E_{\lambda}$. This geometric multiplicity of $\lambda$ is always less than or equal to its algebraic multiplicity in $\chi_{T}(x)$. For recall, the \eax{algebraic multiplicity} of $\lambda$ is the highest power of $(x-\lambda)$ that divides $\chi_{T}(x)$.

\begin{theorem}
    Let $\lambda$ be an eigenvalue of $T:V \to V$. Then the geometric multiplicity of $\lambda$ is always less than or equal to its algebraic multiplicity.
\end{theorem}
\begin{proof}
    Let $k$ me the geometric multiplicity of $\lambda$. Let $(v_{1},\ldots,v_{k})$ be an ordered basis of $E_{\lambda}$. Extend this to a basis $\mc{B} = (v_{1},\ldots,v_{k},u_{1},\ldots,u_{n-k})$ of $V$. The matrix of $T$ with respect to the basis $\mc{B}$ is of the form $A = \begin{pmatrix}
        \lambda I_{k} & B \\ O & D
    \end{pmatrix}$. Thus, the characteristic polynomial looks like
    \begin{equation}
        \chi_{T}(x) = \det(xI_{n}-A) = \det \begin{pmatrix}
            (x-\lambda)I_{k} & -B \\ O & xI_{n-k}-D
        \end{pmatrix}
        = (x-\lambda)^{k} \cdot \det(XI_{n-k}-D).
    \end{equation}
    This shows that $(x-\lambda)^{k}$ divides $\chi_{T}(x)$, so we must have an algebraic multiplicity greater than or equal to this $k$.
\end{proof}

\section{Diagonalizability}
We first define what this means for a linear mapping from $V$ to $V$.
\begin{definition}
    A linear operator $T: V \to V$ is said to be a \eax{diagonizable linear operator} if there exists a basis of $V$ consisting of eigenvectors of $T$. This means that the matrix of $T$ with respect to this basis if a digaonal matrix and the matrix of $T$ with respect to any other basis is similar to this diagonal matrix.
\end{definition}
A similar definition works for matrices.
\begin{definition}
    An $n \times n$ matrix $A$ over $\F$ is said to be a \eax{diagonizable matrix} if $A$ is similar to a diagonal matrix. Equivalently, $\F^{n}$ then admits a basis consisting of eigenvectors of $A$, thinking of $T_{A}:\F^{n} \to \F^{n}$ as a linear operator.
\end{definition}

Now let us suppose that $T$ is diagonizable. Let $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$ be the distinct eigenvalues of $T$. There then exists an ordered basis consisting of eigenvectors of $T$ and with respect to this basis, the matrix of $T$ is a diagonal matrix with diagonal entries consisting solely of $\lambda_{1},\lambda_{2},\ldots,\lambda_{k}$.

If $\lambda_{i}$ is of algebraic multiplicity $d_{i}$, then the matrix of $T$ looks like $\begin{pmatrix}
    \lambda_{1}I_{d_{1}} & & & \\
     & \lambda_{2}I_{d_{2}} & & \\
     & & \ldots & \\
     & & & \lambda_{k}I_{d_{k}}
\end{pmatrix}$. Thus, the characteristic polynomial then looks like $(x-\lambda_{1})^{d_{1}} (x-\lambda_{2})^{d_{2}} \cdots (x-\lambda_{k})^{d_{k}}$.

The geometric multiplicity of $\lambda_{i}$ is the dimension of $E_{\lambda_{i}}$, that is, the nullity of the operator $(\lambda_{i}I_{n}-A)$. But here, $\ker(\lambda_{i}I_{n}-A) = d_{i}$, which is just the algebraic multiplicity of $\lambda_{i}$. Hence, if $T$ is diagonizable, then each eigenvalue of it has the same algebraic multiplicity and geometric multiplicity.

\begin{proposition}
    If $E_{\lambda_{1}},\ldots,E_{\lambda_{k}}$ are the eigenspaces corresponding to the distinct eigenvalues, say, $\lambda_{1},\ldots,\lambda_{k}$ of $T$, then $E = E_{\lambda_{1}} + \cdots + E_{\lambda_{k}}$ is a direct sum.
\end{proposition}
\begin{proof}
    It is enough to show that $E_{\lambda_{1}},\ldots,E_{\lambda_{k}}$ are independent. Let $v_{1}+v_{2}+\ldots+v_{k} = 0$, where $v_{i} \in E_{\lambda_{i}}$. As $v_{1},v_{2},\ldots,v_{k}$ come from distinct eigenspaces, they are linearly independent, and our equation must imply that $v_{1} = \ldots = v_{k} = 0$.
\end{proof}
\begin{proposition}
    If $T$ is a diagonizable operator, and if $\lambda_{1},\ldots,\lambda_{k}$ are the distinct eigenvalues of $T$, then
    \begin{equation}
        V = E_{\lambda_{1}} \oplus \ldots \oplus E_{\lambda_{k}}.
    \end{equation}
\end{proposition}
\begin{proof}
    As $T$ is diagonizable, the algebraic and geometric multiplicities are equal for all the eigenvalues $\lambda_{i}$. Denote $\dim E_{\lambda_{i}} = d_{i}$. As $\chi_{T}(x)$ completely factors into linear factors, due to $T$ being diagonizable, we have $n = d_{1} + \ldots + d_{k}$. Also, $E_{\lambda_{1}} + \ldots + E_{\lambda_{k}}$ is a direct sum, that is,
    \begin{equation}
        \dim(E_{\lambda_{1}}+\ldots+E_{\lambda_{k}}) = \dim E_{\lambda_{1}} + \ldots + \dim E_{\lambda_{k}} = n.
    \end{equation}
    This direct sum is a subspace of $V$ and has the dimension as $V$. This mut mean that the direct sum is exactly $V$.
\end{proof}

\begin{theorem}
    Let $T$ be a linear operator on a finite dimensional vector space $V$, and let $\lambda_{1},\ldots,\lambda_{k}$ be the distinct eigenvalues of $T$. Also let $E_{\lambda_{i}}$ be the eigenspace of $\lambda_{i}$. Then, the following are equivalent.
    \begin{itemize}
        \item $T$ is diagonizable,
        \item $\chi_{T}(x) = (x-\lambda_{1})^{d_{1}} \cdots (x-\lambda_{k})^{d_{k}}$ and $\dim E_{\lambda_{i}} = d_{i}$,
        \item $V = E_{\lambda_{1}} \oplus \ldots \oplus E_{\lambda_{k}}$.
    \end{itemize}
\end{theorem}

\section{Polynomials}
\textit{January 28th.}

Let $\F[x]$ denote the set of all polynomials with coefficients coming from the field $\F$. With respect to the addition, it is an Abelian group. The multiplication here is associative, commutative, and distributive; there also exists a multiplicative identity. This makes $\F[x]$ into a commutative ring. Note that $\F[x]$ is also an infinite dimensional vector space over $\F$, since scalar multiplication is also defined. Together, these combine to form an algebra over the field.

\begin{definition}
    Let $d \in \F[x]$ with $d \neq 0$. For $f \in \F[x]$, we say that $d$ divides $f$ if there exists a $q \in \F[x]$ such that $f = dq$ in $\F[x]$.
\end{definition}
\begin{corollary}
    For $f \in \F[x]$, $f(c) = 0$ if and only if $x-c$ divides $f(x)$.
\end{corollary}
\begin{corollary}
    A polynomial $f \in \F[x]$ of degree $n$ has at most $n$ roots in $\F$.
\end{corollary}
\begin{proof}
    The proof is by induction. Note that this is true for $n = 0, 1$. If $\alpha$ is a root, then $f(x) = (x-\alpha) \cdot q(x)$. As $q(x)$ is of degree $n-1$, and all roots of $q(x)$ are root of $f(x)$, this follows by hypothesis.
\end{proof}

\begin{definition}
    An \eax{ideal} of $\F[x]$ is a subspace of $\F[x]$, say $I$, such that if $f \in I$ and $g \in \F[x]$, then $fg \in I$.
\end{definition}
\begin{example}
    Let $f \in \F[x]$. Define $I_{f} = \langle f\rangle = \{fg : g \in \F[x]\}$. Note that $I_{f}$ is called a \eax{principal ideal}, that is, it is an ideal generated by a single element.
\end{example}
\begin{theorem}
    $\F[x]$ is a principal ideal domain, that is, every ideal in $\F[x]$ is a principal ideal.
\end{theorem}
\begin{proof}
    Let $d$ be a polynomial of least degree in the ideal $I$, where $I$ is a non-zero ideal. Let, without loss of generatlity, $d$ be monic (if not, simply multiply it by a sutitable scalar).

    Let $f \in I$. Then there exists $q,r \in \F[x]$ such that $f = dq + r$ and either $r = 0$ or $\deg r < \deg d$. Note that since $f,d \in I$, $dq \in I$, so $f-dq \in I \implies r \in I$. As $d$ was of minimal degree in $I$, we must have $r = 0$. Thus, $f = dq$ and, thus, $I = \langle d \rangle$.
\end{proof}
If $I$ is an ideal of $\F[x]$, then there exists a unique polynomial $d \in I$ such that $I = \langle d \rangle$.

\subsection{Interaction with Linear Operators}
Let $f \in \F[x]$, and let $T:V \to V$ be a linear mapping. If
\begin{equation*}
    f(x) = a_{0} + a_{1}x + \ldots + a_{k}x^{k}
\end{equation*}
with $a_{k} \neq 0$, we define
\begin{equation*}
    f(T) = a_{0}I_{n} + a_{1}T + \ldots + a_{k}T^{k}.
\end{equation*}
Note that $f(T)$ is also a linear mapping from $V$ to $V$. Let $I$ be the set of all $f \in \F[x]$ such that $f(T)$ is the zero operator. All such polynomials are called \eax{annihilator}s. $I$ satisfies the properties of a vector space; it is a subspace of the space of all polynomials. $I$ is also an ideal of $\F[x]$.

\begin{definition}
    The \eax{minimal polynomial} of the linear operator $T:V \to V$ is the generator of the ideal of annihilators.
\end{definition}
Denote the minimal polynomial by $m_{T}(x)$. So, $m_{T}(x)$ is
\begin{enumerate}
    \item monic,
    \item of least degree among all annihilators of $T$.
\end{enumerate}
If $A$ is a $n \times n$ matrix, the minimal polynomial of $A$ is defined as the unique monic polynomial $m_{A}(x)$ of least degree such that $m_{A}(A) = O_{n \times n}$. It can be verified that if $A$ is the matrix of a linear operator $T:V \to V$ and if $f \in \F[x]$, then the matrix of the operator $f(T) : V \to V$ is $f(A)$ with respect to the same basis. It follows that the minimal polynomial of $T$ is same as the minimal polynomial of a matrix of $T$.

Note that $T$ belongs to $\text{Hom}_{\F}(V, V)$, which is of dimension $n^{2}$. Thus, $I, T, T^{2}, \ldots, T^{n^{2}}$ is a linearly dependent set and there exist scalars $a_{0},a_{2},\ldots,a_{n^{2}}$ such that
\begin{equation}
    a_{0}I + a_{1} T + a_{2}T^{2} + \ldots + a_{n^{2}} T^{n} = O.
\end{equation}
So, an annihilator of $T$ is
\begin{equation*}
    f(x) = a_{0} + a_{1}x + a_{2}x^{2} + \ldots + a_{n^{2}} x^{n^{2}}
\end{equation*}
and we must have $\deg m_{T}(x) \leq n^{2}$.

\begin{theorem}
    Let $T: V \to V$ with $n$ the dimension of the space $V$. The characteristic polynomial of $T$ and the minimal polynomial of $T$ have the same roots, except (possibly) for the multiplicities.
\end{theorem}
\begin{proof}
    We claim that $m_{T}(c) = 0$ if and only if $c$ is an eigenvalue. Let $m_{T}(c) = 0$. Thus, $m_{T}(c) = (x-c) \cdot q(x)$, with $q \in \F[x]$ and $\deg q < \deg m$. Also, $q(T)$ is \textit{not} the zero operator. So, there exists a $u \in V$ (non-zero vector) such that $q(T)(u) = v \neq 0$. Then,
    \begin{align}
        0 = m(T)(u) = (T-cI) \cdot q(T)(u) = (T-cI)v
    \end{align}
    which shows that $v$ is an eigenvector of $T$ with eigenvalue $c$. So all roots of $m_{T}(x)$ are roots of the characteristic polynomial.

    Conversely, let $c$ be an eigenvalue of $T$. Say, $Tv = cv$ for some $v \neq 0$. Thus, $m_{T}(T)(v) = m(c)(v)$. But $m_{T}(T) = 0$ must mean that $0 = m(c)(v)$, and $m(c) = 0$. So every root of the characteristic polynomial is a root of the minimal polynomial.
\end{proof}

\textit{January 30th.}
\begin{proposition}
    If $\lambda$ is an eigenvalue of $T$, then $f(\lambda)$ is an eigenvalue of $f(T)$ for $f \in \F[x]$.
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}
\begin{proposition}
    Let $T: V \to V$ be a diagonizable operator. The minimal polynomial is the product of distinct linear factors, that is, if
    \begin{align*}
        \chi_{T}(x) = \prod_{i=1}^{k} (x-\lambda_{i})^{d_{i}}
    \end{align*}
    where the $\lambda_{i}$'s are the distinct eigenvalues, then
    \begin{align*}
        m_{T}(x) = \prod_{i=1}^{k} (x-\lambda_{i}).
    \end{align*}
\end{proposition}
\begin{proof}
    As $T$ is a diagonalizable operator, there exists a basis of $V$ consisting of eigenvectors of $T$, say $\mc{B} = (v_{1},v_{2},\ldots,v_{n})$. Note that $m_{T}(T)v_{i} = 0$ for all valid $i$. For each $v_{i} \in \mc{B}$, there exists a $\lambda_{i}$ such that $(T-\lambda_{i}I)v_{i} = 0$, which tells us
    \begin{equation}
        m_{T}(T) = (T-\lambda_{1}I)(T-\lambda_{2}I) \cdots (T-\lambda_{k}I) v_{i} = 0.
    \end{equation}
    Hence, $m_{T}(x)$ is an annihilator for $T$, and it is of minimal degree by the above theorem.
\end{proof}

\begin{theorem}[\eax{Cayley-Hamilton theorem}]
    Let $T: V \to V$ be a linear operator on a finite dimensional vector space $V$. If $\chi_{T}(x)$ is the characteristic polynomial of $T$, then $\chi_{T}(T) = 0$, that is, the characteristic polynomial annihilates $T$. Hence, the minimal polynomial of $T$ divides the characteristic polynomial.
\end{theorem}
\begin{proof}
    Let $\mc{B} = (v_{1},v_{2},\ldots,v_{n})$ be a basis of $V$, and let $A = (a_{ij})$ be the matrix of $T$ with respect to the basis $\mc{B}$. We have
    \begin{align}
        a_{1j}v_{1} + a_{2j}v_{2} + \ldots + a_{nj}v_{nj} &= Tv_{j} \notag \\
        \implies -a_{1j}v_{1}-a_{2j}v_{2} - \ldots + (T-a_{jj})v_{j} - a_{(j+1)(j)} v_{j+1} -\ldots - a_{nj}v_{n} &= 0.
    \end{align}
    This sysmte of equations can be written as
    \begin{equation}
        B_{n \times n} \begin{pmatrix}
            v_{1} \\ \vdots \\ v_{n}
        \end{pmatrix} = \begin{pmatrix}
            0 \\ \vdots \\ 0
        \end{pmatrix}
    \end{equation}
    where
    \begin{equation}
        B = \begin{pmatrix}
            T-a_{11}I & -a_{21} & \cdots & -a_{n1} \\
            -a_{12} & T-a_{22}I & \cdots & -a_{n2} \\
            \vdots & \vdots & \ddots & \vdots \\
            -a_{1n} & -a_{2n} & \cdots & T-a_{nn}I
        \end{pmatrix}.
    \end{equation}
    Therefore $\det{B} = \chi_{T}(T)$. It is enough to show that $\det{B} = 0$ as an operator, that is, to show $\det{B} (b_{i}) = 0$ for all $v_{i} \in \mc{B}$. Let $(\adj{B})_{ij} = c_{ij}$, and $(B)_{ij} = b_{ij}$. Note that
    \begin{equation*}
        \sum_{k=1}^{n} c_{ik} b_{kj} = \begin{cases}
            \det{B} &\text{ if } i = j,\\
            0 &\text{ if otherwise}.
        \end{cases}
    \end{equation*}
    Now,
    \begin{align}
        \sum_{j=1}^{n} b_{kj} v_{j} &= 0 \text{ for all } 1 \leq k \leq n \notag \\
        \implies \sum_{j=1}^{n} b_{kj} v_{j} &= 0 \notag.
    \end{align}
    Summing over all rows,
    \begin{align}
        \sum_{k=1}^{n} \left( \sum_{j=1}^{n} c_{ik} b_{kj} v_{j} \right) &= 0 \notag \\
        \implies \sum_{j=1}^{n} \left( \sum_{k=1}^{n} c_{ik} b_{kj} \right) v_{j} &= 0.
    \end{align}
    The left hand side is zero except for when $i = j$, in which case it is $\det{B}$---
    \begin{align}
        0 = \sum_{j=1}^{n} \left( \sum_{k=1}^{n} c_{ik} b_{kj} \right) v_{j} = (\det{B})v_{i}
    \end{align}
    which implies that the operator $\det{B}$ is zero on all the basis vectors, and hence it is the zero vecotr. Thus, since $\chi_{T}(T) = \det{B}$, $\chi_{T}(T)$ is also the zero operator.
\end{proof}

\textit{February 4th.}
\begin{proposition}
    If the minimal polynomial $m_{T}(x) \in \F[x]$ of a linear operator $T: V \to V$ splits into distinct linear factors, then $T$ is diagonalizable.
\end{proposition}
\begin{proof}
    Let $m_{T}(x) = (x-\lambda_{1})(x-\lambda_{2}) \cdots (x-\lambda_{k})$ where the $\lambda_{i}$'s are distinct. We are to show that $V = E_{\lambda_{1}} \oplus \ldots \oplus E_{\lambda_{k}}$. We wish to find polynomials $h_{1}(x), \ldots, h_{k}(x)$ such that
    \begin{enumerate}
        \item $h_{1}(x) + \ldots + h_{k}(x) = 1$,
        \item $(x-\lambda_{i}) \cdot h_{i}(x)$ is divisible by $m_{T}(x)$ for all $1 \leq i \leq k$.
    \end{enumerate}
    The second condition implies that $(T-\lambda_{i}I) \cdot h_{i}(T)$ is the zero operator. The first condition implies that $\sum_{i=1}^{k} h_{i}(T)(v) = v$. But the second condition again implies that $h_{i}(T)(v)$ is an eigenvector corresponding to $\lambda_{i}$, that is, $h_{i}(T)(v) \in E_{\lambda_{i}}$. If we can find these $h_{i}$'s satisfying the two conditions then we can say that $V$ is the direct sum of the eigenspaces.

    For $1 \leq i \leq k$, let $f_{i}(x) = \frac{m_{T}(x)}{(x-\lambda_{i})} = \prod_{j \neq i} (x-\lambda_{j})$. As the $\lambda_{i}$'s are distinct, the $f_{i}$'s are relatively prime, so there exist $g_{1}, \ldots, g_{k}$ such that
    \begin{equation}
        f_{1}(x)g_{1}(x) + \ldots + f_{k}(x) g_{k}(x) = 1.
    \end{equation}
    Let $h_{i}(x) = f_{i}(x) g_{i}(x)$ for all $1 \leq i \leq k$. Both the conditions hold, and the result follows.
\end{proof}

\begin{corollary}
    Let $T:V \to V$ be a linear operator on a finite dimensional complex vector space such that $T^{m} = I$ for some positive integral $m$. Then $T$ is diagonalizable.
\end{corollary}

\begin{proposition}
    Let $T:V \to V$ be  linear operator, and let $U$ be an invariant subspace of $T$, that is, $T(U) \subseteq U$ (or equivalently, $T(u) \in U$ for all $u \in U$). The minimal polynomial $m_{T|_{U}}(x)$ of the operator $T|_{U} : U \to U$ divides the minimal polynomial $m_{T}(x)$ of the operator $T: V \to V$ in $\F[x]$.
\end{proposition}
\begin{proof}
    Note that $m_{T}(T)(u) = 0$ for all $u \in U$ as $U \subseteq V$. Thus, $m_{T}(T) = 0$ on the subspace $U$. So $m_{T}(x)$ annihilates $T|_{U}$. So, as $m_{T|_{U}}(x)$ is the minimal polynomial of $T|_{U}$, it should divide all annihilators of $T|_{U}$ and thus divides $m_{T}(x)$ in $\F[x]$.
\end{proof}

\section{Triangularizability}
A similar definition works as in the case of diagonalizability.
\begin{definition}
    A linear operator $T: V \to V$ is said to be a \eax{triangularizable linear operator} if there exists a basis of $V$ with respect to which the matrix of $T$ is a triangular matrix, be it upper or lower.
\end{definition}
If our basis is $\mc{B} = (v_{1},v_{2},\ldots,v_{n})$, then we can show that $Tv_{k} \in \text{span}(v_{1},\ldots,v_{k})$.

\begin{theorem}
    A linear operator $T: V \to V$ is triangularizable if and only if the minimal polynomial splits into linear factors.
\end{theorem}
\begin{proof}
    Let $T:V \to V$ be triangularizable, that is, there exists a basis with respect to which the matrix of $T$ is a triangular matrix, with diagonal entries $\lambda_{1}, \ldots, \lambda_{n}$, say. Then the characteristic polynomial of $T$ is $(x-\lambda_{1})(x-\lambda_{2}) \cdots (x-\lambda_{n})$ where the $\lambda_{i}$'s are not necessarily distinct. But $m_{T}(x)$ divides $\chi_{T}(x)$, hence is again a product of linear factors.

    Conversely, let $m_{T}(x) = (x-\lambda_{1})(x-\lambda_{2}) \cdots (x-\lambda_{k})$ where the $\lambda_{i}$'s are not necessarily distinct. We prove by induction on the number of factors of $m_{T}(x)$. If $k = 1$, then $m_{T}(x) = x-\lambda_{1}$; as $m_{T}(T) = 0$, $T = \lambda_{1}I$, matrix of $T$ is the scalar matrix. Now let $k > 1$, and let the result hold for smaller positive integers. Let $U = \text{Im} (T-\lambda_{k}I)$. We find that $U$ is a proper subspace of $V$. Note that $U$ is an invariant subspace of $T$; if we let $u = (T-\lambda_{k}I)(v)$ for some $v \in V$, then
    \begin{equation}
        T(u) = T(T-\lambda_{k}I)(v) = (T-\lambda_{k}I)T(v) \in U.
    \end{equation}
    The minimal polynomial $m_{T|_{U}}(x)$ of $T|_{U}$ divides $m_{T}(x)$, and hence $m_{T|_{U}}(x) = (x-\alpha_{1}) \cdots (x-\alpha_{l})$, where $l \leq k$, and $\alpha_{1},\ldots,\alpha_{l} \in \{\lambda_{1},\ldots,\lambda_{k}\}$. By hypothesis, $T|_{U}$ is triangularizable. So there exists a basis of $U$, say $(u_{1},\ldots,u_{m})$ with respect to which the matrix of $T|_{U}$ is a triangular matrix. So $T|_{U}(u_{k}) \in \text{span}(u_{1},\ldots,u_{k})$. Extend this to a basis $\mc{B} = (u_{1},\ldots,u_{m},v_{m+1},\ldots,v_{n})$. If we rewrite $Tv_{j} = (T-\lambda_{k}I)v_{j} + \lambda_{k}Iv_{j}$, we see that $(T-\lambda_{k}I)v_{j} \in U = \text{span}(u_{1},\ldots,u_{m})$, and $Tv_{j} \in \text{span}(u_{1},\ldots,u_{m},v_{j})$; the matrix of $T$ with respect to $\mc{B}$ is a triangular matrix. 
\end{proof}
\begin{corollary}
    Every operator $T:V \to V$, where $V$ is a complex finite dimensional vector space, is triangularizable.
\end{corollary}

\subsection{Determinant of Partitioned Matrices}

\begin{proposition}
    Let $\Gamma = \begin{pmatrix}
        A & O \\ O & I
    \end{pmatrix}$ or $\Gamma = \begin{pmatrix}
        I & O \\ O & A
    \end{pmatrix}$, where $A$ is a square matrix. Then we necessarily have $\det{\Gamma} = \det{A}$.
\end{proposition}
\begin{proof}
    Let $\Gamma$ be of order $(n+1) \times (n+1)$ and $A$ be of order $n \times n$. By definition,
    \begin{equation}
        \det{\Gamma} = \sum_{\sigma \in S_{n+1}} \epsilon(\sigma) g_{1 \sigma(1)} \cdots g_{(n+1) \sigma(n+1)}.
    \end{equation}
    Note that $g_{(n+1)\sigma(n+1)}$ is 1 if $\sigma(n+1) = n+1$, and $0$ otherwise. Also, $\epsilon(\sigma)$ remains the same when $\sigma$ is considered to be an element of $S_{n}$. Thus,
    \begin{equation}
        \det \Gamma = \sum_{\sigma \in S_{n}} \epsilon(\sigma) g_{1 \sigma(1)} \cdots g_{n \sigma(n)} = \det A.
    \end{equation}
    Iterating this, we get the desired result. A similar proof works for the other type of matrix stated.
\end{proof}

\begin{proposition}
    Let $\Gamma = \begin{pmatrix}
        A & B \\ O & D
    \end{pmatrix}$ where $A$ and $D$ are square matrices. Then we necessarily have $\det \Gamma = \det{A} \cdot \det{D}$.
\end{proposition}
\begin{proof}
    Let the orders be $A_{k \times k}$, $D_{l \times l}$, $B_{k \times l}$ and $O_{l \times k}$. Note that $\Gamma$ can be broken up as
    \begin{equation}
        \Gamma = \begin{pmatrix}
            I_{k} & O_{k \times l} \\
            O_{l \times k} & D_{l \times l}
        \end{pmatrix} \begin{pmatrix}
            I_{k} & B_{k \times l} \\
            O_{l \times k} & I_{l}
        \end{pmatrix} \begin{pmatrix}
            A_{k \times k} & O_{k \times l} \\
            O_{l \times k} & I_{l}
        \end{pmatrix}.
    \end{equation}
    The determinant is multiplicative, so $\det \Gamma = \det D \cdot \det A$ as the determinant of the middle matrix can be shown to be 1.
\end{proof}

\begin{proposition}
    Let $\Gamma = \begin{pmatrix}
        A & B \\ C & D
    \end{pmatrix}$ where $A$ and $D$ are square matrices. If $A$ is invertible, then we necessarily have $\det \Gamma = \det A \cdot \det (D - CA^{-1}B)$.
\end{proposition}
\begin{proof}
    Again, we break down $\Gamma$.
    \begin{equation}
        \Gamma = \begin{pmatrix}
            I & O \\ CA^{-1} & I
        \end{pmatrix} \begin{pmatrix}
            A & O \\ O & D-CA^{-1}B
        \end{pmatrix} \begin{pmatrix}
            I & A^{-1}B \\ O & I
        \end{pmatrix}.
    \end{equation}
    From here, it is clear that $\det \Gamma = \det{A} \cdot \det(D-CA^{-1}B)$.
\end{proof}

\begin{proposition}
    Let $\Gamma = \begin{pmatrix}
        A & B \\ C & D
    \end{pmatrix}$ where $A$ and $D$ are square matrices. If $D$ is invertible, then we necessarily have $\det \Gamma = \det D \cdot \det (A - BD^{-1}C)$.
\end{proposition}
\begin{proof}
    Yet again, we break down $\Gamma$.
    \begin{equation}
        \Gamma = \begin{pmatrix}
            I & BD^{-1} \\ O & I
        \end{pmatrix} \begin{pmatrix}
            A-BD^{-1}C & O \\ O & D
        \end{pmatrix} \begin{pmatrix}
            I & O \\ D^{-1}C & I
        \end{pmatrix}.
    \end{equation}
    From here, it is clear that $\det \Gamma = \det{D} \cdot \det(A-BD^{-1}C)$.
\end{proof}

\section{On the Characteristic and Minimal Polynomials}
\textit{February 6th.}
\begin{theorem}
    Let $A$ be a $m \times n$ matrix and $B$ be a $n \times m$ matrix with $m \leq n$. Then,
    \begin{equation}
        \chi_{BA}(x) = x^{n-m} \chi_{AB}(x).
    \end{equation}
\end{theorem}
\begin{proof}
    Note that there exist non-singular matrices $P$ and $Q$ such that $PAQ = \begin{pmatrix}
        I_{r} & O \\ O & O
    \end{pmatrix}$, where $r$ is the rank of $A$. Partition $Q^{-1}BP^{-1}$ as $\begin{pmatrix}
        C & D \\ E & G
    \end{pmatrix}$ where $C$ is of order $r \times r$, with the other submatrices being of appropriate order. Then,
    \begin{equation}
        PABP^{-1} = PAQQ^{-1}BP^{-1} = \begin{pmatrix}
            I_{r} & O \\ O & O
        \end{pmatrix} \begin{pmatrix}
            C_{r \times r} & D \\ E & G
        \end{pmatrix} = \begin{pmatrix}
            C & D \\ O & O
        \end{pmatrix}.
    \end{equation}
    Similarly,
    \begin{equation}
        Q^{-1}BAQ = Q^{-1}BP^{-1}PAQ = \begin{pmatrix}
            C_{r \times r} & D \\ E & G
        \end{pmatrix} \begin{pmatrix}
            I_{r} & O \\ O & O
        \end{pmatrix} = \begin{pmatrix}
            C & O \\ E & O
        \end{pmatrix}.
    \end{equation}
    Thus,
    \begin{equation}
        \chi_{AB}(x) = \chi_{PABP^{-1}}(x) = \det \begin{pmatrix}
            xI_{r} - C & -D \\ O & xI
        \end{pmatrix} = x^{m-r} \det(xI-C)
    \end{equation}
    and
    \begin{equation}
        \chi_{BA}(x) = \chi_{Q^{-1}BAQ}(x) = \det \begin{pmatrix}
            xI - C & O \\ -E & xI
        \end{pmatrix} = x^{n-r} \det(xI-C)
    \end{equation}
    which tells us that $\chi_{BA}(x) = x^{n-m} \chi_{AB}(x)$.
\end{proof}

\begin{enumerate}
    \item Suppose $T:V \to V$ and $U \subseteq V$.
    \begin{enumerate}
        \item If $U \subseteq \ker T$, then $U$ is $T$-invariant.
        \item If $T(V) \subseteq U$, then $U$ is $T$-invariant.
    \end{enumerate}
    \item If $V_{1},\ldots,V_{m}$ are $T$-invariant subspaces, then $V_{1} + \ldots + V_{m}$ is $T$-invariant.
    \item Let $P:V \to V$ be a linear operator such that $P^{2} = P$;  then the eigenvalues of $P$ are 0 or 1.
    \item Let $T:V \to V$ be an invertible operator. Then $\lambda$ is an eigenvalue of $T$ if and only if $\frac{1}{\lambda}$ is an eigenvalue of $T^{-1}$.
    \item Let $T:V \to V$, with $0 \neq v \in V$. Then $W = \text{span}(v, Tv, T^{2}v, \ldots)$ is $T$-invariant, and is the smallest $T$-invariant subspace of $V$ containing $v$.
    \item Let $T:V \to V$ and $\text{rank}T = k \leq n$, where $n$ is the dimension of $V$. Then $T$ has at most $k+1$ distinct eigenvalues.
    \item 
\end{enumerate}

\chapter{INNER PRODUCT SPACES}
\textit{February 13th.}

\section{An Introduction}
The function $\ip{,} : \R^{n} \times \R^{n} \to \R$ defined as
\begin{equation}
    \ip{X,Y} = x_{1}y_{1} + x_{2}y_{2} + \ldots + x_{n}y_{n}
\end{equation}
is called an inner product. Specifically, this is the dot product on the vector space over the reals. It satisfies the following properties;
\begin{enumerate}
    \item $\ip{X,X} \geq 0$ for all $X \in \R^{n}$.
    \item $\ip{X,X} = 0$ if and only if $X = 0$.
    \item $\ip{X,Y} = \ip{Y,X}$.
    \item $\ip{X_{1}+X_{2},Y} = \ip{X_{1},Y} + \ip{X_{2},Y}$.
    \item $\ip{\alpha X, Y} = \alpha \ip{X,Y}$.
\end{enumerate}
In $\C^{n}$, we have the product
\begin{equation}
    \ip{Z,W} = z_{1}\conj{w_{1}} + z_{2}\conj{w_{2}} + \ldots + z_{n}\conj{w_{n}}.
\end{equation}
This satisfies the properties---
\begin{enumerate}
    \item $\ip{Z,Z} \geq 0$.
    \item $\ip{Z,Z} = 0$ if and only if $Z = 0$.
    \item $\ip{Z,W} = \conj{\ip{W,Z}}$.
    \item $\ip{Z_{1}+Z_{2},W} = \ip{Z_{1},W} + \ip{Z_{2},W}$.
    \item $\ip{\alpha Z, W} = \alpha \ip{Z,W}$.
\end{enumerate}
These properties are, respectively, called the positivity, the definiteness, the conjugate symmetry, the additivity, and the homogeneity of the inner product over the complex vector space. We now define a general inner product.\\

Let the underlying field be either $\R$ or $\C$, and let $V$ be a vector space over this field. An \eax{inner product} on $V$ is simply a function $\ip{,}:V \times V \to \F$ such that it satisfies the following properties for all $v,u,v_{1},v_{2} \in V$ and $\alpha \in \F$---
\begin{enumerate}
    \item $\ip{v,v} \geq 0$,
    \item $\ip{v,v} = 0$ if and only if $v = 0$,
    \item $\ip{v_{1}+v_{2},u} = \ip{v_{1},u} + \ip{v_{2},u}$,
    \item $\ip{\alpha v, u} = \alpha \ip{v,u}$, and
    \item $\ip{v,u} = \conj{\ip{u,v}}$.
\end{enumerate}
A vector space over $\F$, $\F$ being either $\R$ or $\C$, is called an \eax{inner product space} if $V$ is equipped with a valid inner product. As seen earlier, on $\R^{n}$, the usual dot product makes $\R^{n}$ an inner product space. As another example, if $V$ is the space of all real valued continuous function $f:(-1,1) \to \R$, then the inner product on here can be defined as
\begin{equation}
    \ip{f,g} = \int_{-1}^{1} f(x)g(x) dx.
\end{equation}
On $\C^{m \times n}$, we can define the inner product as
\begin{align}
    \ip{A,B} = \tr(B^{\ast}A).
\end{align}
Every inner product $\ip{u,v}$ for any vector space $V$ will look like
\begin{align}
    \ip{u,v} = Y^{\ast} AX,
\end{align}
where $Y$ and $X$ are the coordinate vectors of $v$ and $u$ with respect to some basis $\mc{B}$. This will be proved later.

For an inner product space $V$, the following properties may be derived from the basic properties;
\begin{enumerate}
    \item $\ip{0,v} = 0$ for all $v \in V$.
    \item Fix $v \in V$. Define $f_{v}:V \to \F$ as $u \mapsto \ip{u,v}$. Then $f_{v}$ is a linear mapping from the space $V$ to the space $\F$ for any $v \in V$.
    \item Let $v = \alpha_{1}v_{2} + \alpha_{2}v_{2} + \ldots + \alpha_{k}v_{k}$ and $u = \beta_{1}u_{1} + \beta_{2}u_{2} + \ldots + \beta_{l}u_{l}$ where $u,v,u_{j},v_{i} \in V$ and $\alpha_{i},\beta_{j} \in \F$. Then,
    \begin{align}
        \ip{v,u} = \sum_{i=1}^{k} \sum_{j=1}^{l} \alpha_{i}\conj{\beta_{j}} \ip{v_{i},u_{j}}.
    \end{align}
\end{enumerate}

\section{The Notion of Length and Orthogonality}
Let $(V,\ip{,})$ be an inner product space. We define the \eax{norm} of a vector $v \in V$, denoted by $\norm{v}$, as
\begin{equation}
    \norm{v} = \sqrt{\ip{v,v}}.
\end{equation}
For $V$ being either $\R^{n}$ or $\C^{n}$, we can easily verify that the norm becomes the usual Euclidean length of a vector. Note that $\norm{v} = 0$ if and only if $v$ is the zero vector in $V$. It can also be shown that $\norm{\lambda v} = \abs{\lambda} \norm{v}$ for some $\lambda \in \F$.

\begin{definition}
    We say that two vectors $v,w \in V$ are \eax{orthogonal vectors} if $\ip{v,w} = 0$.
\end{definition}
Note that the zero vector is orthogonal to every vector in the vector space, even itself; in fact, it is the only vector othogonal to itself. We can also make sense of a Pythogrean theorem here. If $u,v \in V$ are orthogonal, then we can show that
\begin{align}
    \norm{u+v}^{2} = \norm{u}^{2} + \norm{v}^{2}.
\end{align}
Given two vectos $u,v \in V$, we can write $u$ as the sum of a scalar multiple of $v$, say $cv$, and a vector $w$ such that $\ip{w,v} = 0$. If we rewrite $u$ as $u = cv + (u-cv)$, and impose that $\ip{u-cv,v} = 0$, then we get $c = \dfrac{\ip{u,v}}{\norm{v}^{2}}$ fulfulling our conditions.

We also have a \eax{Cauchy-Schwarz} inequality. It says that for any $u,v \in V$, then
\begin{align}
    \abs{\ip{u,v}} \leq \norm{u}\norm{v}
\end{align}
and equality holds if and only if one of the vectors is a scalar multile of the other.
\begin{proof}
    If either one of the vectors is the zero vector, both sides are just zero. Hence, assume that neither vector is zero, and note that we can write
    \begin{equation}
        u = \frac{\ip{u,v}}{\norm{v}^{2}}v + w
    \end{equation}
    where $\ip{w,v} = 0$. Thus,
    \begin{align}
        \norm{u}^{2} = \ip{\frac{\ip{u,v}}{\norm{v}^{2}}v + w, \frac{\ip{u,v}}{\norm{v}^{2}}v + w} = \frac{\ip{u,v}\conj{\ip{u,v}}}{\norm{v}^{4}}\ip{v,v} + \ip{w,w} = \frac{\abs{\ip{u,v}}^{2}}{\norm{v}^{2}} + \ip{w,w} \geq \frac{\abs{\ip{u,v}}^{2}}{\norm{v}^{2}}.
    \end{align}
    The inequality follows. The equality is left as an exercise to the reader.
\end{proof}

Let $V$ be an inner product space, and let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of $V$. Let $X = (x_{1},\ldots,x_{n})^{t}$ and $Y = (y_{1},\ldots,y_{n})^{t}$ be the coordinate vectors of $v,w \in V$, respectively, with respect to the basis $\mc{B}$ Then,
\begin{align}
    \ip{v,w} = Y^{\ast}AX \text{ where } A = (a_{ij}) \text{ and } a_{ij} = \ip{v_{j},v_{i}}.
\end{align}
This can be seen since
\begin{align}
    \ip{v,w} = \ip{\sum_{i=1}^{n} x_{i}v_{i}, \sum_{j=1}^{n} y_{j}v_{j}} = \sum_{i=1}^{n} \sum_{j=1}^{n} x_{i} \conj{y_{j}} \ip{v_{i},v_{j}}.
\end{align}
Conversely, let $V$ be a vector space of dimension $n$ and $\mc{B}$ be a basis of $V$. Then defining $\ip{v,w} = Y{^t}AX$, where $A$ is of order $n \times n$ satisfying $A^{\ast} = A$, gives an inner product

\textit{March 4th.}
\begin{theorem}[The \eax{triangle inequaity}]
    For all $v,w \in V$, $\norm{v+w} \leq \norm{v} + \norm{w}$ holds.
\end{theorem}
\begin{proof}
    We square the left side to get
    \begin{align}
        \norm{v+w}^{2} &= \ip{v+w,v+w} = \ip{v,v} + \ip{v,w} + \ip{w,v} + \ip{w,w} \notag \\
        &= \norm{v}^{2} + 2\text{Re}\ip{v,w} + \norm{w}^{2} \leq \norm{v}^{2} + 2\abs{\ip{v,w}} + \norm{w}^{2} \notag \\
        &\leq \norm{v}^{2} + 2\norm{v}\norm{w} + \norm{w}^{2} = (\norm{v}+\norm{w})^{2}.
    \end{align}
\end{proof}

\begin{theorem}[The \eax{parallelogram law}]
    For all $x,y \in V$, $\norm{x+y}^{2} + \norm{x-y}^{2} = 2\norm{x}^{2} + 2\norm{y}^{2}$ holds. 
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\subsection{Orthogonality and Orthonormality}
Note that if $v$ is orthogonal to $w$ and $z$, then it is orthogonal to both $w+z$ and $\alpha w$ for $v,w,z \in V$ and $\alpha \in \F$. Thus, if $x \in \text{span}\{w,z\}$, then $v$ is also orthogonal to $x$. The entire subspace spanned by $w$ and $z$ is orthogonal to $v$.

\begin{proposition}
    A set of orthogonal vectors, say $(S = \{v_{1},v_{2},\ldots,v_{n}\}$ with $\ip{v_{i},v_{j}} = 0$ for all $i \neq j$, is a linearly independent set, provided that $S$ does not contain the zero vector.
\end{proposition}
\begin{proof}
    Let $\alpha_{1}v_{1} + \alpha_{2}v_{2} + \ldots + \alpha_{n}v_{n} = 0$. For any valid $j$, note that
    \begin{align*}
        0 = \ip{0,v_{j}} = \ip{\sum_{i=1}^{n} \alpha_{i}v_{i}, v_{j}} = \alpha_{j}\ip{v_{j},v_{j}}
    \end{align*}
    which must imply that $\alpha_{j} = 0$ since $v_{j}$ is not the zero vector.
\end{proof}

\begin{definition}
    A set of orthogonal vectors is said to be an \eax{orthonormal set of vectors} if the norm of every vector in the set is unity.
\end{definition}

We note that if $S = \{v_{1},v_{2},\ldots,v_{n}\}$ is an orthogonal set, then $S' = \{\frac{v_{1}}{\norm{v_{1}}},\frac{v_{2}}{\norm{v_{2}}},\ldots,\frac{v_{n}}{\norm{v_{n}}}\}$ is an orthonormal set, provided $v_{i}$ is never the zero vector.

In the vector space $\R^{n}$, an orthonormal basis forms the columns of an invertible matrix $A$ such that $A^{t}A = I_{n}$. We take this as our definition of an orthogonal matrix.

\begin{definition}
    A invertible matrix $A$ is said to be an \eax{orthogonal matrix} if $A^{t} = A^{-1}$, that is, $A^{t}A = AA^{t} = I_{n}$.
\end{definition}

\begin{proposition}
    Let $\mc{B} = \{v_{1},\ldots,v_{n}\}$ be an orthonormal basis of $V$, where $V$ is an inner product space. Then for every $v \in V$, $v = \sum_{i=1}^{n} \ip{v,v_{i}} v_{i}$.
\end{proposition}
\begin{proof}
    We simply have $\ip{v,v_{j}} = \ip{\sum_{i=1}^{n} \alpha_{i}v_{i},v_{j}} = \alpha_{j}\ip{v_{j},v_{j}} = \alpha_{j}$, for some $v = \alpha_{1}v_{1} + \alpha_{2}v_{2} + \ldots + \alpha_{n}v_{n}$ with $\alpha_{i} \in \F$.
\end{proof}

\begin{appendices}

\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries}
  {\chaptername\ \thechapter}{20pt}{\Huge}

\titlespacing*{\chapter}{0pt}{20pt}{40pt}

\chapter{Appendix}
Extra content goes here.

\printindex

\end{appendices}

\end{document}