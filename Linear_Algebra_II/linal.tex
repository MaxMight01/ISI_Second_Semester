
\documentclass[15pt,a4paper]{book}

\usepackage{amsmath, amsthm, amssymb} 
\usepackage{graphicx} % For including graphics
\usepackage{hyperref} % For clickable links
\usepackage{bookmark} % Better control over bookmarks
\usepackage{geometry} % Customize page layout
\usepackage{xcolor} % Colors for text and graphics
\usepackage{enumitem} % Customizable lists
\usepackage{fancyhdr} % Header and footer
\usepackage{titlesec} % Custom section/chapter titles
\usepackage[toc,page]{appendix} % For the appendix
\usepackage{longtable} % For tables spanning multiple pages
\usepackage{mathrsfs} % For script fonts in math mode
\usepackage{tocloft} % Custom table of contents
\usepackage{datetime2} % For dates
\usepackage{caption} % For better control over captions
\usepackage{float} % Fine control over figure/table placement
\usepackage{imakeidx} % For index
\usepackage{afterpage} % For blank page

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\renewcommand{\cftchapfont}{\normalfont} % Remove bold for chapter names
\renewcommand{\cftchappagefont}{\normalfont} % Remove bold for chapter page numbers
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\eax}[1]{\emph{#1}\index{#1}} % Macro for emphasis and index
\newcommand{\abs}[1]{\left| #1 \right|} % Absolute value
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}

% Custom Notation List Environment
\newlist{notationlist}{description}{1}
\setlist[notationlist]{font=\bfseries,labelsep=1em}

% Geometry Settings
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=2.5cm,
    right=2.5cm,
}

% Hyperref Colors
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    citecolor=red
}

\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}

% Custom Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Chapter name on top left
\fancyhead[R]{\rightmark}  % section name on top right
\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Making index
\makeindex[intoc]

% Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries \centering}
  {\chaptername\ \thechapter}{20pt}{\Huge \centering}

\titlespacing*{\chapter}{0pt}{20pt}{100pt}

\begin{document}

\pagestyle{empty}

\begin{titlepage}
    \begin{center}
    \vspace*{\fill}
    % Title in all caps
    {\Huge \textbf{\MakeUppercase{Linear Algebra II}}\par}

    \vspace{0.5cm} % Adjust vertical spacing between title and subtitle
    % Subtitle in normal text, slightly enlarged
    {\Large Anita Naolekar, notes by Ramdas Singh\par}

    \vspace{0.5cm} % Additional spacing before the author
    % Author information
    {\large Second Semester\par}
    \vspace*{\fill}
    \end{center}
\end{titlepage}

\clearpage

\pagenumbering{roman}

\chapter*{List of Symbols}
\begin{notationlist}
    \item
\end{notationlist}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
\pagenumbering{arabic}
\pagestyle{fancy}


%%-------------------------------------------------------------------------------------------------


\chapter{PERMUTATION GROUPS}


\textit{January 3rd.}

Let $S_{n}$ denote the set of all bijections (permutations) on the set $\{1,2,\ldots,n\}$. If $\sigma, \tau \in S_{n}$, let us define $\sigma \tau$ to be the bijection defined as
\begin{equation}
    (\sigma \tau)(i) = \sigma(\tau(i)) \forall 1 \leq i \leq n.
\end{equation}

This gives us a binary operation on $S_{n}$ which is associative, and $S_{n}$ will then contain the identity permutation 1 such that $\sigma 1 = 1 \sigma = \sigma$ for all $\sigma \in S_{n}$. For every such $\sigma$, we can also find a $\sigma^{-1} \in S_{n}$ such that $\sigma \sigma^{-1} = \sigma^{-1}\sigma = 1$. The set $S_{n}$ equipped with this binary operation, thus, forms a group. In this case, we call $S_{n}$ as the \eax{symmetric group} of degree $n$. We now define a cycle in regards to permutations.

\begin{definition}
    A \eax{cycle} is a a string of positive integers, say $(i_{1},i_{2},\ldots,i_{k})$, which represents the permutation $\sigma \in S_{n}$ (with $k\leq n$) such that $\sigma(i_{j})=i_{j+1}$ for all $1 \leq j \leq k-1$, and $\sigma(i_{k}) = i_{1}$, and fixes all other integers. 
\end{definition}

We also note that $S_{3}$ is the smallest Abelian group possible, upto isomorphism. $S_{3}$ is one of the only two groups of order 6, and can be written as
\begin{equation}
    S_{3} = \{1, \sigma=(1,2,3), \sigma^{2}=(1,3,2), \tau=(1,2),\sigma\tau=(1,3),\tau\sigma=(2,3)\}.
\end{equation}
Some other observations arise. We find that $\sigma^{3} = \tau^{2} = 1$, and that $\tau\sigma = \sigma^{2}\tau$. We notice another fact via this $\sigma$;

\begin{remark}
    A \eax{k-cycle} $\sigma=(i_{1},i_{2},\ldots,i_{k})$ is of order $k$, that is, $\sigma^{k}=1$.
\end{remark}

\begin{definition}
    Two cycles in $S_{n}$ are called disjoint if they have no intger in common.
\end{definition}
We note that if $\sigma$ and $\tau$ are two disjoint cycles in $S_{n}$ then $\sigma$ and $\tau$ commute, that is, $\sigma \tau = \tau \sigma$.

\begin{proposition}
    Every $\sigma$ in $S_{n}$ can be written uniquely as a product of disjoint cycles.
\end{proposition}
Every cycle can be written as a product of 2-cycles. 2-cycles are called \eax{transpositions}. This can easily be seen as
\begin{equation}
    (a_{1},a_{2},\ldots,a_{n})=(a_{1},a_{n})(a_{1},a_{n-1})\cdots(a_{1},a_{3})(a_{1},a_{2}).
\end{equation}

\section{Even and Odd Permutations}
Let $x_{1},x_{2},\ldots,x_{n}$ be indeterminates, and let
\begin{equation}
    \Delta = \prod_{1 \leq i < j \leq n} (x_{i}-x_{j}).
\end{equation}
Let $\sigma \in S_{n}$, and define 
\begin{equation}
    \sigma(\Delta) = \prod_{1 \leq i < j \leq n} (x_{\sigma(i)}-x_{\sigma(j)}).
\end{equation}
We find that $\sigma(\Delta) = \pm \Delta$. Based on this, we classify permutations as odd or even.

\begin{definition}
    A permutation $\sigma$ is said to be an \eax{even permutation} if $\sigma(\Delta) = \Delta$, and is said to be an \eax{odd permutation} if $\sigma(\Delta) = -\Delta$. The sign of a permutation $\sigma$, denoted by $\epsilon(\sigma)$, is $+1$ if $\sigma$ is even, and is $-1$ if $\sigma$ is odd. So, $\sigma(\Delta) = \epsilon(\sigma)\Delta$.
\end{definition}

\begin{proposition}
    The map $\epsilon: S_{n} \to \{-1,+1\}$, where $\epsilon(\sigma)$ is the sign of $\sigma$, is a homomorphism, that is, $\epsilon(\sigma \tau) = \epsilon(\sigma)\epsilon(\tau)$ for all $\sigma, \tau \in S_{n}$.
\end{proposition}
\begin{proof}
    Start with $\tau(\Delta)$;
    \begin{equation}
        \tau(\Delta) = \prod_{1 \leq i < j \leq n} (x_{\tau(i)}-x_{\tau(j)}).
    \end{equation}
    Let there be $k$ factors of this polynomial where $\tau(i)>\tau(j)$ with $i<j$. We find that $\tau(\Delta) = (-1)^{k}\Delta$, and so, $\epsilon(\tau) = (-1)^{k}$. Now, $\sigma \tau(\Delta)$ has exactly $k$ factors of the form $x_{\sigma(j)}-x_{\sigma(i)}$, with $j > i$. Bringing out a factor $(-1)^{k}$, we find that $\sigma \tau (\Delta)$ has all factors of the form $x_{\sigma(i)}-x_{\sigma(j)}$, with $j > i$. Thus,
    \begin{equation}
        \epsilon(\sigma \tau)\Delta = \sigma \tau (\Delta) = (-1)^{k} \prod_{1 \leq i < j \leq n} (x_{\sigma(i)}-x_{\sigma(j)}) = (-1)^{k} \sigma(\Delta) = (-1)^{k} \epsilon(\sigma) \Delta = \epsilon(\tau) \epsilon(\sigma) \Delta.
    \end{equation}
    Cancelling out the $\Delta$, we find $\epsilon(\sigma \tau) = \epsilon(\sigma) \epsilon(\tau)$.
\end{proof}
$\epsilon$ is a homomorphism to an Abelian group, so $\epsilon(\sigma \tau) = \epsilon(\sigma) \epsilon(\tau) = \epsilon(\tau) \epsilon(\sigma)$.

\begin{proposition}
    If $\lambda = (i,j)$ is a transposition, then $\epsilon(\lambda) = -1$.
\end{proposition}
\begin{proof}
    If $\lambda = (1,2) \in S_{n}$, it is easy to show that
    \begin{equation}
        \lambda(\Delta) = (x_{1}-x_{2}) \cdots (x_{1}-x_{n}) (x_{2}-x_{3}) \cdots (x_{2}-x_{n}) \cdots = (-1)(\Delta).
    \end{equation}
    Now, if $\sigma = (i,j)$, with $(i,j) \neq (1,2)$, then $(i,j) = \lambda (1,2) \lambda$ where $\lambda$ interchanges $1$ and $i$, and interchanges $2$ and $j$. Using that fact that $\epsilon$ is a homomorphism, $\epsilon(\sigma) = -1$.
\end{proof}

A cycle $\sigma$ of length $k$ is an even permutation if and only if $k$ is odd. This is because it can be decomposed into $k-1$ transpositions, and we would then have $\epsilon(\sigma) = (-1)^{k-1} = 1$ (using the fact that $\epsilon$ is a homomorphism). Some more corollaries of the previous proposition include the fact that $\epsilon$ is a surjective map, and that $\epsilon(\sigma^{-1}) = \epsilon(\sigma)$.

If, for $\sigma \in S_{n}$, $\sigma$ can be decomposed as $\sigma_{1}\sigma_{2} \cdots \sigma_{k}$, where $\sigma_{i}$ is a $m_{i}$-cycle, then $\epsilon(\sigma_{i}) = (-1)^{m_{i}-1}$, and $\epsilon(\sigma) = (-1)^{(\sum m_{i}) - k}$.

\begin{proposition}
    $\sigma$ is an odd permutation if and only if the number of cycles of even length in its cycle decomposition is odd.
\end{proposition}

\section{The Determinant}

\begin{definition}
    If $A = (a_{ij})$ is a square matrix of order $n$, then the \eax{determinant} of $A$ is defined as
    \begin{equation}
        \det{A} = \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} a_{2 \sigma(2)} \cdots a_{n \sigma(n)}.
    \end{equation}
\end{definition}
Using this definition of the determinant of a square matrix, one may derive the usual determinant properties with ease.\\ \\
\textit{January 7th.}

\begin{remark}
    The following properties may be inferred:
    \begin{itemize}
        \item If $A$ contains a row of zeroes, or a column of zeroes, then $\det{A} = 0$.
        \item $\det{I_{n}} = 1$.
        \item The determinant of a diagonal matrix is the product of the diagonal elements. This is because if $\sigma \in S_{N}$ is not the identity permutation, then there exists at least one element in the corresponding term where $i \neq \sigma(i)$, and $a_{i \sigma(i)}$ makes the term zero. For the identity transformation, it contains only those elements of the form $a_{ii}$.
    \end{itemize}
\end{remark}

Other non-trivial properties may also be shown with ease.

\begin{corollary}
    If $A$ is an upper triangular matrix, then $\det{A}$ is the product of the diagonal entries.
\end{corollary}
\begin{proof}
    If $a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \neq 0$, then $a_{n \sigma(n)} \neq 0$, that is, $\sigma(n) = n$, as $a_{ni} = 0 \; \forall \; i < n$. Again, $\sigma_{(n-1) \sigma(n-1)} \neq 0$ leads us to conclude that $\sigma(n-1) = n-1$ as $\sigma$ is a bijection and has to lead to a non-zero element. By similar logic, $\sigma(i) = i$ for all valid $i$. So, $\sigma$ is the identity permutation.
\end{proof}
\begin{corollary}
    If $A$ is a lower triangular matrix, then $\det{A}$ is the product of the diagonal entries.
\end{corollary}
\begin{proof}
    The proof of this is similar to the previous proof if we consider that the determinant of the tranpose of a matrix is equal to the determinant of said matrix.
\end{proof}

\begin{theorem}
    The determinant of a matrix is equal to the determinant of its transpose, that is, $\det{A} = \det{A^{t}}$ for a square matrix $A$.
\end{theorem}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}


\begin{proposition}
    Let $B$ be obtained from $A$ by multiplying a row (or column) of $A$ by a non-zero scalar, $\alpha$. Then, $\det{B} = \alpha \det{A}$.
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\begin{proposition}
    If $B$ is obtained from $A$ by interchanging any two rows (or columns) of $A$, then $\det{B} = -\det{A}$.
\end{proposition}
\begin{proof}
    Let $B$ be obtained from $A$ by interchanging the rows $k$ and $l$, with $k < l$. We then have
    \begin{align}
        \det{B} &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) b_{1 \sigma(1)} b_{2 \sigma(2)} \cdots b_{n \sigma(n)} \notag \\
        &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{(k-1) \sigma(k-1)} a_{l \sigma(k)} \sigma_{(k+1) \sigma(k+1)} \cdots a_{k \sigma(l)} \cdots a_{n \sigma(n)}.
    \end{align}
    As $\sigma$ runs through all elements in $S_{n}$, $\tau = \sigma(k, l)$ also runs through all $S_{n}$. Hence, via $\epsilon(\tau) = -\epsilon(\sigma)$, the equation now looks like
    \begin{align}
        \det{B} &= -\sum_{\tau \in S_{n}} \epsilon(\tau) a_{1 \tau(1)} \cdots a_{l \tau(l)} \cdots a_{k \tau(k)} \cdots a_{n \tau(n)} = -\det{A}.
    \end{align}
\end{proof}

\begin{proposition}
    If two rows (or columns) of $A$ are equal, then $\det{A} = 0$.
\end{proposition}
\begin{proof}
    Suppose that the rows $k$ and $l$ of $A$ are equal. Interchanging will alter the determinant by $-1$, so $\det{A} = -\det{A} \implies 2 \det{A} = 0 \implies \det{A} = 0$ if $2 \neq 0$ in the field $F$ from where the elements of $A$ arrive.

    If $2 = 0$ in $F$, that is, $F$ is of characteristic $2$, we pair the $\sigma$ term in the expression of $\det {A}$ with the term $\tau$ where $\tau = \sigma (k, l)$. The terms corresponding to $\sigma$ and $\tau$ in the expressions are the same, differing in only the sign. Hence, $\det{A} = 0$.
\end{proof}

\begin{theorem}
    For a fixed $k$, let the row $k$ of $A$ be the sum of the two row vectors $X^{t}$ and $Y^{t}$, that is, $a_{kj} = x_{j} + y_{j}$ for all $1 \leq j \leq n$. Then $\det{A} = \det{B} + \det{C}$ where $B$ is obtained from $A$ by replacing the row $k$ of $A$ by the row vector $X^{t}$, and $C$ is obtained from $A$ by replacing the row $k$ of $A$ by the row vector $Y^{t}$.
\end{theorem}
\begin{proof}
    We utilize the fact that $a_{kj} = x_{j} + y_{j}$. We have
    \begin{align}
        \det{A} &= \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{k \sigma(k)} \cdots a_{n \sigma(n)} \notag \\
        &= \left( \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots x_{\sigma(k)} \cdots a_{n \sigma(n)} \right) + \left( \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots y_{\sigma(k)} \cdots a_{n \sigma(n)} \right) \notag \\
        &= \det{B} + \det{C}. \notag
    \end{align}
\end{proof}

\begin{proposition}
    If a scalar multiple of a row (or column) is added to a row (or column) of a matrix, the determinant remains unchanged.
\end{proposition}
\begin{proof}
    The proof follows immediately from the previously proved properties.
\end{proof}
\textit{January 10th.}

\begin{definition}
    For $a_{ij} \in A$, the \eax{cofactor} of $a_{ij}$ is $A_{ij}=(-1)^{i+j} \det M_{ij}$, where $M_{ij}$ is the $(n-1) \times (n-1)$ matrix obtained from $A$ by deleting the $i^{\text{th}}$ row and $j^{\text{th}}$ column of $A$.
\end{definition}
\begin{lemma}
    Fix $k,j$. If $a_{kl} = 0$ for all $l \neq j$, then $\det{A} = a_{kj}A_{kj}$.
\end{lemma}
\begin{proof}
    Take $A$ to be a $n \times n$ matrix. We deal in cases.
    \begin{itemize}
        \item Case I: $k=j=n$. In the expansion of the determinant,
        \begin{equation}
            \det{A} = \sum_{\sigma \in S_{n}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)}, \notag
        \end{equation}
        only those $\sigma$'s survive where $\sigma(n) = n$. These $\sigma$'s can be thought of as permutations of $S_{n-1}$ instead. The sign of $\sigma \in S_{n}$ and $\sigma \in S_{n-1}$ is the same as $n$ is fixed. Thus, we get
        \begin{equation}
            a_{nn} \sum_{\sigma \in S_{n-1}} \epsilon(\sigma) a_{1 \sigma(1)} \cdots a_{(n-1) \sigma(n-1)} = a_{nn} \det M_{nn} = (-1)^{n+n} a_{nn} A_{nn} = a_{nn} A_{nn}.
        \end{equation}
        \item Case II: $(k,j) \neq (n,n)$. We construct a matrix $B$ by interchanging $n-k$ rows and $n-j$ columns to bring $a_{ij}$ to the position $(n,n)$. Thus, we have $\det B = (-1)^{n-k+n-j} \det A = (-1)^{k+j} \det A$. But $B = a_{kj} \det M_{kj}$, so
        \begin{equation}
            \det A = (-1)^{k+j} a_{kj} \det M_{kj} = a_{kj} A_{kj}.
        \end{equation}
    \end{itemize}
\end{proof}
\begin{theorem}
    Let $A$ be a $n \times n$ matrix, and let $1 \leq k \leq n$. Then, $\det {A} = \sum \limits_{j=1}^{n} a_{kj} A_{kj}$, expansion by the $k^{\text{th}}$ row.
\end{theorem}
\begin{proof}
    Write out the $k^{\text{th}}$ row of $A$ as $x_{1}^{t}+\ldots+x_{n}^{t}$, where $x_{i} = (0,\ldots,0,a_{ki},0,\ldots,0)^{t}$, and all the other rows remaining are the same. Writing the matrix $A$ as the sum of $n$ matrices where each matrix is the same as $A$ but with a row that looks like $x_{i}^{t}$, we can easily show that $\det{A} = \sum_{j=1}^{n} a_{kj} A_{kj}$.
\end{proof}
\begin{example}
    Let $n \geq 1$, and let $A_{n} = \begin{pmatrix}
        a_{1}^{n-1} & a_{1}^{n-2} & \ldots & a_{1} & 1 \\
        a_{2}^{n-1} & a_{2}^{n-2} & \ldots & a_{2} & 1 \\
        \ldots & \ldots & \ldots & \ldots & \ldots \\
        a_{n}^{n-1} & a_{n}^{n-2} & \ldots & a_{n} & 1
    \end{pmatrix}$. Then, $\det {A_{n}} = \prod \limits_{1 \leq i \leq j \leq n} (a_{i}-a_{j})$.
\end{example}
\begin{proof}
    If $a_{i}=a_{j}$ for some $i \neq j$, then $\det{A_{n}} = 0$ as two rows are then identical. Hence, assume that the $a_{i}$'s are distinct. Now construct
    \begin{equation}
        B_{n} = 
        \begin{pmatrix}
            x_{1}^{n-1} & x_{1}^{n-2} & \ldots & x_{1} & 1 \\
            a_{2}^{n-1} & a_{2}^{n-2} & \ldots & a_{2} & 1 \\
            \ldots & \ldots & \ldots & \ldots & \ldots \\
            a_{n}^{n-1} & a_{n}^{n-2} & \ldots & a_{n} & 1
        \end{pmatrix}.
    \end{equation}
    Notice that $\det B_{n} \in F[x]$, where $F$ is the field, and $x$ is an indeterminate. $\det {B}$ is also of degree $(n-1)$; let us call this polynomial $f(x)$. Each of $a_{2},\ldots,a_{n}$ are roots of $f(x)$, so $f(x)$ must be of the form $f(x) = C(x-a_{2}) \ldots (x-a_{n})$. Equating coefficients of $x^{n-1}$, we get
    \begin{equation}
        C = \prod_{2 \leq i < j \leq n} (a_{i}-a_{j}) = \det \begin{pmatrix}
            a_{2}^{n-2} & \ldots & a_{2} & 1 \\
            \ldots & \ldots & \ldots & \ldots \\
            a_{n}^{n-2} & \ldots & a_{n} & a_{1}
        \end{pmatrix}.
    \end{equation}
    Thus, we must have
    \begin{align}
        f(x) &= \left( \prod_{2 \leq i < j \leq n} (a_{i}-a_{j}) \right) (x-a_{2}) \cdots (x-a_{n}) \\
        \implies \det A_{n} = f(1) &= \prod_{1 \leq i < j \leq n} (a_{i}-a_{j}).
    \end{align}
\end{proof}
\begin{example}
    Show that there exists a unique polynomial of degree $n$ that takes arbitrary prescribed values at the $(n+1)$ points $x_{0},x_{1},\ldots,x_{n}$.
\end{example}

\chapter{EIGENVECTORS AND EIGENVALUES}

\section{A Brief Summary of Linear Transformers}
Let $\mc{B} = (v_{1},\ldots,v_{n})$ be a basis of vector space $V$ and $\mc{C} = (w_{1},\ldots,w_{n})$ be a basis of a vector space $W$. As these are bases, given a $v \in V$, there exists a unique $X \in F^{n}$ such that $v = \mc{B}X$, called the \eax{coordinate vector} of $v$ with respect to the basis $\mc{B}$. We note that since the mapping from a $v \in V$ to a $X \in F^{n}$ is linear in nature and is bijection, the vector spaces $V$ and $F^{n}$ are isomorphic to each other. Similarly, a mapping that takes $w \in W$ to $Y \in F^{m}$ shows that $W$ and $F^{m}$ are isomorphic to each other. 

Now suppose that there exists a linear map that takes $v \mapsto Tv$ with $v \in V$ and $Tv \in W$. This transformer $T$ is with respect to the bases $\mc{B}$ and $\mc{C}$ of $V$ and $W$, respectively. We construct the $m \times n$ matrix $A$ so that the $j^{\text{th}}$ column of $A$ is the coordinate vector of $Tv_{j}$ with respect to the basis $\mc{C}$. We will then have $T(\mc{B}) = \mc{C}A$. For any vector $v \in V$, we have
\begin{align}
    v &= \mc{B}X = v_{1}x_{1}+ \ldots v_{n}x_{n} \notag \\
    \implies T(v) &= T(v_{1})x_{1} + \ldots T(v_{n})x_{n} = (T(v_{1}),\ldots,T(v_{n})) \begin{pmatrix}
        x_{1} \\ x_{2} \\ \ldots \\ x_{n}
    \end{pmatrix} = T(\mc{B})X = (\mc{C}A) X \\
    &= (w_{1},\ldots,w_{m}) AX;
\end{align}
the coordinate vector of $Tv$ with respect to the basis $AX$. In fact, if we denote the isomorphism from $V$ to $F^{n}$ by $\phi_{\mc{B}}$ and the isomorphism from $W$ to $F^{m}$ by $\phi_{\mc{C}}$, we get $\phi_{\mc{C}} \circ T = (\text{mult. by $A$}) \circ \phi_{\mc{B}}$.

The next theorem will be divided into two parts.
\begin{theorem}
    \begin{enumerate}
        \item The vector space form. Let $T:V \to W$ be a linear mapping between finite dimensional vector spaces $V$ and $W$, of dimensions $n$ and $m$ respectively. There are bases $\mc{B}$ and $\mc{C}$ of $V$ and $W$ respectively such that the matrix of $T$ with respect to the bases $\mc{B}$ and $\mc{C}$ looks like $\begin{pmatrix}
            I_{r} & O_{r \times (n-r)} \\ O_{(m-r) \times r} & O_{(m-r) \times (n-r)}
        \end{pmatrix}_{m \times n}$.
        \item The matrix form. If $A$ is a $m \times n$ matrix, then there exists an invertible matrix $Q_{m \times m}$ and an invertible matrix $P_{n \times n}$ such that $Q^{-1}AP$ is of the form $\begin{pmatrix}
            I_{r} & 0 \\ 0 & 0
        \end{pmatrix}$, where $r$ is the rank of $A$.
        \item In fact, both these forms of the theorem are equivalent.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item Let $(u_{1},\ldots,u_{n-r})$ be a basis of $\ker{T}$. We can extend this to a basis $\mc{B}$ by appending independent vectors that do not belong to the kernel of $T$, that is, $(v_{1},\ldots,v_{r},u_{1},\ldots,u_{n-r})$. Let $(Tv_{1},\ldots,Tv_{r})$ be a basis of $\text{Im}{T}$. We can extend this to a basis of $W$, say $\mc{C} = (w_{1},\ldots,w_{r},w_{r+1},\ldots,w_{m})$, where $w_{i} = Tv_{i}$ for $1 \leq i \leq r$. These bases are the desired ones.
        \item $P$ is a sequence of column operations, multipled to form a matrix, and $Q^{-1}$ is a sequence of row operations, multiplied to form a matrix, that get the matrix $A$ into the desired form. These are our desired $P$ and $Q$.
        \item Suppose the vector space form holds. Let $A$ be a $m \times n$ matrix over $F$, with $A:F^{n} \to F^{m}$ defined as $X \mapsto AX$. There then exists a basis $\mc{B}$ of $F^{n}$ and a basis $\mc{C}$ of $F^{m}$ such that the linear map $A$ with respect to ther bases $\mc{B}$ and $\mc{C}$ has the desired matrix. We then have $\mc{B} =I_{n}P_{n \times n}$ and $\mc{C} = I_{m}Q_{m \times m}$, with both $P$ and $Q$ invertible. We claim that the matrix of the linear mapping $A$ with respect to the bases $\mc{B}$ and $\mc{C}$ is $Q^{-1}AP$.
    \end{enumerate}
\end{proof}

\textit{January 16th.}
\begin{proposition}
    \begin{enumerate}
        \item Let $T:V \to W$ be a linear map, and $A$ the matrix of $T$ with respect to the bases $\mc{C}$ and $\mc{C}$ of $V$ and $W$ respectively. Let $\mc{B}'$ and $\mc{C}'$ be new bases of $V$ and $W$ respectively, and let the change of basis matrices be given by $\mc{B}' = \mc{B}P$ and $\mc{C}' = \mc{C}Q$. Then the matrix of $T$ with respect to $\mc{B}'$ and $\mc{C}'$ is $Q^{-1}AP$.
        \item If $A' = Q_{1}^{-1}AP_{1}$, where $P_{1}$ and $Q_{1}$ are $n \times n$ and $m \times m$ invertible matrices, respectively, then $A'$ is the matrix of $T$ with respect to the bases $\mc{B}P_{1}$ and $\mc{C}Q_{1}$.
    \end{enumerate}
    
\end{proposition}
\begin{proof}
    Let the coordinate vector of $v$ with respect to the basis $\mc{B}'$ be $X'$. We claim that the coordinate vector of $Tv$ with respect to the basis $\mc{C}'$ is $Y'$, where $Y' = (Q^{-1}AP)X'$. We assume that $\mc{B}' = \mc{B}P_{n \times n}$, $\mc{C}' = \mc{C}Q_{m \times m}$, and $T(\mc{B}) = \mc{C}A_{m \times n}$. If $v = \mc{B}X$, then $T(v) = \mc{C}(AX)$. If we let $v = \mc{B}'\mc{X}' = v_{1}'x_{1}' + \ldots + v_{n}'x_{n}'$, then
    \begin{equation}
        T(v) = \mc{C}'Y' = (\mc{C}Q)' = \mc{C}(QY') = \mc{C}(APX') \implies QY' = APX' \implies Y' = (Q^{-1}AP)X'
    \end{equation}
    To prove the second part, we will show that the first part implies it. Let $A_{m \times n}$ be a matrix. Let $T_{A}$ be the linear map from $\R^{n} \to \R^{m}$ given by multiplication by $A$, that is $T_{A} : \R^{n} \to \R^{m}$ given by $X \mapsto AX$. By the first part, there exist bases $P_{n \times n}$ and $Q_{m \times m}$, both invertible, such that with respect $P$ and $Q$, the matrix of $T_{A}$ looks like $\begin{pmatrix}
        I & O \\ O & O
    \end{pmatrix}$, that is, $Q^{-1}AP = \begin{pmatrix}
        I & O \\ O & O
    \end{pmatrix}$.
\end{proof}

\subsection{Linear Operators}
Let $T:V_{\mc{B}} \to V_{\mc{B}}$. Let $A$ be the matrix of $T$ with respect to the basis $\mc{B}$. The other matrices of $T$ with respect to new bases are $P^{-1}AP$, where $P_{n \times n}$ is invertible. Also, the fact that $T$ is bijective, one-one, or onto are all equivalent for a finite dimensional vector space $V$.

\section{Eigenvectors and Eigenvalues}
\begin{definition}
    A non-zero vector $v \in V$ is said to be an \eax{eigenvector} of $T$ if $T(v) = \lambda v$ for some $\lambda \in \F$. If $A$ is a $n \times n$ matrix, a non-zero column vector $X$ is said to be an eigenvector of $A$ if $AX = \lambda X$ for some $\lambda \in \F$. $\lambda$, in both these cases, is called the \eax{eigenvalue} of $v$ and $X$ respectively.
\end{definition}
Usually, we always disregard the zero vector being an eigenvector. If $v$ is an eigenvector of $T:V \to V$, and $v = \mc{B}X$ with respect to some basis $\mc{B}$ of $V$, then $X$ is an eigenvector of the matrix of $T$ with respect to the basis $\mc{B}$. In fact,
\begin{equation}
    \mc{B}(AX) = (\mc{B}A)X = T(\mc{B})X = T(\mc{B}X) = Tv = \lambda v = \lambda \mc{B} X = \mc{B} (\lambda X) \implies AX = \lambda X.
\end{equation}
The converse is also true; if $X$ is an eigenvector of $A_{n \times n}$, then $X$ is also an eigenvector of $T_{A} : \R^{n} \to \R^{n}$.

\begin{proposition}
    0 is an eigenvalue of $A_{n \times n}$ ($T:V \to V)$ if and only if $A$ ($T$) is non-invertible (not an isomorphism).
\end{proposition}
Suppose $v$ is an eigenvector of $T: V \to V$ with eigenvalue $\lambda$. Let $W$ be the subspace spanned by $v$. Then every vector $w \in W$ is an eigenvector of $T$ with eigenvalue $\lambda$. The proof of this is left as an exercise.

\begin{definition}
    Two matrices $A_{n \times n}'$ and $A_{n \times n}$ are called similar if there exists an invertible matrix $P_{n \times n}$ such that $P^{-1}AP = A'$.
\end{definition}

Again let $T: V \to V$ be a linear operator, and let $\mc{B} = (v_{1},\ldots,v_{n})$. Suppose, with respect to the basis $\mc{B}$, the matrix of $T$ is $\begin{pmatrix}
    \lambda_{1} & \ldots & \ldots & \ldots \\ 0 & \ldots & \ldots & \ldots \\ \ldots & \ldots & \ldots & \ldots \\ 0 & \ldots & \ldots & \ldots    
\end{pmatrix}$. Then $v_{1}$ is an eigenvector with eigenvalue $\lambda_{1}$.


\begin{appendices}

\titleformat{\chapter}[display]
  {\normalfont\Large\bfseries}
  {\chaptername\ \thechapter}{20pt}{\Huge}

\titlespacing*{\chapter}{0pt}{20pt}{40pt}

\chapter{Appendix}
Extra content goes here.

\printindex

\end{appendices}

\end{document}